<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Â§áÊàòËÄÉÁ†î‰∏≠]]></title>
    <url>%2F2019%2F07%2F15%2F%E5%A4%87%E6%88%98%E8%80%83%E7%A0%94%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[ËÄÉÁ†îÁªìÊùüÂâçÔºà19Âπ¥12ÊúàÂ∫ïÔºâÊòØ‰∏çÂÜôÂçöÂÆ¢ÁöÑ„ÄÇÂ¶ÇÊûúÊúâ‰ªÄ‰πàÁñëÈóÆÊàëÂèØËÉΩ‰πü‰∏ç‰ºöÂèäÊó∂ÂõûÂ§çÔºåÊä±Ê≠âüòÖ]]></content>
      <categories>
        <category>ÊùÇË¥ßÈì∫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Âú®Â∞èÁ±≥ÊâãÊú∫‰∏ä‰ΩøÁî®Â§™ÊûÅÈò≥Ê°ÜÊû∂]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%9C%A8%E5%B0%8F%E7%B1%B3%E6%89%8B%E6%9C%BA%E4%B8%8A%E4%BD%BF%E7%94%A8%E5%A4%AA%E6%9E%81%E9%98%B3%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[ÈáçË¶ÅÊñá‰ª∂‰∏ÄÂÆöË¶ÅÂ§á‰ªΩË∞®Èò≤ÁøªËΩ¶!ÈáçË¶ÅÊñá‰ª∂‰∏ÄÂÆöË¶ÅÂ§á‰ªΩË∞®Èò≤ÁøªËΩ¶!ÈáçË¶ÅÊñá‰ª∂‰∏ÄÂÆöË¶ÅÂ§á‰ªΩË∞®Èò≤ÁøªËΩ¶! Â§™ÊûÅÊ°ÜÊû∂ÂÆûÂú®ÊòØÂ§™Â•ΩÁî®‰∫ÜÔºåÂº∫ÁÉàÂÆâÂà© Step1:ÂÆâË£ÖMagisk1.1 Ëß£Èô§BL(Boy‚Äôs Love)ÈîÅÔºöËøõÂÖ•Áî≥ËØ∑Ëß£ÈîÅÈ°µÈù¢ÊåâÁÖßÊèêÁ§∫Êìç‰ΩúÂç≥ÂèØ(‡∏á ‚Ä¢_‚Ä¢)‡∏á 1.2 ÂÆâË£ÖÁ¨¨‰∏âÊñπRecoveryÂú®ÁîµËÑë‰∏äËøõË°å‰∏ãÈù¢ÁöÑÊìç‰ΩúÔºö ‰∏ãËΩΩÂ•áÂÖîÂà∑Êú∫Âπ∂ÂÆâË£Ö ÈïøÊåâ Èü≥Èáè‰∏ã+ÂÖ≥Êú∫ÈîÆ Áõ¥Âà∞Â±èÂπïÂá∫Áé∞‰∏Ä‰∏™‰øÆÊú∫Âô®‰∫∫ÁöÑÂÖîÂ≠êüêá‰∏∫Ê≠¢ Ê≠§Êó∂ÊâìÂºÄÂ•áÂÖîÂà∑Êú∫ÔºåÂπ∂Â∞ÜÊâãÊú∫Áî®Êï∞ÊçÆÁ∫øËøûÊé•ÁîµËÑë ËøõÂÖ•‚ÄúÂÆûÁî®Â∑•ÂÖ∑‚ÄùÔºåÊâæÂà∞‚ÄúÂà∑ÂÖ•Recovery‚ÄùÈÄâÈ°πÂπ∂ÊâìÂºÄ ÈÄâÊã©‚ÄúÂåπÈÖç„ÄÇ„ÄÇ„ÄÇÂà∑ÂÖ•‚Äù ÔºåÁ≠âÂæÖÂÆÉËá™Âä®ÂÆåÊàêÂç≥ÂèØÔºàÂ¶ÇÊûúÂÆÉÂ§±Ë¥•‰∫ÜÔºåÂè™ËÉΩËá™Â∑±Êâæ‰∫ÜhhhhÔºâ ÂÅáËÆæÂ•áÂÖîÂà∑Êú∫ÂúÜÊª°ÂÆåÊàê‰ªªÂä°,ÈÇ£‰πàÊ≠§Êó∂‰Ω†ÁöÑÊâãÊú∫Âá†‰πéÊ≤°ÊúâÂºÇÊ†∑ÔºåÂèØ‰ª•Ê≠£Â∏∏ÂºÄÊú∫ÔºåÊï∞ÊçÆÁ∫ø‰πüÂèØ‰ª•ÊãîÊéâ‰∫ÜÔºåÂêéÁª≠ÁöÑÊìç‰Ωú‰∏çÂÖ≥ÁîµËÑëÁöÑ‰∫ãÊÉÖ„ÄÇ Â¶ÇÊûúÂÆÉËá™Âä®ËøõÂÖ•RecoveryÁïåÈù¢ÔºåÈÇ£‰πàÈïøÊåâÂÖ≥Êú∫Â∞±ÂèØ‰ª•ÈÄÄÂá∫‰∫Ü„ÄÇÂè¶Â§ñÔºåÊâãÂä®ËøõÂÖ•RecoveryÊ®°ÂºèÁöÑÊñπÂºèÊòØ ÈïøÊåâ Èü≥Èáè‰∏ä+ÂÖ≥Êú∫ÈîÆ Ê≠§Â§Ñ‰∏çÂæó‰∏çËØ¥‰∏Ä‰∏ãËøô‰∏™Á¨¨‰∏âÊñπrecoveryÁöÑÂº∫Â§ß‰πãÂ§Ñ„ÄÇÂÆÉÂº∫Â∞±Âº∫Âú®ÂÆÉÂèØ‰ª•Â∞ÜÊâãÊú∫Ëá™Âä®Ëß£ÂØÜÔºÅÂ∞ΩÁÆ°ËøôÂú®ÊàëÂà∑xposedÂèòÁ†ñÁöÑÊó∂ÂÄôÂ∏Æ‰∫ÜÊàëÂ§ßÂøôÔºöÊàëÂΩìÊó∂Êñá‰ª∂Ê≤°ÊúâÂ§á‰ªΩÔºå‰∏çÁü•ÈÅìÊÄé‰πàÂú®‰∏çÂºÄÊú∫ÁöÑÊÉÖÂÜµ‰∏ãËΩ¨ÁßªÊï∞ÊçÆ„ÄÇÊü•‰∫Ü‰∏Ä‰∫õËµÑÊñôËØ¥ÊòØÂá∫‰∫é‰øùÊä§ÊâãÊú∫ÁöÑÈúÄË¶ÅÔºåÊâãÊú∫ÂÜÖÂÆπÈÉΩÊòØÂä†ÂØÜÁöÑ„ÄÇ‰ΩÜËøô‰πüÊÑèÂë≥ÁùÄÂ¶ÇÊûúÂà´‰∫∫Êç°Âà∞ÊàëÁöÑÊâãÊú∫ÔºåÊåâÁÖßËøô‰∏™ÊïôÁ®ãÂ∞±ÂèØ‰ª•Âú®‰∏çÂºÄÊú∫ÁöÑÊÉÖÂÜµ‰∏ãËØªÂèñÊàëÊâãÊú∫ÈáåÁöÑÊï∞ÊçÆÔºå‰øùÊä§‰ΩúÁî®ÂΩ¢ÂêåËôöËÆæ„ÄÇ 1.3 Âà∑ÂÖ•MagiskÂºÄÊú∫ÂêéÂú®ÊâãÊú∫‰∏äËøõË°å‰∏ãÈù¢ÁöÑÊìç‰ΩúÔºö Âú®ÊâãÊú∫‰∏ä‰∏ãËΩΩMagiskÔºåÂÆÉÊòØ‰∏™zipÂåÖÔºåË¶ÅËÆ∞‰ΩèÂÆÉ‰∏ãËΩΩÂà∞Âì™Èáå‰∫Ü„ÄÇ ÊâãÂä®ËøõÂÖ•RecoveryÊ®°Âºè ÈÄâÊã©Á¨¨‰∏Ä‰∏™ÈÄâÈ°π ÔºöÂÆâË£Ö ÊâæÂà∞‰Ω†‰∏ãËΩΩÁöÑMagisk.zipÂåÖÔºåÂãæÈÄâÂÆÉÂÆâË£Ö„ÄÇ ÂÆåÊàêÂêéÈáçÂêØ Âõæ‰∏≠ÁöÑÊåÇËΩΩÂäüËÉΩÂèØ‰ª•ÊääÂ≠òÂÇ®Âô®ÊåÇËΩΩÂà∞ÁîµËÑë‰∏äËøõË°åËØªÂÜôÊï∞ÊçÆokÔºåËá≥Ê≠§MagiskÂ∑≤ÁªèÂÆâË£ÖÂà∞ÊâãÊú∫‰∏≠‰∫ÜÔºåÊâãÊú∫‰∏≠Ê≠§Êó∂Â∑≤ÁªèÂ§ö‰∫Ü‰∏Ä‰∏™ËÑ∏Ë∞±ÂõæÊ†áÁöÑÂ∫îÁî® Êâæ‰∏çÂà∞‰∏ãËΩΩÂú∞ÊñπÔºü ÊµèËßàÂô®‰∏ãËΩΩÁöÑÊñá‰ª∂ÈÉΩÊòØ‰øùÂ≠òÂú®DownloadÊñá‰ª∂Â§π‰∏≠ Step2:ÂÆâË£ÖÂ§™ÊûÅÔºöÂú®ÊâãÊú∫‰∏äËøõË°å‰ª•‰∏ãÊìç‰ΩúÔºö ‰∏ãËΩΩmagisk-taichi.zip,ÔºàÈìæÊé•ÊòØ‰∏™ÁúãËµ∑Êù•‰∏çÂ§™Ê≠£ËßÑÁöÑÁΩëÁõòÔºå‰ΩÜÊòØËÉΩÁî®Ôºâ ÊâìÂºÄStep1ÂÆâË£ÖÂ•ΩÁöÑAPPÔºåÂ¶ÇÊûúÊ≤°ÊúâÂèØ‰ª•ÊâãÂä®‰∏ãËΩΩ Âú®‰æßËæπÊ†èÈÄâÊã©Ê®°Âùó ÁÇπÂáªÂ±èÂπï‰∏ãÊñπÁöÑÂä†Âè∑ÔºåÊâæÂà∞‰∏ãËΩΩÂ•ΩÁöÑmagisk-tachiÂéãÁº©ÂåÖËøõË°åÂÆâË£Ö„ÄÇ Âà∞ËøôÈáåÂ§™ÊûÅÂ∞±ÂÆâË£ÖÂÆåÊàêÔºåÊâãÊú∫‰∏ä‰ºöÂ§ö‰∏Ä‰∏™Â§™ÊûÅÂõæÊ†áÁöÑÂ∫îÁî®„ÄÇÂ¶ÇÊûúMagiskÂÆâË£ÖÊàêÂäüÔºåÈÇ£‰πàÊâìÂºÄÂêéÁöÑÂ§™ÊûÅÂ∑¶‰∏äËßíÂ∞±ÊòØ Â§™ÊûÅ.Èò≥Ôºå‰∏çÁÑ∂Â∞±ÊòØÂ§™ÊûÅ.Èò¥ Step3:‰∏ãËΩΩÊ®°ÂùóÂπ∂‰ΩøÁî®ÂÆòÊñπÂ§™ÊûÅ‰ΩøÁî®ËØ¥ÊòéÔºöÂ§™ÊûÅÊîØÊåÅÁöÑÊ®°ÂùóÂàóË°®ÔºàÊúâÂäüËÉΩ‰ªãÁªçÔºâ ÊâìÂºÄÂ§™ÊûÅÔºåÊ∑ªÂä†‰Ω†ÈúÄË¶Å‰ΩøÁî® Xposed Ê®°ÂùóÁöÑ APPÔºåÂ¶ÇQQÂà∞Â§™ÊûÅ‰∏≠„ÄÇ ËøõÂÖ•Ê®°ÂùóÁÆ°ÁêÜÔºåÂãæÈÄâ‰∏ä‰Ω†ÈúÄË¶Å‰ΩøÁî®ÁöÑ Xposed Ê®°ÂùóÔºåÂ¶Ç Âéª‰Ω†Â§ßÁà∑ÁöÑÂÜÖÁΩÆÊµèËßàÂô®„ÄÇ Âº∫Âà∂ÂÅúÊ≠¢‰Ω†ÈúÄË¶ÅËßÇÂØüÊïàÊûúÁöÑAPPÔºåÂ¶Ç‰∏äÈù¢ÁöÑQQ„ÄÇ ÊâìÂºÄQQËßÇÂØüÊïàÊûú„ÄÇ ‰∏Ä‰∫õÁü•ËØÜÁÇπÔºö MagiskÁöÑGithub‰ªìÂ∫ìÂ§™ÊûÅÁöÑGithub‰ªìÂ∫ìÂº∫Âà∂ÂÅúÊ≠¢ÁöÑÊñπÊ≥ï‰∏ç‰ºöÔºü]]></content>
      <categories>
        <category>ÊùÇË¥ßÈì∫</category>
      </categories>
      <tags>
        <tag>ÊêûÊú∫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonÈ°πÁõÆ‰∏≠ÁöÑÈÖçÁΩÆÊñá‰ª∂Á≥ªÁªü]]></title>
    <url>%2F2019%2F04%2F06%2Fpython%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[ÂèÇËÄÉÈ°πÁõÆ‰∏∫wukong-robotÔºåËøôÊ¨°ÊòØÂ≠¶‰π†‰∏Ä‰∏ãÈÖçÁΩÆÊñá‰ª∂ÊñπÈù¢ÁöÑÁü•ËØÜ„ÄÇ ËÆæËÆ°ÊÄùË∑Ø1.Áî®‰ªÄ‰πàÊ†ºÂºèÁöÑÊñá‰ª∂‰Ωú‰∏∫ÈÖçÁΩÆÊñá‰ª∂?ÂÖ≥‰∫éÈÖçÁΩÆÊñá‰ª∂Áî®‰ªÄ‰πàÊ†ºÂºèÔºåÁõÆÂâçÂ∑≤ÁªèÊúâÂæàÂ§öÁßçÈÄâÊã©‰∫ÜÔºöini,yaml, json,xmlÔºåHOCON‚Ä¶ ÂÆÉ‰ª¨‰πãÈó¥ÁöÑÊØîËæÉÂíåÁâπÁÇπËßÅËøôÁØáÂíåËøôÁØáÔºåwukong-robotÈÄâÊã©ÁöÑÊòØyamlÊ†ºÂºè‰Ωú‰∏∫ÈÖçÁΩÆÊñá‰ª∂Ôºå ‰ΩÜÊçÆËØ¥HOCONÊòØÁõÆÂâçÊúÄÂ•ΩÁöÑÈÖçÁΩÆÊñá‰ª∂Ê†ºÂºèÔºå‰ª•ÂêéÊàë‰ºöËÄÉËôëÊõøÊç¢ÊéâÂéüÊù•ÁöÑyamlÊ®°Âºè„ÄÇ 2. ÈÖçÁΩÆÊñá‰ª∂Â≠òÊîæÂà∞Âì™ÈáåÔºüËøô‰∏™ÂèØ‰ª•Ëá™Áî±ÈÄâÊã©ÔºåÊÄé‰πàÊñπ‰æøÊÄé‰πàÊù•Â∞±Ë°åÔºåwukong-robotÈ°πÁõÆÊòØÂ≠òÊîæÂà∞‰∫Ü‰∏Ä‰∏™Ëá™ÂÆö‰πâÁöÑÈöêËóèÊñá‰ª∂Â§π‰∏≠ 3. Â¶Ç‰ΩïÂå∫ÂàÜÂºÄÂèëÁéØÂ¢É‰∏éÂ∫îÁî®ÁéØÂ¢ÉÂú®ÂºÄÂèëÁöÑËøáÁ®ã‰∏≠Âõ†‰∏∫Áî®Âà∞‰∫Ü‰∏Ä‰∫õÂú®Á∫øÁöÑÊúçÂä°ÔºåÂÆÉ‰ª¨ÈÉΩÈúÄË¶Å‰∏Ä‰∫õÂêÑÁßçÂêÑÊ†∑ÁöÑsecretkey„ÄÇËøô‰∫õÂ∑≤ÁªèÂ±û‰∫éÊòØÊïèÊÑü‰ø°ÊÅØ‰∫ÜÔºå‰∏çËÉΩÂ∞±ËøôÊ†∑‰∏ä‰º†Âà∞Github„ÄÇÂõ†Ê≠§‰∏Ä‰∏™ÈúÄÊ±ÇÂ∞±ÊòØÂºÄÂèë‰∏™ËøáÁ®ã‰∏≠‰ΩøÁî®ÁöÑÈÖçÁΩÆÊñá‰ª∂‰∏éÂèëÂ∏ÉÁöÑÈÖçÁΩÆÊñá‰ª∂‰∏çËÉΩÁõ∏Âêå„ÄÇ 4. Â¶Ç‰ΩïÂú®‰∏çÈáçÂêØÁöÑÊÉÖÂÜµ‰∏ãÊõ¥Êñ∞ÈÖçÁΩÆwukong-robot ‰ΩøÁî®‰∫ÜwatchdogÊù•ÂÆûÁé∞„ÄÇ ÁÉ≠Êõ¥Êñ∞ÈÖçÁΩÆÊñá‰ª∂ÁöÑÂÆûÁé∞ÈúÄË¶ÅÁ®ãÂ∫èÂèØ‰ª•Ê£ÄÊµãÂà∞ÈÖçÁΩÆÊñá‰ª∂ÁöÑÂèòÂåñÔºåwatchdogÂàöÂ•ΩÂèØ‰ª•Âπ≤Ëøô‰ª∂‰∫ã ‰ª£Á†ÅÊûÑÊÄùconfig.pyÊ®°ÂùóÔºöË¥üË¥£ÂØπÈÖçÁΩÆÊñá‰ª∂ÁöÑÂ¢ûÂà†ÊîπÊü•Êìç‰Ωú config.py12345678910111213141516171819def reload(): passdef init(): passdef do_init(): passdef get_path(): passdef has_path(): passdef has(item): passdef get_text(): passdef dump(configStr): pass config_monitor.py Ê®°ÂùóÔºöÂÆÉÁªßÊâøËá™FileSystemEventHandler „ÄÇFileSystemEventHandler ÂÜÖÁΩÆ‰∫Ü‰∏Ä‰∫õÊñá‰ª∂Á≥ªÁªü‰∫ã‰ª∂ÁöÑÂ§ÑÁêÜÊé•Âè£, Êé•Âè£ÂòõÔºåËá™ÁÑ∂ÂÜÖÂÆπÈÉΩÊòØÁ©∫ÁöÑÔºåËá™Â∑±ÈÄâÂá†‰∏™ÊúâÁî®ÁöÑÂéªÂÆûÁé∞Âç≥ÂèØ12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class FileSystemEventHandler(object): """ Base file system event handler that you can override methods from. """ def dispatch(self, event): """Dispatches events to the appropriate methods. :param event: The event object representing the file system event. :type event: :class:`FileSystemEvent` """ self.on_any_event(event) _method_map = &#123; EVENT_TYPE_MODIFIED: self.on_modified, EVENT_TYPE_MOVED: self.on_moved, EVENT_TYPE_CREATED: self.on_created, EVENT_TYPE_DELETED: self.on_deleted, &#125; event_type = event.event_type _method_map[event_type](event) def on_any_event(self, event): """Catch-all event handler. :param event: The event object representing the file system event. :type event: :class:`FileSystemEvent` """ def on_moved(self, event): """Called when a file or a directory is moved or renamed. :param event: Event representing file/directory movement. :type event: :class:`DirMovedEvent` or :class:`FileMovedEvent` """ def on_created(self, event): """Called when a file or directory is created. :param event: Event representing file/directory creation. :type event: :class:`DirCreatedEvent` or :class:`FileCreatedEvent` """ def on_deleted(self, event): """Called when a file or directory is deleted. :param event: Event representing file/directory deletion. :type event: :class:`DirDeletedEvent` or :class:`FileDeletedEvent` """ def on_modified(self, event): """Called when a file or directory is modified. :param event: Event representing file/directory modification. :type event: :class:`DirModifiedEvent` or :class:`FileModifiedEvent` """ Âú®Ëøô‰∏™È°πÁõÆ‰∏≠‰∏ªË¶ÅÊòØË¶ÅÁî®‰∫éÊ£ÄÊµãÈÖçÁΩÆÊñá‰ª∂ÊîπÂä®ÔºåÂõ†Ê≠§Âè™ÂÆûÁé∞‰∫ÜÊñá‰ª∂‰øÆÊîπ‰∫ã‰ª∂ÁöÑÂ§ÑÁêÜÊé•Âè£„ÄÇÊèèËø∞Ôºö Ê£ÄÊµãÂà∞Êñá‰ª∂‰øÆÊîπÊó∂ÔºåË∞ÉÁî®configÁöÑ reload()ÊñπÊ≥ïÔºåÂπ∂ config_monitor.py1234567891011121314from butler import configfrom watchdog.events import FileSystemEventHandlerclass ConfigMonitor(FileSystemEventHandler): def __init__(self, conversation): FileSystemEventHandler.__init__(self) self._conversation = conversation # Êñá‰ª∂‰øÆÊîπ ‰∫ã‰ª∂Â§ÑÁêÜÈÄªËæëÔºå ËøôÊòØ‰∏™‰∫ã‰ª∂ÔºåÊØèÊ¨°Êñá‰ª∂Ë¢´‰øÆÊîπÂ∞±‰ºöËß¶Âèë def on_modified(self, event): if not event.is_directory: config.reload() self._conversation.reload() ‰∏äÈù¢ÁöÑconfigMonitorËøòÈúÄË¶ÅÈÖçÂêàObsever‰ΩøÁî®ÔºåobserverÂ±û‰∫éwatchdogÔºåÂÖ∂ Schedule()ÊñπÊ≥ïÂèØ‰ª•ÁõëËßÜË∑ØÂæÑÂπ∂Ë∞ÉÁî®Âú®ÁªôÂÆö‰∫ã‰ª∂Â§ÑÁêÜÁ®ãÂ∫è‰∏≠ÊåáÂÆöÁöÑÈÄÇÂΩìÊñπÊ≥ï‰ª•ÂìçÂ∫îÊñá‰ª∂Á≥ªÁªü‰∫ã‰ª∂„ÄÇevent handler ÊåáÁöÑÊòØ ‰∫ã‰ª∂Â§ÑÁêÜÂô®„ÄÅ ‰∫ã‰ª∂Â§ÑÁêÜÁ®ãÂ∫èÔºå‰πüÂè´‰∫ã‰ª∂Âè•ÊüÑ 12345event_handler = ConfigMonitor(_conversation) # FileSystemEventHandlerÂØπË±°Ôºå_observer.schedule(event_handler, constants.get_config_path(), False) # ‰∏∫ËßÇÂØüËÄÖÊåáÂÆöËßÇÂØüÁõÆÂΩï‰ª•Âèä‰∫ã‰ª∂ÁÆ°ÁêÜÂô®_observer.schedule(event_handler, constants.DATA_PATH, False)_observer.start()]]></content>
      <categories>
        <category>PythonÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Áî®pythonÂÆûÁé∞‰∏Ä‰∏™ÁºìÂ≠òÊú∫Âà∂]]></title>
    <url>%2F2019%2F04%2F04%2F%E7%94%A8python%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Â≠¶‰π†‰∏Ä‰∏ãÂÖ≥‰∫éwukong-robotÁöÑÂ£∞Èü≥Êñá‰ª∂ÁºìÂ≠òÊú∫Âà∂ ‰ΩøÁî®ÁºìÂ≠òÁöÑÁõÆÁöÑÁî±‰∫é‰ΩøÁî®ÁöÑÊòØÂú®Á∫øËØ≠Èü≥ÂêàÊàêÔºåÊØè‰∏ÄÊ¨°ÂØπËØùÈÉΩË¶ÅËÆøÈóÆÁΩëÁªúÂêàÊàêËØ≠Èü≥„ÄÇ‰∏∫‰∫ÜÂ∞ΩÈáèÂáèÂ∞ëËÆøÈóÆÁΩëÁªúÁöÑÊ¨°Êï∞ÔºåÂèØ‰ª•ÂØπ‰∏Ä‰∫õÂèØ‰ª•ÂèçÂ§ç‰ΩøÁî®ÁöÑËØ≠Èü≥ËøõË°åÁºìÂ≠ò„ÄÇ ËÆæËÆ°ÊÄùË∑ØÂú®speak()ÂáΩÊï∞‰∏≠Ê∑ªÂä†‰∏Ä‰∏™ Â∏ÉÂ∞îÁ±ªÂûãÁöÑ ‚Äúcache‚Äù ÂèÇÊï∞‰Ωú‰∏∫ÊòØÂê¶Ë¶ÅÂêØÁî®ÁºìÂ≠òÁöÑÊ†áÂøó„ÄÇÂΩì‰º†ÂÖ•‰∏∫TrueÊó∂ÔºåÈ¶ñÂÖà‰ΩøÁî® get_cache(msg)Â∞ùËØïÂëΩ‰∏≠ÁºìÂ≠òÔºåËã•Êú™ÂëΩ‰∏≠ÔºåÂ∞±Ë∞ÉÁî® save_cache(voice, msg)Â∞ÜËøôÂè•ÁºìÂ≠ò‰∏ãÊù•„ÄÇ speak1234567891011voice = ''if utils.get_cache(msg): logger.info("ÂëΩ‰∏≠ÁºìÂ≠òÔºåÊí≠ÊîæÁºìÂ≠òËØ≠Èü≥") voice = utils.get_cache(msg)else: try: voice = self.tts.get_speech(msg) if cache: utils.save_cache(voice, msg) except Exception as e: logger.error('‰øùÂ≠òÁºìÂ≠òÂ§±Ë¥•Ôºö&#123;&#125;'.format(e)) save_cache(voice, msg)ÊèèËø∞Ôºö Ëé∑ÂèñvoiceÊñá‰ª∂ÁöÑÂêéÁºÄÂêçÔºåÂπ∂Â∞ÜmsgÊï£ÂàóÂåñ‰∏∫ÂçÅÂÖ≠ËøõÂà∂Â≠óÁ¨¶‰∏≤ Â∞ÜvoiceÊñá‰ª∂Â≠òÊîæÂà∞È°πÁõÆÁöÑtempÊñá‰ª∂Â§π‰∏≠ÔºåÊñá‰ª∂Âêç‰∏∫msgÁöÑ16ËøõÂà∂ÂìàÂ∏åÂÄº+voiceÁöÑÂêéÁºÄ ‰∏∫‰ªÄ‰πàË¶Å‰ΩøÁî®MD5ÂëΩÂêçÊñá‰ª∂ÔºüÁîüÊàêÁöÑÊëòË¶ÅÈïøÂ∫¶Âõ∫ÂÆöÔºömsgÁöÑÈïøÂ∫¶ÊòØ‰∏çÁ°ÆÂÆöÁöÑÔºå‰ΩÜÊòØÊñá‰ª∂ÂêçÁöÑÈïøÂ∫¶Âç¥ËÉΩÂõ∫ÂÆöÂèØ‰ª•È™åËØÅ‰∏ÄËá¥ÊÄßÔºöÊØè‰∏Ä‰∏™msgÈÉΩÊúâÂõ∫ÂÆöÁöÑÊï£ÂàóÂÄº utils.py12345def saveCache(voice, msg): """ Ëé∑ÂèñÁºìÂ≠òÁöÑËØ≠Èü≥ """ foo, ext = os.path.splitext(voice) md5 = hashlib.md5(msg.encode('utf-8')).hexdigest() shutil.copyfile(voice, os.path.join(constants.TEMP_PATH, md5+ext)) get_cache(msg)ÊèèËø∞Ôºö Ëé∑ÂèñmsgÁöÑmd5ÂìàÂ∏åÂÄº Â∞ÜÂìàÂ∏åÂÄº‰∏éÂÖ∂‰ªñÈÉ®ÂàÜÂêàÊàêË∑ØÂæÑ Ëã•Ë∑ØÂæÑÊâÄÊåáÁöÑÊñá‰ª∂ÊòØÂ≠òÂú®ÁöÑÔºåÈÇ£‰πàËøîÂõûË∑ØÂæÑÔºå‰∏çÁÑ∂ËøîÂõûNone 12345678910def getCache(msg): """ Ëé∑ÂèñÁºìÂ≠òÁöÑËØ≠Èü≥ """ md5 = hashlib.md5(msg.encode('utf-8')).hexdigest() mp3_cache = os.path.join(constants.TEMP_PATH, md5 + '.mp3') wav_cache = os.path.join(constants.TEMP_PATH, md5 + '.wav') if os.path.exists(mp3_cache): return mp3_cache elif os.path.exists(wav_cache): return wav_cache return None Âª∂‰º∏Ëøô‰∏™ÁºìÂ≠òÁöÑÊú∫Âà∂ÁÆÄÊòéÊòìÊáÇÔºåÂ¶ÇÊûúÂÖ∂‰ªñÈ°πÁõÆ‰πüÈúÄË¶Å‰∏Ä‰∏™ÁºìÂ≠òÁ≥ªÁªüÔºåÊàëÊÉ≥‰∏ªË¶ÅÂ∞±ÊòØË¶ÅÊÉ≥Â•ΩÔºö ÁºìÂ≠òÊñá‰ª∂ÊÄé‰πàÂ≠ò‰ªÄ‰πàÊó∂ÂÄôÂ≠òÔºü ÁºìÂ≠òÊñá‰ª∂ÊÄé‰πàÊâæ‰ªÄ‰πàÊó∂ÂÄôÊâæÔºü ‰ª•‰∏ä]]></content>
      <categories>
        <category>PythonÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonÁöÑmodules]]></title>
    <url>%2F2019%2F04%2F01%2Fpython%E7%9A%84modules%2F</url>
    <content type="text"><![CDATA[‰∏Ä‰∏™Ê®°ÂùóÁöÑÂ±ûÊÄßÈÉΩÊúâÈÇ£‰∫õÔºü ‰∏Ä‰∏™moduleÁöÑAttributeÈÉΩÊúâÂì™‰∫õÔºü‰∏∫‰∫Ü‰∫ÜËß£‰∏Ä‰∏™modulel‰∏≠ÁöÑÊâÄË∞ìÂ±ûÊÄßÊúâÂì™‰∫õÔºåÊàëÊñ∞Âª∫‰∫Ü‰∏Ä‰∏™Á©∫ÁöÑmoduleÂèñÂêçtest.py, Âπ∂ÊîæÂà∞‰∫Ü‰∏Ä‰∏™Êñá‰ª∂Â§π‰∏≠,ËøêË°å‰∫Ü‰∏ãÈù¢ÁöÑ‰ª£Á†Å 123456import pkgutilfor finder, name, ispkg in pkgutil.walk_packages(YOURPATH): loader = finder.find_module(name) mod = loader.load_module(name) print(dir(mod)) ÂæóÂà∞‰∫ÜÂ¶Ç‰∏ãÁöÑÁªìÊûú 1[&apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__spec__&apos;] ÂèØÁü•ÊØè‰∏™Ê®°ÂùóÈÉΩÊúâ‰∏äÈù¢ËøôÂá†È°πÊé•ÁùÄÊàëÂú®Ê®°Âùó‰∏≠Ë∞¢‰∫Ü‰∏§‰∏™Á±ª TestÔºåTest1 ,Ëøò import‰∫Ü‰∏™Ê®°Âùóos12345678910111213141516171819import osclass Test(): def __init__(self): a = 2 b = 3 c = 4class Test1(): def __init__(self): a = 1 b = 2 c = 3 def testfunc(self): c=2 ÁªìÊûúÂèòÊàêÔºö 1[&apos;Test&apos;, &apos;Test1&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__spec__&apos;, &apos;os&apos;] Áªº‰∏äÂèØÁü•ÔºöÊ®°ÂùóÁöÑÂ±ûÊÄßÈô§‰∫ÜÂéüÊú¨Â∞±ÊúâÁöÑÂÜÖÁΩÆÂ±ûÊÄß‰πãÂ§ñÔºåËá≥Â∞ëÂåÖÊã¨ÔºöÊ®°Âùó‰∏≠ÂÆö‰πâÁöÑÁ±ªÂêç‰ª•ÂèäÂØºÂÖ•ÁöÑÂåÖÂêç„ÄÇ]]></content>
      <categories>
        <category>PythonÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂΩì‰ª£‰∏≠ÂõΩÊîøÊ≤ªÂà∂Â∫¶Â≠¶‰π†Á¨îËÆ∞]]></title>
    <url>%2F2019%2F03%2F31%2F%E5%BD%93%E4%BB%A3%E4%B8%AD%E5%9B%BD%E6%94%BF%E6%B2%BB%E5%88%B6%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ÊàëÂèóÂ§ü‰∫ÜÂú®ÂæÆÂçöÊàñËÄÖÂêåÂ≠¶ÊúãÂèãÂè£‰∏≠Á¢éÁâáÂåñÁöÑÂÖ≥‰∫é‰∏≠ÂõΩÊîøÊ≤ªÂà∂Â∫¶ÁöÑÂêÑÁßçËØ¥Ê≥ïÔºåÂê¨‰∫Ü‰πüÊòØ‰∏ÄÁü•ÂçäËß£Ê≤°Êúâ‰ªÄ‰πàÁî®„ÄÇÊàëÊÉ≥‰Ωú‰∏∫‰∏Ä‰∏™‰∏≠ÂõΩÂÖ¨Ê∞ëÔºåÊúâÂøÖË¶Å‰∫ÜËß£Ëá™Â∑±ÂõΩÂÆ∂ÁöÑÊîøÊ≤ªÂà∂Â∫¶„ÄÇÂàöÂ•ΩÂú®BÁ´ôÊâæÂà∞‰∫ÜÂ§çÊó¶Êµ¶ÂÖ¥Á•ñÊïôÊéà‰∏ªËÆ≤ÁöÑ„ÄäÂΩì‰ª£‰∏≠ÂõΩÊîøÊ≤ªÂà∂Â∫¶„ÄãÔºåÊâìÁÆóÁ≥ªÁªüÁöÑÂ≠¶‰π†‰∏Ä‰∏ã„ÄÇ`]]></content>
      <categories>
        <category>ÊîøÊ≤ª</category>
      </categories>
      <tags>
        <tag>ÊîøÊ≤ª</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂÖ≥‰∫éÁî®pythonÂÆûÁé∞ÂèØÊèíÊãîÊû∂ÊûÑ]]></title>
    <url>%2F2019%2F03%2F28%2F%E5%85%B3%E4%BA%8E%E7%94%A8python%E5%AE%9E%E7%8E%B0%E5%8F%AF%E6%8F%92%E6%8B%94%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[ÊÑüËßâÊèí‰ª∂Ëøô‰∏úË•øÊå∫ÊúâÊÑèÊÄùÁöÑÔºåÊâæ‰∏™È°πÁõÆÂ≠¶‰π†‰∏Ä‰∏ã„ÄÇËøôÁØáÊòØÂ≠¶‰π†Á¨îËÆ∞ÔºåÂ≠¶ÁÇπÂÜôÁÇπÂêß„ÄÇ Êèí‰ª∂ËÆæËÆ°Âú®wukong-robotÈ°πÁõÆ‰∏≠ÔºåÊâÄË∞ìÊèí‰ª∂‰πüÂ∞±ÊòØ‰∏Ä‰∏™ÊúâÁùÄÁâπÊÆäÊ†ºÂºèÁöÑpyÊñá‰ª∂„ÄÇËøô‰∏™Ê≤°‰ªÄ‰πàÁ°¨ÊÄßË¶ÅÊ±ÇÔºåÈÉΩÊòØËá™Â∑±ËßÑÂÆöÁöÑ Êèí‰ª∂ÁöÑÂä†ËΩΩÊôÆÈÄöÊ®°ÂùóorÊèí‰ª∂ÔºüÂ¶Ç‰ΩïÂà§Êñ≠‰∏Ä‰∏™Ê®°ÂùóÊòØÊèí‰ª∂ËøòÊòØÊôÆÈÄöÊ®°ÂùóÔºüÈ¶ñÂÖàÈúÄË¶ÅÂú®È°πÁõÆ‰∏≠ÊåáÂÆö‰∏Ä‰∏™Êñá‰ª∂Â§πÊù•Â≠òÊîæÊèí‰ª∂ÔºåÊØîÂ¶ÇÂèñÂêçÔºöPlugins ;ÁÑ∂ÂêéÊâÄÊúâÁöÑÊèí‰ª∂ÈÉΩË¶ÅËá™ÂÆö‰πâ‰∏Ä‰∏™ÁâπÂÆöÁöÑÁ±ª, ÊØîÂ¶ÇÂÆö‰πâ‰∏∫ Plugin. ÈÄöËøá‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÊàëÂèØ‰ª•ÈÅçÂéÜÊüê‰∏™Ë∑ØÂæÑ‰∏≠ÊâÄÊúâÁöÑÊ®°ÂùóÔºåÂπ∂ÊâæÂá∫Êèí‰ª∂Ôºö plugin_loader.py1234567891011121314151617181920212223for finder, name, ispkg in pkgutil.walk_packages(locations): # try: loader = finder.find_module(name) mod = loader.load_module(name) except Exception: logger.warning("Êèí‰ª∂ &#123;&#125; Âä†ËΩΩÂá∫ÈîôÔºåË∑≥Ëøá".format(name), exc_info=True) continue if not hasattr(mod, 'Plugin'): # ÂèØ‰ª•ÊòØ Ê®°Âùó‰∏≠Êúâ‰∏Ä‰∏™ÂêçÂè´PluginÁöÑÁ±ª„ÄÇÊàñËÄÖÊòØ Ê®°Âùó‰∏≠ÂÜô‰∫Ü import Plugin ËøôÂè•Âà§Êñ≠ÈÉΩ‰ºöÈÄöËøáÁöÑ logger.info("Ê®°Âùó &#123;&#125; ÈùûÊèí‰ª∂ÔºåË∑≥Ëøá".format(name)) continue # plugins run at query Âú®Êü•ËØ¢Êó∂ËøêË°å plugin = mod.Plugin(con) # ÊääËøô‰∏™Êèí‰ª∂ÂÆû‰æãÂåñ if plugin.SLUG == 'AbstractPlugin': # ËøôË°å‰ª£Á†ÅÊúâ‰ªÄ‰πàÁî®Ôºü ÁåúÊµãÊòØÈò≤Ê≠¢Êèí‰ª∂ÂøòÂÜôSLUG? ÊÑüËßâÊ≤°‰ªÄ‰πàÂøÖË¶Å plugin.SLUG = name if issubclass(mod.Plugin, AbstractPlugin): #Âä†ËΩΩÊèí‰ª∂ Ëøô‰∏™ÂáΩÊï∞ÁöÑ‰ΩúÁî®ÊòØ Âà§Êñ≠ÂΩìÂâçÊèí‰ª∂ÊòØ‰∏çÊòØÁªßÊâøËá™Abstract logger.info("Êèí‰ª∂ &#123;&#125; Âä†ËΩΩÊàêÂäü ".format(name)) _plugins_query.append(plugin) else: logger.info("Êèí‰ª∂ &#123;&#125; Áà∂Á±ª‰∏çÂêàÊ†º ".format(name)) slugÂ±ûÊÄßÊòØÂú®Âü∫Á±ª‰∏≠ÂÆö‰πâÂ•ΩÁöÑÔºåÂ≠êÁ±ªÂ∫îËØ•‰ΩøÁî®Êñ∞ÁöÑÂÄºË¶ÜÁõñÂÆÉ„ÄÇËøôÈáåÊòØÊèí‰ª∂ÂêçÔºåÁ®ãÂ∫è‰ºöÊ†πÊçÆslugÂú®PluginÂàóË°®ÈáåÊü•ÊâæÁöÑ walk_packages(): ÂÆÉÁöÑËÉΩÂäõÊòØÊ†πÊçÆË∑ØÂæÑÊêúÁ¥¢Ë∑ØÂæÑÂåÖ‰∏≠ÊâÄÊúâÁöÑÊ®°Âùó(ÊàëËßâÂæóÂ∫îËØ•ÊòØÊâÄÊúâÂÜÖÂÆπ)Âπ∂ ËøîÂõûmodule_finder, nameÂíåispkg„ÄÇ module_finder: ÂÆÉÂèØ‰ª•ËøîÂõûÊåáÂÆöÊ®°ÂùóÁöÑloaderÂØπË±°ÔºåloaderÂØπË±°ÁöÑload_module()ÊñπÊ≥ïÂèàÂèØ‰ª•ËøîÂõûmoduleÂØπË±°„ÄÇ name:Â∞±ÂæàÂ•ΩÁêÜËß£‰∫ÜÔºåÂÆÉÂ∞±ÊòØÊ®°ÂùóÁöÑÂêçÂ≠óÔºåÊØîÂ¶ÇÊúâÊ®°Âùótest.py„ÄÇnameÂ∞±ÊòØ‚Äùtest‚Äù ispkg: ‰πüÂ∞±ÊòØÂ≠óÈù¢ÊÑèÊÄùÔºåÂèçÂ∫îËøô‰∏™‰∏úË•øÊòØ‰∏çÊòØ‰∏™ÂåÖ„ÄÇ‰∏™‰∫∫ÁåúÊµãÊòØÂõ†‰∏∫ÂåÖÈáå‰πüÂèØËÉΩ‰ºöÂ•óÁùÄÂåÖÔºåÊâÄ‰ª•ÈúÄË¶ÅÂàÜÊã£‰∏Ä‰∏ã„ÄÇ hasattr(): ÂÆÉÂèØ‰ª•Âà§Êñ≠‰∏Ä‰∏™ÂØπË±°‰∏≠ÊòØÂê¶ÊúâÊüê‰∏™Â±ûÊÄßÔºàAttributeÔºâ„ÄÇ ‰∏Ä‰∏™Á©∫Ê®°ÂùóÁöÑÂ±ûÊÄßÂåÖÊã¨Ôºö1[&apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__spec__&apos;] Êé•ÁùÄÊàëÂú®Ê®°Âùó‰∏≠ÂÜô‰∫Ü‰∏§‰∏™Á±ª TestÔºåTest1 ,Ëøò import‰∫Ü‰∏™Ê®°ÂùóosÔºåËøôÊó∂Â±ûÊÄßÂ∞±ÂèòÊàê1[&apos;Test&apos;, &apos;Test1&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__spec__&apos;, &apos;os&apos;] Áî±Ê≠§ÊàëÊâÄÂæóÂà∞ÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØÊòØÔºötestÊ®°Âùó‰∏≠ÁöÑÁ±ªÂêçÂíåimportÁöÑÊ®°ÂùóÂêçÊòØÂåÖÂê´Âú®Â±ûÊÄßÂàóË°®ÈáåÁöÑ„ÄÇ ÂÖ∑‰ΩìÊù°‰ª∂Âú®‰∏äÈù¢ÁöÑ‰ª£Á†Å‰∏≠ÔºåÊ®°ÂùóÈúÄË¶ÅÊª°Ë∂≥‰ª•‰∏ãÁöÑÊù°‰ª∂ÊâçËÉΩË¢´ËØÜÂà´‰∏∫Êèí‰ª∂Ôºö Ê®°ÂùóÂåÖÂê´Â±ûÊÄß‚ÄùPlugin‚Äù, ËøôÈáåÊòØÊòØÊåáÊ®°Âùó‰∏≠ÊúâPluginÁ±ª ‰∏îPluginÁ±ªÁªßÊâøËá™AbstractPluginÁ±ª Á±ª‰∏≠ÊúâSLUGÂ±ûÊÄß„ÄÇ Ê≤°ÊúâÁöÑËØùÂ∫îËØ•‰ºöÊä•ÂºÇÂ∏∏ÔºåËøôÈáåÊ≤°Êúâtry catchÔºåÂÖ®Èù†Êèí‰ª∂ÂºÄÂèëËÄÖÈÅµÂÆàÁ∫¶ÂÆö Êèí‰ª∂ÁöÑËøêË°åÂú®Êú¨È°πÁõÆ‰∏≠ÔºåÊèí‰ª∂ÁöÑËøêË°åÈÉΩÈù†brain.py. brainÊ®°ÂùóÁöÑËøêË°åÊòØË¢´Âä®ÁöÑÔºåÁî®Êà∑ÊØèËØ¥‰∏ÄÊ¨°Êåá‰ª§Ôºåbrain.pyÁöÑqueryÊñπÊ≥ïÂ∞±‰ºöËøêË°å‰∏ÄÊ¨°„ÄÇËøêË°åËøáÁ®ã‰∏≠ÂÆÉÈÅçÂéÜÊèí‰ª∂listÔºåÂ∞ùËØïÊåâÁÖßÊüêÁßçËßÑÂàôÊääÂëΩ‰ª§‰∏éÊèí‰ª∂ÂåπÈÖç„ÄÇÂΩìÁî®Êà∑ÂèëÂá∫ÁöÑÂëΩ‰ª§ÂëΩ‰∏≠Êèí‰ª∂Êó∂ÔºåÂ∞±Ë∞ÉÁî®ËØ•Êèí‰ª∂ÁöÑhandle()ÊñπÊ≥ïÔºåÊèí‰ª∂ÁöÑÂäüËÉΩ‰πüÂ∞±ÊòØÂú®handle()‰∏≠ËøõË°åÂÆûÁé∞ÁöÑ„ÄÇÂÖ≥ÈîÆ‰ª£Á†ÅÂ¶Ç‰∏ãÔºöbrain.py12345678910111213141516171819202122232425262728for plugin in self.plugins: # ÊãøÁùÄ Áî®Êà∑ÁöÑËØùÂíåNLUËØÜÂà´ÁªìÊûú ËØïÊé¢ÊâÄÊúâÊèí‰ª∂ if not plugin.isValid(text, parsed): continue # ÂëΩ‰∏≠Âêé ËøõÂÖ•‰ΩøÁî®ÈÄªËæë logger.info("'&#123;&#125;' ÂëΩ‰∏≠ÊäÄËÉΩ &#123;&#125;".format(text, plugin.SLUG)) if plugin.IS_IMMERSIVE: # Âà§Êñ≠ÊòØ‰∏çÊòØÊ≤âÊµ∏Ê®°Âºè self.conversation.setImmersiveMode(plugin.SLUG) continueHandle = False try: self.handling = True continueHandle = plugin.handle(text, parsed) # ËøõÂÖ•Â§ÑÁêÜÈÄªËæë self.handling = False except Exception: logger.critical('Failed to execute plugin', exc_info=True) reply = u"Êä±Ê≠âÔºåÊèí‰ª∂&#123;&#125;Âá∫ÊïÖÈöú‰∫ÜÔºåÊôöÁÇπÂÜçËØïËØïÂêß".format(plugin.SLUG) self.conversation.speak(reply, plugin=plugin.SLUG) else: logger.debug("Handling of phrase '%s' by " + "plugin '%s' completed", text, plugin.SLUG) finally: if not continueHandle: return Truelogger.debug("No plugin was able to handle phrase &#123;&#125; ".format(text))return False Ê≥®Ôºö handleÊñπÊ≥ïËøîÂõû None]]></content>
      <categories>
        <category>PythonÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÁªôGithub‰ªìÂ∫ìÈáåÊèê‰∫§‰∫ÜÊïèÊÑü‰ø°ÊÅØ]]></title>
    <url>%2F2019%2F03%2F28%2F%E7%BB%99Github%E4%BB%93%E5%BA%93%E9%87%8C%E6%8F%90%E4%BA%A4%E4%BA%86%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[Êò®Â§©Êôö‰∏äÊúÄÂêéÊèê‰∫§‰∫Ü‰ª£Á†Å‰πãÂêéGihubÂèëÈÇÆ‰ª∂ËØ¥Ê£ÄÊµãÂà∞‰∫Ü‰ª£Á†Å‰∏≠ÁöÑÊïèÊÑü‰ø°ÊÅØ„ÄÇËøôÊó∂ÂÄôÊàëÁåúÊÉ≥Ëµ∑Êù•ÊàëÂõæÊñπ‰æøÁõ¥Êé•ÂÜô‰ª£Á†ÅÈáåÊèê‰∫§‰∫Ü:-( ÊîπÔºÅÂ¶ÇÊûúÂè™ÊòØÂØÜÁ†ÅÔºåÂØÜÈí•ËøôÁßç‰ø°ÊÅØ„ÄÇÊàëÂàöÂºÄÂßãÊÉ≥Âà∞Êää‰ª£Á†ÅÊîπÊéâÈáçÊñ∞Êèê‰∫§Â•Ω‰∫ÜÔºå‰ΩÜÊòØÂèà‰∏ÄÊÉ≥ÔºåÂéüÊù•Êèê‰∫§ÁöÑÊòØÊúâÂéÜÂè≤ÁöÑÔºå‰∏çËÉΩÂè™Êîπ‰ª£Á†Å„ÄÇÈÇ£‰πàÊ≠£Á°ÆÊìç‰ΩúÂ∞±ÊòØÊääÂØÜÁ†ÅÊîπ‰∫ÜÂØÜÈí•Êç¢‰∫ÜÂÖàÔºåÁÑ∂ÂêéÂÜçÊîπ‰ª£Á†Å Á¨¨‰∏ÄÊó∂Èó¥ÂéªÊîπÂØÜÁ†ÅÔºåÂà†ÂØÜÈí•Â∞±Â•Ω‰∫Ü„ÄÇ:-(ÊàëÂ∞±ÊòØËøôÁßçÊÉÖÂÜµÔºåÁõ¥Êé•ÊääÂØÜÈí•Áõ¥Êé•ÂÜô‰ª£Á†ÅÈáåÈù¢‰∫Ü„ÄÇ]]></content>
      <categories>
        <category>githubÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepinÁ≥ªÁªüÊäòËÖæÁ¨îËÆ∞]]></title>
    <url>%2F2019%2F03%2F25%2FDeepin%E7%B3%BB%E7%BB%9F%E6%8A%98%E8%85%BE%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ÊäòËÖædeepinÁ≥ªÁªüÁöÑËµ∑Âõ†ÊòØÁúã‰∫ÜÁü•‰πéÁöÑ‰∏ÄÁØáÂÖ≥‰∫éUbuntuÂíåDeepinÂì™‰∏™Êõ¥ÊúâÂâçÈÄîÁöÑËÆ®ËÆ∫Ë¥¥„ÄÇ‰∏ÄÂºÄÂßãË£Ö‰∫ÜËôöÊãüÊú∫‰ΩìÈ™åÔºåËÆ©Êàë‰∏ãÂÆöÂÜ≥ÂøÉË£ÖÂèåÁ≥ªÁªüÁöÑÊòØÊàëÂèëÁé∞ÂÆÉÁöÑÂ∫îÁî®ÂïÜÂ∫óÂ§™ÁâõÈÄº‰∫ÜÔºåÁî©‰∫ÜUbuntuÂá†Êù°Ë°óÁöÑÁâõÈÄºÔºö Âü∫Êú¨‰ªãÁªçÁΩëÁªúÈÄöËÆØÊñπÈù¢QQ,TIM,ÂæÆ‰ø°‰∏ÄÂ∫î‰ø±ÂÖ®„ÄÇÂπ∂‰∏îÁâàÊú¨ÈùûÂ∏∏Êñ∞Ôºå‰ª•QQ‰∏∫‰æãÔºö ‰∏ÄÁõ¥Ê≤°ÊúâÂÖ≥Ê≥®ËøáLinuxÊ°åÈù¢ÂèëÂ±ïÁöÑÊàëÁÆÄÁõ¥ÊÉäÂëÜ‰∫Ü„ÄÇ‰πãÂâçÁî®ÔºµbuntuÁöÑÊó∂ÂÄô‰∏ãËΩΩËøáÁöÑÔº±Ôº±ÁâàÊú¨ËÄÅÂπ∂‰∏îÈ¢ëÁπÅÂ¥©Ê∫ÉÂá†‰πé‰∏çËÉΩÁî®ÔºåËøô‰∏™Âü∫Êú¨‰∏ÄÂàáÊ≠£Â∏∏„ÄÇ ÁºñÁ®ãÂ∑•ÂÖ∑ÊñπÈù¢Â∫îÁî®ÂïÜÂüé‰πüÊòØÂçÅÂàÜ‰∏∞ÂØåÔºö ÂÖ≥‰∫éÂΩïÂ±èÂíåGIFËá™Â∏¶ÁöÑÂΩïÂ±èËΩØ‰ª∂ËõÆÂ•ΩÁî®ÁöÑÔºå‰øùÂ≠òÁöÑÊó∂ÂÄôÂèØ‰ª•ÈÄâÊã©GIFÂíåËßÜÈ¢ë‰∏§ÁßçÊ†ºÂºè„ÄÇÊ≠£Â•ΩÊàë‰πüÊ≤°ÊúâÊâæÂà∞ÂÖ∂‰ªñÂ•ΩÁî®ÁöÑÂΩïÂ±èËΩØ‰ª∂„ÄÇÊàëÂú®windows10 ‰∏äÁî®ÁöÑ‰∏Ä‰∏™ÂΩïÂà∂GIFÁöÑÁ•ûÂô®Âè´ScreentoGIF, ‰ΩÜÊòØÂæàÈÅóÊÜæÂÆÉÂè™ÊúâwindowsÁâà„ÄÇ ÂÖ≥‰∫éÂ≠ó‰ΩìDeepinËá™Â∏¶Â≠ó‰Ωì‰∏çÂ§ö‚Ä¶ÈÉΩÊòØ‰∏Ä‰∫õÂºÄÊ∫êÂ≠ó‰ΩìÔºåÂ∏∏Áî®ÁöÑÈªë‰Ωì„ÄÅÂÆã‰Ωì‰∏Ä‰∏™ÈÉΩÊ≤°Êúâ„ÄÇ Ëá™Â∏¶‰∫Ü‰∏Ä‰∏™Â≠ó‰ΩìÂÆâË£ÖÂô®, ÊîØÊåÅÂçïÁã¨ÂíåÊâπÈáèÂØºÂÖ•„ÄÇÁî±‰∫éÂÆâË£ÖÁöÑÊòØÂèåÁ≥ªÁªüÔºåwindowsÁöÑÂàÜÂå∫ÊòØÂèØËßÅÁöÑ„ÄÇÊääÂéüÊù•Windows‰∏ãÁöÑÂ≠ó‰ΩìÂ§çÂà∂ËøáÊù•ÂØºÂÖ•Âç≥ÂèØ„ÄÇ ÂÆâË£ÖÂ≠ó‰ΩìÂá†‰πéÊòØÂøÖÈ°ªÁöÑÔºåÁî®‰∫ÜWPSÂ∞±Áü•ÈÅì‰∫ÜÔºåÊ≤°ÊúâÈªë‰ΩìÂíåÂÆã‰ΩìÊòØ‰ªÄ‰πàÊÑüËßâ„ÄÇ]]></content>
      <categories>
        <category>linuxÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Âà∂‰ΩúraspbianÊúÄÂ∞èÈïúÂÉè]]></title>
    <url>%2F2019%2F03%2F19%2F%E5%88%B6%E4%BD%9Craspbian%E6%9C%80%E5%B0%8F%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[‰∏∫‰∫ÜÈÅøÂÖçÂèëÁîüÊÑèÂ§ñ‰ªéÂ§¥ÂÜçÊù•ÔºåÊàëËßâÂæóËøòÊòØÂ§á‰ªΩ‰∏Ä‰∏ãÁöÑÂ•Ω„ÄÇÊü•‰∫ÜÂæàÂ§öÁΩë‰∏äËµÑÊñôÔºåÂèëÁé∞Âπ∂‰∏çÊòØÁî®‰∏™ËΩØ‰ª∂ÁÆÄÂçïÁ≤óÊö¥Â∞±ËÉΩÂ§á‰ªΩÁöÑ„ÄÇÂõ†Ê≠§ÊàëËßâÂæóÊúâÂøÖË¶ÅÂ≠¶‰π†‰∏Ä‰∏ã„ÄÇ]]></content>
      <categories>
        <category>Ê†ëËéìÊ¥æ</category>
      </categories>
      <tags>
        <tag>Ê†ëËéìÊ¥æ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pycharm2018_2ËøúÁ®ãÂºÄÂèëÊ†ëËéìÊ¥æÂ∞èËÆ∞]]></title>
    <url>%2F2019%2F03%2F19%2FPycharm2018_2%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ËÄÉËôëÂà∞ËøúÁ®ãË∞ÉËØïÁöÑÈúÄË¶ÅÔºåÊàëËøòÊòØÊîæÂºÉvscodeÂéª‰ΩøÁî®Pycharm‰∫Ü„ÄÇÂ∑•ÂÖ∑ËøòÊòØ‰∏ì‰∏öÁöÑÂ•Ω„ÄÇ‰∏ãÈù¢‰∏ªË¶ÅËØ¥‰∏Ä‰∏ãpycharmÊÄé‰πàËøûÊé•pipenvÂàõÂª∫ÁöÑËôöÊãüÁéØÂ¢É„ÄÇ ÂÖ≥‰∫épycharmÊÄé‰πàËøúÁ®ãÂºÄÂèëÔºåÊàëÊü•‰∫ÜÂæàÂ§öËµÑÊñô ÂØπÊàëÊúÄÊúâÁî®ÁöÑÊòØËøô‰Ωç‰ΩúËÄÖÁöÑÊñáÁ´†:pythonÂ≠¶‰π†Á¨îËÆ∞-PycharmËøúÁ®ãËøûÊé•ÊúçÂä°Âô®ÁöÑPythonËôöÊãüÁéØÂ¢É(Virtualenv)‰πãÂâçÁúãÁöÑÂæàÂ§öÊñáÁ´†Ê≤°ÊúâÊèêÊÄé‰πàËøûÊé•pythonËôöÊãüÁéØÂ¢ÉÔºåËøôÁØáÊñáÁ´†ËÆ©ÊàëÊäì‰Ωè‰∫ÜÈáçÁÇπÔºöË¶ÅÊâæÂà∞ËôöÊãüÁéØÂ¢É‰∏ãÁöÑpythonË∑ØÂæÑÔºåÁêÜËß£‰∫ÜËøô‰∏™‰πãÂêéÊàëÂú®Ëøô‰∏ÄÊ≠•Â∞±‰∏éÊïôÁ®ã‰∏çÂêå‰∫ÜÔºö ÈÄöËøáËøôÁØáÊñáÁ´†ÊàëÁü•ÈÅìpipenvÊâÄÂàõÂª∫ÁöÑËôöÊãüÁéØÂ¢ÉÂú®Âì™Èáå‰∫Ü„ÄÇÊàëÊâÄ‰ΩøÁî®ÁöÑinterpreter Ë∑ØÂæÑÂ¶Ç‰∏ãÔºö1/home/pi/.local/share/virtualenvs/smhonraspi-8yb1O5dD/bin/python3 ‰ΩøÁî®ËôöÊãüÁéØÂ¢ÉÂèØ‰ª•ËÆ©ÊàëÊõ¥Â•ΩÁöÑÁÆ°ÁêÜÂÆâË£ÖÁöÑÂåÖÔºåÂπ∂‰∏î‰øÆÊîπÂåÖÂÜÖÊñá‰ª∂‰∏çÂÜçÈúÄË¶ÅrootÊùÉÈôê„ÄÇÊàëÂæÄÈáåÈù¢Â§çÂà∂Êñá‰ª∂ÁöÑÊó∂ÂÄôÂèØ‰ª•Áõ¥Êé•Áî®ÂèØËßÜÂåñÁöÑÂ∑•ÂÖ∑ÂæÄÈáåÈù¢ÊãñÔºåËÄå‰∏çÂøÖ‰ΩøÁî®sudo cp ÂëΩ‰ª§ ‰∏Ä‰∫õÈóÆÈ¢òÂÆÉÊ≤°ÊúâÂäûÊ≥ïË∑≥ËΩ¨Âà∞ pocketsphinxÁöÑÂÆö‰πâÂéüÂõ†ÊòØÊàëÊ≤°Êúâ download from ÁúüÊòØÊô∫Èöú pycharm‰ºöÊää‰Ω†ËøúÁ®ã‰∏ªÊú∫‰∏äÊâÄÊúâÁöÑÂåÖÈÉΩÊãâÂà∞‰Ω†Êú¨Âú∞‰∏ªÊú∫‰∏äÈù¢‰∏Ä‰ªΩÔºåÊâÄ‰ª•Â¶ÇÊûú‰Ω†Âú®ËøúÁ®ã‰∏ªÊú∫‰∏äË£Ö‰∫ÜÊñ∞ÁöÑÂåÖÔºå‰∏ÄÂÆöË¶ÅËÆ∞Âæódownload‰∏Ä‰∏ãÔºå‰∏çË¶ÅÂÉèÊàëËøô‰πàÊô∫Èöú„ÄÇÂè≥ÈîÆ edit remote file ‰ΩøÁî®ËØ≠Èü≥ÂêàÊàêapi Êí≠ÊîæÈü≥‰πêÂá∫ÈóÆÈ¢ò ÈúÄË¶ÅÂÆâË£Ö o]]></content>
      <categories>
        <category>Ê†ëËéìÊ¥æ</category>
      </categories>
      <tags>
        <tag>Ê†ëËéìÊ¥æ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RaspbianLiteÁâà‰ΩøÁî®Â∞èËÆ∞]]></title>
    <url>%2F2019%2F03%2F19%2FRaspbianLite%E7%89%88%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[‰ªäÂ§©ÁªôÊ†ëËéìÊ¥æÂÆâË£Ö‰∫Üraspbian liteÁâàÁ≥ªÁªü„ÄÇÂéüÊú¨ÁöÑwith desktop ÁâàÊú¨‰π±‰∏ÉÂÖ´Á≥üÁöÑËΩØ‰ª∂ÂæàÂ§öÔºå‰ΩìÁßØ‰πüÂæàÂ§ß„ÄÇ‰∏ãÈù¢ËÆ∞ÂΩï‰∏Ä‰∏ã‰ªéÂÆâË£ÖÂà∞‰ΩøÁî®ÁöÑËøáÁ®ãÂíå‰ΩìÈ™å„ÄÇ Á≥ªÁªüÂÆâË£Ö‰∏ãËΩΩÈìæÊé•‰∏ãËΩΩÁöÑÊó∂ÂÄôÊ≥®ÊÑè‰∏çË¶ÅÈÄâÈîôÔºåÂ¶ÇÂõæÔºöÂÖ∑‰ΩìÂÆâË£ÖËøáÁ®ãÁï• ÂºÄÊú∫ÂêéÁöÑÂáÜÂ§áLiteÁâàÁöÑraspbianÊòØÊ≤°ÊúâÊ°åÈù¢ÁöÑÔºåÊâÄÊúâÁöÑÈÖçÁΩÆÈÉΩÂè™ËÉΩÈÄöËøáÂëΩ‰ª§Ë°å‰∫ÜÔºåÊ≠£Â•ΩÂÜçÁªÉ‰π†‰∏Ä‰∏ã„ÄÇ‰∏∫‰∫ÜÁªôÂêéÁª≠ÂÆâË£ÖËΩØ‰ª∂ÂíåpythonÂåÖÂÅöÂáÜÂ§áÔºåÈúÄË¶ÅÂÖàÂÆåÊàê‰∏ãÈù¢ÁöÑÊ≠•È™§ Êõ¥Êç¢ÈªòËÆ§ÂØÜÁ†ÅËøûÊé•WiFiÂíåÊøÄÊ¥ªSSHÂàöË£ÖÂ•ΩÁöÑÊ†ëËéìÊ¥æÈªòËÆ§ÊòØÂÖ≥Èó≠SSHÁöÑÔºåÂøÖÈ°ªËøû‰∏äÊòæÁ§∫Âô®ÂíåÈîÆÁõòÂéªÊîπÈÖçÁΩÆ„ÄÇÂêåÊó∂ÂèØ‰ª•È°∫‰æøÊääÂÖ∂‰ªñË¶ÅÁî®ÁöÑÈÄâÈ°πÊâìÂºÄÊØîÂ¶ÇSPIÔºåGPIO¬∑¬∑¬∑ÂëΩ‰ª§Ôºöraspi-config ÁªôAPTÊç¢Ê∫ê Ë¶ÅÊ≥®ÊÑèÁ≥ªÁªüÁâàÊú¨ÔºåÊúÄÊñ∞ÁâàÂ∫îËØ•Áî®stretchÁöÑÊ∫ê„ÄÇÁΩë‰∏äÂ•ΩÂ§öÊïôÁ®ãÈÉΩËøáÊó∂‰∫ÜÔºåËøòÊúâÁî®wheezyÂíåjessieÁâàÊú¨ÁöÑÂë¢ ÂÆòÊñπÊîØÊåÅÁöÑÊ∫êÂàóË°®ÊàëÁî®ÁöÑÊòØÊ∏ÖÂçéÁöÑÊ∫ê Âõ†‰∏∫Ê∏ÖÂçéÁöÑÁΩëÁ´ôÂÅöÁöÑÂ•ΩÁúãÂæàÈù†Ë∞±ÁöÑÊ†∑Â≠ê Âõ†‰∏∫raspbianÊ≤°ÊúâÈ¢ÑË£ÖvimÔºåviÂèà‰∏çÂ•ΩÁî®ÔºåÊâÄ‰ª•ÊàëÁî®nanoÊâìÂºÄÈÖçÁΩÆÊñá‰ª∂ÔºåÁé∞Âú®‰øÆÊîπÁ¨¨‰∏Ä‰∏™ÈÖçÁΩÆÊñá‰ª∂ sources.listÔºö1sudo nano /etc/apt/sources.list ÁÑ∂ÂêéÊää‰∏ãÈù¢‰∏§Ë°åÊîæËøõÂéªÔºåÊñá‰ª∂‰∏≠ÂéüÊù•ÁöÑÂÜÖÂÆπË¶Å‰πàÂä†#Ê≥®ÈáäÔºåË¶Å‰πàÂà†Êéâ„ÄÇ ‰øùÂ≠òÂëΩ‰ª§‰∏∫ ctrl+O ÔºåÂÜçÊåâ‰∏ÄÊ¨°enterÂç≥ÂèØ„ÄÇÂÆåÊàêÂêéÊåâ ctrl+XÈÄÄÂá∫12deb https://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpideb-src https://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi Â∞èÁªÜËäÇÔºöÊàëËøôÈáåÁöÑÊ∫êÁî®ÁöÑÊòØhttpsÂçèËÆÆ„ÄÇ‰∏Ä‰∫õËÄÅÁâàÊú¨ÁöÑÊïôÁ®ãÈÉΩÊòØhttpÂçèËÆÆÔºåÁÑ∂Âêé‰ºöÂëäËØâ‰Ω†Â¶ÇÊûúÈúÄË¶ÅhttpsÂçèËÆÆÂàôÈúÄË¶Å‰∫ãÂÖàÂÆâË£Ö apt-transport-httpsÂåÖ„ÄÇ‰∫ãÂÆûÊòØÊ†ëËéìÊ¥æÂ∑≤ÁªèÂÜÖÁΩÆ‰∫ÜËøô‰∏™ÂåÖÔºå‰∏çÈúÄË¶ÅËá™Â∑±Ë£Ö‰∫Ü Á¨¨‰∫å‰∏™ÈÖçÁΩÆÊñá‰ª∂ raspi.list1sudo nano /etc/apt/sources.list.d/raspi.list Áî®Â¶Ç‰∏ãÂÜÖÂÆπÊõøÊç¢ÂéüÊù•ÁöÑÂÜÖÂÆπÔºö12deb https://mirror.tuna.tsinghua.edu.cn/raspberrypi/ stretch main uideb-src https://mirror.tuna.tsinghua.edu.cn/raspberrypi/ stretch main ui ÁªôpipÊç¢Ê∫êpipÊòØ‰∏Ä‰∏™pythonÂåÖÁÆ°ÁêÜÂ∑•ÂÖ∑Ôºå ÂÆÉ‰∏çÊç¢Ê∫ê‰πüÊòØÊØîËæÉÊÖ¢„ÄÇËøô‰∏™Êç¢Ê∫êÁöÑÊó∂ÂÄôÊúâ‰∫õÈúÄË¶ÅÊ≥®ÊÑèÁöÑÂú∞Êñπ„ÄÇÈ¶ñÂÖàÊ†πÊçÆËøôÊ†∑ÁöÑÊïôÁ®ãÊù•Êõ¥Êç¢Ê∫êÁöÑÊó∂ÂÄôÔºåÂè™Ë¶ÅÂú®global‰∏ãÁöÑÊ∫êÈìæÊé•‰ΩøÁî®httpsÂçèËÆÆÔºåÈÇ£‰πàÂ∞±‰∏çÁî®ÂÜôtrusted-host=xxx,Âèç‰πãÂ¶ÇÊûúÊ≤°ÊúâÁî®httpÂèàÊ≤°ÊúâËÆæÁΩÆ trusted-hostÁöÑËØùÔºåÂú®Áî®pipÁöÑÊó∂ÂÄô‰ºöÊä•ÈîôÔºö The repository located at mirrors.aliyun.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with ‚Äò‚Äìtrusted-host mirrors.aliyun.com‚Äô. ÂÖ∂Ê¨°ÔºåËøôÊ†∑ÁöÑÊç¢Ê∫êÊñπÊ≥ïÂè™ËÉΩÈíàÂØπ‰∏Ä‰∏™Áî®Êà∑ÔºåÂ¶ÇÊûúÊòØÂú®piÁî®Êà∑ÁöÑÁõÆÂΩï‰∏ãÂÆâË£ÖÁöÑÔºåÈÇ£‰πàÁî®sudoÁöÑÊó∂ÂÄôÂ∞±Ê≤°ÊïàÊûú„ÄÇÊàëÁõÆÂâçÂπ∂‰∏çÁü•ÊÄé‰πà‰ΩúÁî®‰∫éÂÖ®Â±ÄÔºåÂõ†Ê≠§‰∏§‰∏™Áî®Êà∑ÈÉΩÊîπ‰∫ÜÂ∞±Ë°å„ÄÇÈÇ£‰πàÂºïÂá∫‰∫ÜÁ¨¨‰∏â‰∏™ÈóÆÈ¢òÔºåÊ†ëËéìÊ¥æÈªòËÆ§ÈîÅÂÆörootÔºåÊ≤°Êúâroot‰ªÄ‰πàËøõ‰∏ç‰∫ÜrootÊñá‰ª∂Â§π„ÄÇÊàëÂ∞ùËØïÂä†‰∫Üsudo ÁªìÊûúÊä•ÂëΩ‰ª§Ê≤°ÊâæÂà∞ÔºöÈÇ£‰πàÂè™ËÉΩÂºÄÂêØrootÁî®Êà∑;12sudo passwd rootsudo passwd --unlock root ÂàÜÂà´Âú®pi Âíå root ÁõÆÂΩï‰∏ã ËøõË°åÂ¶Ç‰∏ãÊìç‰ΩúÔºö1234mkdir .pipcd .piptouch pip.confnano pip.conf Â∞Ü‰∏ãÈù¢ÁöÑÂÜÖÂÆπÂÜôÂÖ•Âπ∂‰øùÂ≠òÔºö 12[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple/ Â∏∏Áî®Ê∫êÂàóË°®Ôºö ÈòøÈáå‰∫ë http://mirrors.aliyun.com/pypi/simple/ ‰∏≠ÂõΩÁßëÊäÄÂ§ßÂ≠¶ https://pypi.mirrors.ustc.edu.cn/simple/ Ë±ÜÁì£(douban) http://pypi.douban.com/simple/ Ê∏ÖÂçéÂ§ßÂ≠¶ https://pypi.tuna.tsinghua.edu.cn/simple/ ‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶ http://pypi.mirrors.ustc.edu.cn/simple/ ÂÆâË£Ö vimÊ∫ê‰πüÈÉΩÊç¢Ëøá‰∫ÜÔºåÁé∞Âú®ÂÆâË£Ö‰∏Ä‰∏™vim‰ΩìÈ™å‰∏Ä‰∏ãÁΩëÈÄüÔºöÈ¶ñÂÖàÊääÊ∫êÊõ¥Êñ∞‰∏Ä‰∏ãÔºåÈ°∫‰æøÂçáÁ∫ß‰∏Ä‰∏ãÁ≥ªÁªüËΩØ‰ª∂Ôºö12sudo apt-get updatesudo apt-get upgrade Êç¢ÂÆåaptÊ∫ê‰∏ÄÂÆöË¶ÅËøõË°å‰∏äÈù¢ÁöÑÊìç‰ΩúÔºå‰∏çÁÑ∂‰∏ãËΩΩËΩØ‰ª∂‰ºöÊúâÈóÆÈ¢ò„ÄÇ1sudo apt-get vim Êñ∞Êú∫Âô®ÁöÑÂâçÊúüÂáÜÂ§áÂü∫Êú¨Âà∞ËøôÈáåÂ∞±ÁªìÊùü‰∫ÜÔºå‰∏ãÈù¢ÂÆâË£Ö‰∏™‰∫∫ËΩØ‰ª∂ÁöÑÊó∂Èó¥‰∫Ü ËΩØ‰ª∂ÂÆâË£Ö‰∏™‰∫∫Á¨îËÆ∞Ôºö‰∏∫‰∫ÜÂºÄÂèëÊô∫ËÉΩÈü≥ÁÆ±È°πÁõÆÊàëÈúÄË¶ÅÂÆâË£ÖÁöÑËΩØ‰ª∂ÊúâÔºöÂÆâË£ÖÂàóË°®Ôºö ÂÆâË£Ögit :sudo apt-get install -y git ÂÆâË£Öpip:ÂÆòÊñπÊïôÁ®ã pipenv Ôºösudo pip3 install pipenv python3-dev:sudo apt-get install python3-dev Âõ†‰∏∫ÂÆâË£ÖspidevÁöÑÊó∂ÂÄôÊä•Áº∫ÂåÖ‰∫Ü respeakerÈ©±Âä®ÔºöÂÆòÊñπÊïôÁ®ã ÂÆâË£ÖNumPyÔºöpip install numpy, ÊàëÊääÂÆÉÊîæÂú®ËôöÊãüÁéØÂ¢ÉÈáå‰∫Ü ÂÆâË£Öswig: sudo apt-get install -y swig‰∏çÁÑ∂ÂÆâË£Ö pocketsphinxÁöÑÊó∂ÂÄô‰ºöÊä•Èîô ÂÆâË£ÖlibpulseÔºösudo apt-get install libpulse-dev ÂéüÂõ†Âêå‰∏ä ÂÆâË£Ölibasound2-dev: sudo apt-get install libasound2-dev Âêå‰∏ä ÂÆâË£Öpocketsphinx install ÂêéÈù¢Âä†‰∏Ä‰∏™ -y ÂèÇÊï∞ÂèØ‰ª•ÁªôÈÄâÈ°πËá™Âä®ÈÄâYES Âè¶ÔºöpipenvÁöÑlockËøáÁ®ãÂ§™ÊÖ¢ÈÉΩËØ¥pipenvÂêÑÁßçÁâõÈÄºÔºåÊâÄ‰ª•Â∞±Â∞ùËØïÁî®‰∫ÜÂÆÉ„ÄÇ‰ΩÜÊòØÊúâ‰∏™Ë∂ÖÁ∫ßÊó†Â•àÁöÑÈóÆÈ¢òÔºåÂÆÉÂú®lockÁöÑÊó∂ÂÄôÂ§™ÊÖ¢Â§™ÊÖ¢‰∫Ü„ÄÇÂÆòÊñπËØ¥ÊòØ‰ºòÂåñÈóÆÈ¢òÔºåÊ≠£Âú®Ëß£ÂÜ≥ÔºåÂà∞2019Âπ¥‰∫ÜËøòÊ≤°Ëß£ÂÜ≥„ÄÇÁé∞Âú®ÈááÁî®ÁΩëÂèãÁöÑÊñπÊ°àÔºöÂÖàË£Ö‰∏äÔºåÂêélock12pipenv install --skip-lock packetnamepipenv lock ÂÆâË£ÖpocketsphinxÂá∫ÈîôÊ±áÊÄª Áº∫Â∞ëswig Áº∫Â∞ëlibasound2-dev Áº∫Â∞ënumpyÂåÖ ËøêË°åledÁÅØÁ§∫‰æã‰ºöÊä•Èîô123456Traceback (most recent call last): File "/home/pi/smhonraspi/4mics_hat/pixels.py", line 11, in &lt;module&gt; from alexa_led_pattern import AlexaLedPattern File "/home/pi/smhonraspi/4mics_hat/alexa_led_pattern.py", line 18, in &lt;module&gt; import numpyImportError: No module named 'numpy' LiteÁâàÂ±ÖÁÑ∂‰πüÊ≤°ÊúânumpyÔºåÁúüÊòØÁ≤æÁÆÄÂïäÔºåÊ°åÈù¢ÁâàÂü∫Êú¨Â∑≤ÁªèÈ¢ÑË£Ö‰∫Ü ÊâßË°åËøôÊù°ÂëΩ‰ª§ÁöÑÊó∂ÂÄôÊä•Èîô‰∫Ü1sudo pip3 install spidev gpiozero Ëß£ÂÜ≥ÊñπÊ°àÊòØÔºö1sudo apt-get install python3-dev ‰ª•‰∏ä„ÄÇ]]></content>
      <categories>
        <category>Ê†ëËéìÊ¥æ</category>
      </categories>
      <tags>
        <tag>Ê†ëËéìÊ¥æ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰ΩøÁî®shadowsocksËÆ©Ê†ëËéìÊ¥æÁøªÂ¢ô]]></title>
    <url>%2F2019%2F03%2F17%2F%E4%BD%BF%E7%94%A8shadowsocks%E8%AE%A9%E6%A0%91%E8%8E%93%E6%B4%BE%E7%BF%BB%E5%A2%99%2F</url>
    <content type="text"><![CDATA[Âõ†‰∏∫Ê†ëËéìÊ¥æÈªòËÆ§‰ΩøÁî®ÂõΩÂ§ñÁöÑÊ∫êÔºåËΩØ‰ª∂ÂåÖ‰∏ãËΩΩÁöÑÁâπÂà´ÊÖ¢ÂçÅÂàÜÂΩ±Âìç‰ΩìÈ™å„ÄÇÈ¶ñÈÄâÂΩìÁÑ∂ÊòØÊç¢Ê∫êÔºåÁÆÄÂçï‰∏çËä±Èí±„ÄÇÁÑ∂ËÄåÊàëÂõ†‰∏∫‰π∞‰∫ÜssÊúçÂä°ÔºåÊâÄ‰ª•‰∏çËÉΩÊµ™Ë¥πÂïä„ÄÇÂè¶Â§ñÔºåËÉΩ‰∏çËÉΩÂÆâË£ÖÂ•ΩËøô‰∏™Ë∑üÊòØ‰∏çÊòØÊ†ëËéìÊ¥æ‰πüÊó†ÂÖ≥ÔºåÂè™Ë¶ÅÊòØ Debian / Ubuntu Á≥ªÁªüÂ∞±Ë°å ÂëΩ‰ª§Ë°åÂÆ¢Êà∑Á´ØÂÆâË£Ö12pip install git+https://github.com/shadowsocks/shadowsocks.git@mastersudo apt-get install proxychains ÈÖçÁΩÆssÂú®ÁõÆÂΩï etc ‰∏ãÂàõÂª∫Êñá‰ª∂ shadowsocks.json1sudo vim /etc/shadowsocks.json 12345678910&#123; "server":"ÊúçÂä°Âô® IPÊàñÂüüÂêç", "server_port":Á´ØÂè£Âè∑, "local_address":"127.0.0.1", "local_port":1080, "password":"ÂØÜÁ†Å", "timeout":300, "method":"Âä†ÂØÜÊñπÂºè (chacha20-ietf-poly1305 / aes-256-cfb)", "fast_open":false&#125; ÁÑ∂ÂêéÊàëËßâÂæóÂæàÊúâÂøÖË¶ÅËÆ©ÂÆÉÂºÄÊú∫ÂêØÂä® 1sudo vim /etc/rc.local Âú®ÊúÄÂêéÁöÑexit‰πãÂâçÊ∑ªÂä†1/usr/local/bin/sslocal -c /etc/shadowsocks.json -d start ÈÖçÁΩÆ proxychainsÁºñËæë /etc/proxychains.conf1sudo vim /etc/proxychains.conf Â∞ÜÊúÄÂêé‰∏ÄË°å‰øÆÊîπ‰∏∫Ôºö socks5 127.0.0.1 1080:Êé•ÁùÄÊàë‰ª¨Â∞±ÂèØ‰ª•Áõ¥Êé•Áî® ‚Äúproxychains + ÂëΩ‰ª§‚Äù ÁöÑÊñπÂºèÂú®ÁªàÁ´ØÈáå‰ΩøÁî®‰ª£ÁêÜ‰∫ÜÔºå‰æãÂ¶Ç123proxychains curl xxxxproxychains wget xxxxsudo proxychains apt-get xxxx]]></content>
      <categories>
        <category>Ê†ëËéìÊ¥æ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PocketSphinx on Raspberry pi 3]]></title>
    <url>%2F2019%2F03%2F13%2FPocketSphinx-on-Raspberry-pi-3%2F</url>
    <content type="text"><![CDATA[ËøôÁØáÂçöÂÆ¢ÊòØÂÖ≥‰∫éÂ¶Ç‰ΩïÂú®Ê†ëËéìÊ¥æ‰∏äÁî® ReSpeaker 4-mic ÈòµÂàóÊ®°Âùó +PocketSphinx+Python ÂÆûÁé∞ËØ≠Èü≥ËØÜÂà´„ÄÇÁî±‰∫éPocketSphinxÂíåPythonÈÉΩÊòØÁ¨¨‰∏ÄÊ¨°Êé•Ëß¶ÔºåÊúâÂæàÂ§ö‰∏çÁÜüÊÇâÁöÑÊ¶ÇÂøµÂíåÁî®Ê≥ïÔºåÊñáÊ°£‰πüÁúãÂæó‰∫ëÈáåÈõæÈáåÔºå‰∏çËøáÊúÄÂêéÂ•ΩÊ≠πÊòØÊääsampleË∑ëËµ∑Êù•‰∫Ü~ÊííËä±üéä ÂÖ≥‰∫éCMUSphinxCMUSphinx(ÁÆÄÁß∞Sphinx)ÊòØÁæéÂõΩÂç°ÂÜÖÂü∫Ê¢ÖÈöÜÂ§ßÂ≠¶ÂºÄÂèëÁöÑ‰∏ÄÁ≥ªÂàóËØ≠Èü≥ËØÜÂà´Á≥ªÁªüÁöÑÊÄªÁß∞„ÄÇÂÆÉÁöÑÁâàÊú¨‰∏çÊñ≠Êõ¥Êñ∞ ÔºöSphinx,Sphinx2,Sphinx3,Sphinx4ÔºàJavaÔºâ,PocketSphinx(c)„ÄÇÊàëÂú®ËøôÈáå‰ΩøÁî®ÁöÑÂ∞±ÊòØPocketSphinx,Ëá™ÂÆÉÂºÄÂßã,SphinxÂèØ‰ª•ËøêË°åÂú®ÂµåÂÖ•ÂºèËÆæÂ§áÈáå‰∫ÜÔºåÊØîÂ¶ÇÊâãÊú∫ÂíåÊ†ëËéìÊ¥æ„ÄÇPocketÂ∞±ÊòØ‚ÄúÂè£Ë¢ã‚ÄùÁöÑÊÑèÊÄùÔºåÂú®ÂêçÂ≠ó‰∏äÂ∑≤ÁªèÂÆåÂÖ®‰ΩìÁé∞Âá∫Êù•Ëøô‰∏ÄÁâàÊú¨ÁöÑÁâπÂæÅ‰∫Ü„ÄÇËôΩÁÑ∂PocketSphinxÊòØÁî®CÂÆûÁé∞ÁöÑÔºåÂ¶ÇÂõæÊâÄÁ§∫Ôºö ‰ΩÜÂπ∂‰∏çÊÑèÂë≥ÁùÄÂè™ËÉΩ‰ΩøÁî®CËØ≠Ë®ÄÊù•ÂºÄÂèëÂ∫îÁî®„ÄÇÈÄöËøá SWIG (Simplified Wrapper and Interface Generator) ,Â∞±ÂèØ‰ª•ÊääC/C++ÁöÑÊé•Âè£Â∞ÅË£ÖÊàêÂÖ∂‰ªñËØ≠Ë®ÄÁöÑÊé•Âè£ÔºàÂ§ßÊ¶ÇÂ∞±Ëøô‰πà‰∏™ÊÑèÊÄùÂêßÔºâÔºåÊØîÂ¶Ç Python ,ruby,java,C#‚Ä¶ÁõÆÂâçÈ°πÁõÆÂ∑≤ÊúâÁöÑÁâàÊú¨Â¶Ç‰∏ãÔºö Âú®ËøôÈáåÊàë‰ΩøÁî®ÁöÑÊòØPythonÁâàÊú¨ÁöÑ„ÄÇ ÂÆâË£ÖSphinxÊ®°ÂùóÂú®Ê†ëËéìÊ¥æ‰∏äÈù¢ÊâìÂºÄÁªàÁ´ØÔºåÊâßË°å‰∏ãÈù¢‰∏§Êù°ÂëΩ‰ª§Âç≥ÂèØ„ÄÇÂΩìÁÑ∂ÂÆâË£ÖÂâç‰∏∫‰∫Ü‰øùËØÅ‰∏ãËΩΩ‰ΩìÈ™åÔºå‰∏ÄËà¨ÊòØÈúÄË¶ÅÁªôÊ†ëËéìÊ¥æÊç¢Ê∫ê„ÄÇÁÑ∂ËÄåÊàëÂõ†‰∏∫Êúâss‰ª£ÁêÜÔºåÊâÄ‰ª•Â∞±ÈÖçÁΩÆ‰∫Ü‰∏ÄÈÄöÔºåÁî®‰∏ä‰∫ÜËá™Â∑±ÁöÑ‰ª£ÁêÜüòâ12python -m pip install --upgrade pip setuptools wheelpip install --upgrade pocketsphinx Â¶ÇÊûú‰Ω†ÂêëÊàë‰∏ÄÊ†∑ÊòØ‰∏™python+LinuxÂèåÈáçÂ∞èÁôΩÔºå‰∏çÁÜüÊÇâÁõÆÂΩïÁªìÊûÑÔºå‰πüÊêû‰∏çÊ∏ÖÊ•öÊ®°ÂùóÈÉΩÂÆâË£ÖÂà∞Âì™ÈáåÂéª‰∫ÜÔºåÂèØ‰ª•‰ΩøÁî®‰∏ãÈù¢ÁöÑÂëΩ‰ª§ 1pip show pocketsphinx ÁªìÊûúÂ¶Ç‰∏ãÔºö12345678910Name: pocketsphinxVersion: 0.1.15Summary: Python interface to CMU Sphinxbase and Pocketsphinx librariesHome-page: https://github.com/bambocher/pocketsphinx-pythonAuthor: Dmitry PrazdnichnovAuthor-email: dmitry@prazdnichnov.nameLicense: BSDLocation: /usr/local/lib/python3.5/dist-packagesRequires: Required-by: Ê®°ÂùóÂÆâË£ÖÂ•ΩÂêéÁöÑÁªìÊûÑ‰∏∫Ôºö123456789101112131415161718192021.‚îú‚îÄ‚îÄ data ‚îÇ ‚îî‚îÄ‚îÄ goforward.raw‚îú‚îÄ‚îÄ __init__.py‚îú‚îÄ‚îÄ model # Â≠òÊîæËØ≠Ë®ÄÂåÖ‚îÇ ‚îú‚îÄ‚îÄ cmudict-en-us.dict # ÊãºÈü≥Â≠óÂÖ∏Êñá‰ª∂‚îÇ ‚îú‚îÄ‚îÄ en-us # Â£∞Â≠¶Ê®°ÂûãÊñá‰ª∂Â§π‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ feat.params‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ mdef‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ means‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ noisedict‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ README‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ sendump‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ transition_matrices‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ variances‚îÇ ‚îî‚îÄ‚îÄ en-us.lm.bin #ËØ≠Ë®ÄÊ®°ÂûãÊñá‰ª∂‚îú‚îÄ‚îÄ _pocketsphinx.cpython-35m-arm-linux-gnueabihf.so‚îú‚îÄ‚îÄ pocketsphinx.py‚îî‚îÄ‚îÄ __pycache__ ‚îú‚îÄ‚îÄ __init__.cpython-35.pyc ‚îî‚îÄ‚îÄ pocketsphinx.cpython-35.pyc Ê≥®ÔºöÊàë‰πãÂâç‰∏çÁü•ÈÅìËøô‰∏™Ê†ëÁä∂ÂõæÊòØÊÄé‰πàÊêûÂá∫Êù•ÁöÑÔºåÊü•‰∫ÜÊâçÁü•ÈÅìÊòØÁî®‰∫Ü tree ÂëΩ‰ª§„ÄÇbtw,Áü•ÈÅìËøô‰∏™Ê®°ÂùóÂú®Âì™ÈáåË£ÖÁùÄÂæàÊúâÂøÖË¶ÅÔºåÂõ†‰∏∫ÂêéÈù¢Êõ¥Êç¢ÈªòËÆ§ËØ≠Ë®ÄÂåÖÔºàÈªòËÆ§Âè™‰ºöËØÜÂà´Ëã±ËØ≠ÔºâË¶ÅÁî®Âà∞. ÊàëÂú®ÂÆâË£ÖÁöÑÊó∂ÂÄôËøòÊä•‰∫Ü‰∏™ÈîôÔºö ÈóÆÈ¢òÂá∫Âú®ÊàëÁöÑÊ†ëËéìÊ¥æ‰∏äÈù¢Ê≤°ÊúâÂÆâË£Ö libpulse-devlibpulse-devÔºöPulseAudio client development headers and libraries 1sudo apt-get install libpulse-dev ReSpeaker 4-Mics ÈòµÂàóËøô‰∏™Ê®°ÂùóÊòØÊ∑òÂÆù‰∏äÈù¢‰π∞ÁöÑÔºå199ÂùóÈí±¬∑¬∑¬∑¬∑ÂÇªË¥µ„ÄÇÂà´ÁöÑÊñáÁ´†ÈáåÁî®ÁöÑÂ§ßÈÉΩÊòØUSBÈ∫¶ÂÖãÈ£éÔºå20Âá†ÂùóÈí±‰∏Ä‰∏™„ÄÇÂìéÔºåËá™Â∑±‰π∞ÁöÑÂì≠ÁùÄ‰πüË¶ÅÁî®‰∏ãÂéª„ÄÇ ÊääÊ®°ÂùóÊèíÂà∞Ê†ëËéìÊ¥æ‰∏ä‰πãÂêéÂºÄÂßãË£ÖÈ©±Âä®ÔºåÊ≥®ÊÑèËøô‰∏™Ê®°Âùó‰∏çÂèØ‰ª•ÁÉ≠ÊèíÊãî„ÄÇ‰∏ÄÂ•óÂÆåÊï¥ÁöÑÊìç‰ΩúÂ¶Ç‰∏ãÔºö123456sudo apt-get updatesudo apt-get upgradegit clone https://github.com/respeaker/seeed-voicecard.gitcd seeed-voicecardsudo ./install.shreboot ÈáçÂêØ‰πãÂêéËøô‰∏™È∫¶ÂÖãÈ£éÊ®°ÂùóÂ∞±ÂêØÂä®‰∫Ü„ÄÇÊé•‰∏ãÊù•ÂèØ‰ª•ËØïËØïË∑ë‰∏Ä‰∏ãËøô‰∏™Ê®°ÂùóÁöÑsampleÔºöËÆ©ÁÅØÈó™È¶ñÂÖàÊâìÂºÄSPI1234sudo raspi-configÈÄâÊã© Interfacing OptionsÈÄâÊã© SPIenable ÂÜçÊù•‰∏ÄÊ≥¢Â¶Ç‰∏ãÁöÑÊìç‰ΩúÔºö1234567pi@raspberrypi:~ $ cd /home/pipi@raspberrypi:~ $ git clone https://github.com/respeaker/4mics_hat.gitpi@raspberrypi:~ $ cd /home/pi/4mics_hatpi@raspberrypi:~/4mics_hat $ sudo apt install python-virtualenv # install python virtualenv toolpi@raspberrypi:~/4mics_hat $ virtualenv --system-site-packages ~/env # create a virtual python environmentpi@raspberrypi:~/4mics_hat $ source ~/env/bin/activate # activate the virtual environment(env) pi@raspberrypi:~/4mics_hat $ pip install spidev gpiozero # install spidev and gpiozero ÊàëÂÖ∂ÂÆû‰∏çÂ§™ÁêÜËß£‰∏∫‰ªÄ‰πàË¶ÅÂºÑËôöÊãüÁéØÂ¢ÉÔºüÊàëËØï‰∫Ü‰∏Ä‰∏ãÈÇ£‰∏™sampleÊòØÂèØ‰ª•Áõ¥Êé•ËøêË°åÁöÑ„ÄÇ ËøûÁª≠ËØÜÂà´Ôºàcontinuous recognitionÔºâÂáÜÂ§áÂ•Ω‰πãÂêéÂ∞±ÂèØ‰ª•ËØïËØïË∑ëPocketSphinxÁöÑsampleÁ®ãÂ∫èLiveSpeech‰∫ÜÔºåÂÆÉ‰ºöËøûÁª≠ËØÜÂà´ËØ≠Èü≥ÂÜÖÂÆπÔºåÂπ∂ËæìÂá∫Âà∞ÁªàÁ´Ø‰∏ä„ÄÇ‰∏çËøáÊ≥®ÊÑèÔºåÂÆÉÈªòËÆ§ÊòØËã±ÊñáËØ≠Èü≥ÔºåÊÉ≥ÊµãËØïÂæóËØ¥Ëã±ËØ≠„ÄÇÊ∫êÁ†ÅÂ¶Ç‰∏ãÔºö123456789101112131415161718import osfrom pocketsphinx import LiveSpeech, get_model_pathmodel_path = get_model_path()speech = LiveSpeech( verbose=False, # sampling_rate=16000,buffer_size=2048,no_search=False,full_utt=False,hmm=os.path.join(model_path, 'en-us'), # ÔºàHidden Markov ModelÔºâ ÈöêÈ©¨Â∞îÂèØÂ§´Ê®°Âûã ÔºåÂ£∞Â≠¶Ê®°Âûãlm=os.path.join(model_path, 'en-us.lm.bin'), # ÔºàLanguage ModelÔºâ ËØ≠Ë®ÄÊ®°Âûãdic=os.path.join(model_path, 'cmudict-en-us.dict') # ÊãºÈü≥Â≠óÂÖ∏)for phrase in speech:print("phrase:", phrase)print(phrase.segments(detailed=True)) ÂΩìÁÑ∂Ëøô‰∏™Á®ãÂ∫èËøòÊòØÊ≤°ÊúâË∑ëËµ∑Êù•ÔºåËøêË°åÁöÑÊó∂ÂÄôÊä•Èîô‰∫Ü„ÄÇ Opening audio device(null) for capture: Connection refused 123456789Error opening audio device (null) for capture: Connection refusedTraceback (most recent call last): File &quot;sphinx.py&quot;, line 14, in &lt;module&gt; dic=os.path.join(model_path, &apos;cmudict-en-us.dict&apos;) File &quot;/usr/local/lib/python3.5/dist-packages/pocketsphinx/__init__.py&quot;, line 206, in __init__ self.ad = Ad(self.audio_device, self.sampling_rate) File &quot;/usr/local/lib/python3.5/dist-packages/sphinxbase/ad_pulse.py&quot;, line 124, in __init__ this = _ad_pulse.new_Ad(audio_device, sampling_rate)RuntimeError: new_Ad returned -1 ÊàëÊòéÊòéË£Ö‰∫ÜmicÔºåÈ©±Âä®ÈÉΩÊ≠£Â∏∏„ÄÇÂêéÊù•Êü•‰∫ÜÂ•ΩÂ§öËµÑÊñôÊâçÂèëÁé∞ÊòØ‰∏Ä‰∏™Âè´ PulseAudioÁöÑËΩØ‰ª∂Ê≤°Ë£ÖÔºåÊ†ëËéìÊ¥æÊ≤°ÊúâËá™Â∏¶Ëøô‰∏™‰∏úË•ø„ÄÇËøòËÉΩÊÄé‰πàÂäûÔºåË£ÖÂ∞±ÂÆå‰∫ÜÔºåËøôÊ¨°‰∏çÊ±ÇÁîöËß£‰∫ÜÔºö 12sudo apt-get install pulseaudiosudo reboot Ë∑ëËµ∑Êù•‰πãÂêéËØï‰∫Ü‰∏Ä‰∏ãÔºåËØÜÂà´ÊïàÊûúÊÉ®‰∏çÂøçÁùπ„ÄÇ Êõ¥Êç¢‰∏≠ÊñáËØ≠Ë®ÄÂåÖ ‰∏ãËΩΩ‰∏≠ÊñáËØ≠Ë®ÄÂåÖ Ëß£ÂéãÂæóÔºö 12345678910111213.‚îú‚îÄ‚îÄ README‚îú‚îÄ‚îÄ zh_cn.cd_cont_5000‚îÇ ‚îú‚îÄ‚îÄ feat.params‚îÇ ‚îú‚îÄ‚îÄ feature_transform‚îÇ ‚îú‚îÄ‚îÄ mdef‚îÇ ‚îú‚îÄ‚îÄ means‚îÇ ‚îú‚îÄ‚îÄ mixture_weights‚îÇ ‚îú‚îÄ‚îÄ noisedict‚îÇ ‚îú‚îÄ‚îÄ transition_matrices‚îÇ ‚îî‚îÄ‚îÄ variances‚îú‚îÄ‚îÄ zh_cn.dic‚îî‚îÄ‚îÄ zh_cn.lm.bin Â§çÂà∂Âà∞modelÊñá‰ª∂Â§π 123sudo cp -a zh_cn.cd_cont_5000/ /usr/local/lib/python3.5/dist-packages/pocketsphinx/modelsudo cp zh_cn.dic /usr/local/lib/python3.5/dist-packages/pocketsphinx/modelsudo cp zh_cn.lm.bin /usr/local/lib/python3.5/dist-packages/pocketsphinx/model ÈúÄË¶ÅÊõ¥Êîπ‰∏Ä‰∏ã‰∏âÊÆµ‰ª£Á†Å 123hmm=os.path.join(model_path, 'zh_cn.cd_cont_5000'),lm=os.path.join(model_path, 'zh_cn.lm.bin'),dic=os.path.join(model_path, 'zh_cn.dic') Êõ¥Êç¢‰πãÂêéÁ°ÆÂÆûÂèØ‰ª•ËØÜÂà´‰∏≠Êñá‰∫ÜÔºåÁÑ∂ÔºåËØÜÂà´Áéá‰æùÁÑ∂‰ΩéÂæóÊÉä‰∫∫ ÂÖ≥ÈîÆÂ≠óÊü•ÊâæÔºàkeyword searchÔºâÊàëÂú®È°πÁõÆÈáå‰ΩøÁî®pocketSphinxÁõÆÁöÑ‰ªÖ‰ªÖÊòØËÆ©ÂÆÉËØÜÂà´ÊåáÂÆöÁöÑÂÖ≥ÈîÆËØçÔºåÊâÄ‰ª•ÊàëÈúÄË¶ÅÁùÄÈáçËÄÉËôëËøô‰∏ÄÊñπÈù¢„ÄÇ]]></content>
      <categories>
        <category>Ê†ëËéìÊ¥æ</category>
      </categories>
      <tags>
        <tag>Ê†ëËéìÊ¥æ</tag>
        <tag>ËØ≠Èü≥ËØÜÂà´</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HelloWorldÁ®ãÂ∫èÁºñËØëËøáÁ®ãÂ∞èËÆ∞]]></title>
    <url>%2F2019%2F02%2F18%2FHelloWorld%E7%A8%8B%E5%BA%8F%E7%BC%96%E8%AF%91%E8%BF%87%E7%A8%8B%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[‰ªäÂ§©Â∞ùËØï‰∫Ü‰∏Ä‰∏ã‰ΩøÁî®GCCÂØπHelloWorldÁ®ãÂ∫èËøõË°åÊåâÁÖßÁ®ãÂ∫èÁºñËØëÁöÑ4‰∏™Èò∂ÊÆµÔºö È¢ÑÂ§ÑÁêÜÈò∂ÊÆµ ÁºñËØëÈò∂ÊÆµ Ê±áÁºñÈò∂ÊÆµ ÈìæÊé•Èò∂ÊÆµ ‰æùÊ¨°ËøõË°åÂ§ÑÁêÜÂπ∂ËßÇÂØüÁîüÊàêÁöÑÊñá‰ª∂ÂÜÖÂÆπ È¢ÑÂ§ÑÁêÜÈò∂ÊÆµÂπ≥Âè∞Ôºöx8612345678#include"stdio.h"int main()&#123; printf("Hello world!\n"); return 0;&#125; È¢ÑÂ§ÑÁêÜÈò∂ÊÆµ‰ΩøÁî®ÂëΩ‰ª§ 1gcc -E hello.c -o hello.i ÂÆÉ‰ºöÁîüÊàê‰∏Ä‰∏™ hello.iÊñá‰ª∂ÔºåÂÜÖÂÆπÂ¶Ç‰∏ãÔºö ÁÇπÂáªÊòæ/ÈöêÂÜÖÂÆπ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431# 1 "hello.c"# 1 "&lt;built-in&gt;"# 1 "&lt;command line&gt;"# 1 "hello.c"# 1 "C:/MinGW/include/stdio.h" 1 3# 19 "C:/MinGW/include/stdio.h" 3# 1 "C:/MinGW/include/_mingw.h" 1 3# 31 "C:/MinGW/include/_mingw.h" 3 # 32 "C:/MinGW/include/_mingw.h" 3# 20 "C:/MinGW/include/stdio.h" 2 3# 1 "C:/MinGW/lib/gcc/mingw32/3.4.5/include/stddef.h" 1 3# 213 "C:/MinGW/lib/gcc/mingw32/3.4.5/include/stddef.h" 3typedef unsigned int size_t;# 325 "C:/MinGW/lib/gcc/mingw32/3.4.5/include/stddef.h" 3typedef short unsigned int wchar_t;# 354 "C:/MinGW/lib/gcc/mingw32/3.4.5/include/stddef.h" 3typedef short unsigned int wint_t;# 27 "C:/MinGW/include/stdio.h" 2 3# 1 "C:/MinGW/lib/gcc/mingw32/3.4.5/include/stdarg.h" 1 3# 44 "C:/MinGW/lib/gcc/mingw32/3.4.5/include/stdarg.h" 3typedef __builtin_va_list __gnuc_va_list;# 29 "C:/MinGW/include/stdio.h" 2 3# 129 "C:/MinGW/include/stdio.h" 3typedef struct _iobuf&#123; char* _ptr; int _cnt; char* _base; int _flag; int _file; int _charbuf; int _bufsiz; char* _tmpfname;&#125; FILE;# 154 "C:/MinGW/include/stdio.h" 3extern __attribute__ ((__dllimport__)) FILE _iob[];# 169 "C:/MinGW/include/stdio.h" 3 FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fopen (const char*, const char*); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) freopen (const char*, const char*, FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fflush (FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fclose (FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) remove (const char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) rename (const char*, const char*); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) tmpfile (void); char* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) tmpnam (char*); char* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _tempnam (const char*, const char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _rmtmp(void); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _unlink (const char*); char* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) tempnam (const char*, const char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) rmtmp(void); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) unlink (const char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) setvbuf (FILE*, char*, int, size_t); void __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) setbuf (FILE*, char*);# 204 "C:/MinGW/include/stdio.h" 3extern int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __mingw_fprintf(FILE*, const char*, ...);extern int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __mingw_printf(const char*, ...);extern int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __mingw_sprintf(char*, const char*, ...);extern int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __mingw_snprintf(char*, size_t, const char*, ...);extern int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __mingw_vfprintf(FILE*, const char*, __gnuc_va_list);extern int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __mingw_vprintf(const char*, __gnuc_va_list);extern int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __mingw_vsprintf(char*, const char*, __gnuc_va_list);extern int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __mingw_vsnprintf(char*, size_t, const char*, __gnuc_va_list);# 293 "C:/MinGW/include/stdio.h" 3 int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fprintf (FILE*, const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) printf (const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) sprintf (char*, const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vfprintf (FILE*, const char*, __gnuc_va_list); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vprintf (const char*, __gnuc_va_list); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vsprintf (char*, const char*, __gnuc_va_list);# 308 "C:/MinGW/include/stdio.h" 3 int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __msvcrt_fprintf(FILE*, const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __msvcrt_printf(const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __msvcrt_sprintf(char*, const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __msvcrt_vfprintf(FILE*, const char*, __gnuc_va_list); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __msvcrt_vprintf(const char*, __gnuc_va_list); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) __msvcrt_vsprintf(char*, const char*, __gnuc_va_list); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _snprintf (char*, size_t, const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _vsnprintf (char*, size_t, const char*, __gnuc_va_list);# 330 "C:/MinGW/include/stdio.h" 3int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) snprintf (char *, size_t, const char *, ...);int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vsnprintf (char *, size_t, const char *, __gnuc_va_list);int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vscanf (const char * __restrict__, __gnuc_va_list);int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vfscanf (FILE * __restrict__, const char * __restrict__, __gnuc_va_list);int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vsscanf (const char * __restrict__, const char * __restrict__, __gnuc_va_list); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fscanf (FILE*, const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) scanf (const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) sscanf (const char*, const char*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fgetc (FILE*); char* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fgets (char*, int, FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fputc (int, FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fputs (const char*, FILE*); char* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) gets (char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) puts (const char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) ungetc (int, FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _filbuf (FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _flsbuf (int, FILE*);extern __inline__ int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) getc (FILE* __F)&#123; return (--__F-&gt;_cnt &gt;= 0) ? (int) (unsigned char) *__F-&gt;_ptr++ : _filbuf (__F);&#125;extern __inline__ int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) putc (int __c, FILE* __F)&#123; return (--__F-&gt;_cnt &gt;= 0) ? (int) (unsigned char) (*__F-&gt;_ptr++ = (char)__c) : _flsbuf (__c, __F);&#125;extern __inline__ int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) getchar (void)&#123; return (--(&amp;_iob[0])-&gt;_cnt &gt;= 0) ? (int) (unsigned char) *(&amp;_iob[0])-&gt;_ptr++ : _filbuf ((&amp;_iob[0]));&#125;extern __inline__ int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) putchar(int __c)&#123; return (--(&amp;_iob[1])-&gt;_cnt &gt;= 0) ? (int) (unsigned char) (*(&amp;_iob[1])-&gt;_ptr++ = (char)__c) : _flsbuf (__c, (&amp;_iob[1]));&#125;# 411 "C:/MinGW/include/stdio.h" 3 size_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fread (void*, size_t, size_t, FILE*); size_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fwrite (const void*, size_t, size_t, FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fseek (FILE*, long, int); long __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) ftell (FILE*); void __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) rewind (FILE*);# 454 "C:/MinGW/include/stdio.h" 3typedef long long fpos_t; int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fgetpos (FILE*, fpos_t*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fsetpos (FILE*, const fpos_t*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) feof (FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) ferror (FILE*);# 479 "C:/MinGW/include/stdio.h" 3 void __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) clearerr (FILE*); void __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) perror (const char*); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _popen (const char*, const char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _pclose (FILE*); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) popen (const char*, const char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) pclose (FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _flushall (void); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _fgetchar (void); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _fputchar (int); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _fdopen (int, const char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _fileno (FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _fcloseall (void); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _fsopen (const char*, const char*, int); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _getmaxstdio (void); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _setmaxstdio (int);# 521 "C:/MinGW/include/stdio.h" 3 int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fgetchar (void); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fputchar (int); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fdopen (int, const char*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fileno (FILE*);# 533 "C:/MinGW/include/stdio.h" 3# 1 "C:/MinGW/include/sys/types.h" 1 3# 21 "C:/MinGW/include/sys/types.h" 3# 1 "C:/MinGW/lib/gcc/mingw32/3.4.5/include/stddef.h" 1 3# 151 "C:/MinGW/lib/gcc/mingw32/3.4.5/include/stddef.h" 3typedef int ptrdiff_t;# 22 "C:/MinGW/include/sys/types.h" 2 3typedef long time_t;typedef long long __time64_t;typedef long _off_t;typedef _off_t off_t;typedef unsigned int _dev_t;typedef _dev_t dev_t;typedef short _ino_t;typedef _ino_t ino_t;typedef int _pid_t;typedef _pid_t pid_t;typedef unsigned short _mode_t;typedef _mode_t mode_t;typedef int _sigset_t;typedef _sigset_t sigset_t;typedef long _ssize_t;typedef _ssize_t ssize_t;typedef long long fpos64_t;typedef long long off64_t;typedef unsigned int useconds_t;# 534 "C:/MinGW/include/stdio.h" 2 3extern __inline__ FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fopen64 (const char* filename, const char* mode)&#123; return fopen (filename, mode);&#125;int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fseeko64 (FILE*, off64_t, int);extern __inline__ off64_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) ftello64 (FILE * stream)&#123; fpos_t pos; if (fgetpos(stream, &amp;pos)) return -1LL; else return ((off64_t) pos);&#125;# 562 "C:/MinGW/include/stdio.h" 3 int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fwprintf (FILE*, const wchar_t*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) wprintf (const wchar_t*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _snwprintf (wchar_t*, size_t, const wchar_t*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vfwprintf (FILE*, const wchar_t*, __gnuc_va_list); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vwprintf (const wchar_t*, __gnuc_va_list); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _vsnwprintf (wchar_t*, size_t, const wchar_t*, __gnuc_va_list); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fwscanf (FILE*, const wchar_t*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) wscanf (const wchar_t*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) swscanf (const wchar_t*, const wchar_t*, ...); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fgetwc (FILE*); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fputwc (wchar_t, FILE*); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) ungetwc (wchar_t, FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) swprintf (wchar_t*, const wchar_t*, ...); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vswprintf (wchar_t*, const wchar_t*, __gnuc_va_list); wchar_t* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fgetws (wchar_t*, int, FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fputws (const wchar_t*, FILE*); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) getwc (FILE*); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) getwchar (void); wchar_t* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _getws (wchar_t*); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) putwc (wint_t, FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _putws (const wchar_t*); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) putwchar (wint_t); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wfdopen(int, const wchar_t *); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wfopen (const wchar_t*, const wchar_t*); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wfreopen (const wchar_t*, const wchar_t*, FILE*); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wfsopen (const wchar_t*, const wchar_t*, int); wchar_t* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wtmpnam (wchar_t*); wchar_t* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wtempnam (const wchar_t*, const wchar_t*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wrename (const wchar_t*, const wchar_t*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wremove (const wchar_t*); void __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wperror (const wchar_t*); FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _wpopen (const wchar_t*, const wchar_t*);int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) snwprintf (wchar_t* s, size_t n, const wchar_t* format, ...);extern __inline__ int __attribute__((__cdecl__)) __attribute__ ((__nothrow__))vsnwprintf (wchar_t* s, size_t n, const wchar_t* format, __gnuc_va_list arg) &#123; return _vsnwprintf ( s, n, format, arg);&#125;int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vwscanf (const wchar_t * __restrict__, __gnuc_va_list);int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vfwscanf (FILE * __restrict__, const wchar_t * __restrict__, __gnuc_va_list);int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) vswscanf (const wchar_t * __restrict__, const wchar_t * __restrict__, __gnuc_va_list);# 620 "C:/MinGW/include/stdio.h" 3 FILE* __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) wpopen (const wchar_t*, const wchar_t*); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _fgetwchar (void); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _fputwchar (wint_t); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _getw (FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) _putw (int, FILE*); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fgetwchar (void); wint_t __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) fputwchar (wint_t); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) getw (FILE*); int __attribute__((__cdecl__)) __attribute__ ((__nothrow__)) putw (int, FILE*);# 2 "hello.c" 2int main()&#123; printf("Hello world!\n"); return 0;&#125;]]></content>
      <categories>
        <category>GCCÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>GCC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5_Parallel_Memory_System]]></title>
    <url>%2F2019%2F01%2F30%2F5_Parallel_Memory_System%2F</url>
    <content type="text"><![CDATA[Outside of the GPU itself, the memory subsystem is the most important determiner of the performance of a graphics system. Graphics workloads demand very high transfer rates to and from memory. Pixel write and blend (read-modifywrite) operations, depth buffer reads and writes, and texture map reads, as well as command and object vertex and attribute data reads, comprise the majority of memory traffic.Âú®GPUÊú¨Ë∫´‰πãÂ§ñÔºåÂ≠òÂÇ®Âô®Â≠êÁ≥ªÁªüÊòØÂõæÂΩ¢Á≥ªÁªüÊÄßËÉΩÁöÑÊúÄÈáçË¶ÅÁöÑÂÜ≥ÂÆöÂõ†Á¥†„ÄÇ ÂõæÂΩ¢Â∑•‰ΩúË¥üËΩΩÈúÄË¶ÅÈùûÂ∏∏È´òÁöÑÂÜÖÂ≠ò‰º†ËæìÈÄüÁéá„ÄÇ ÂÉèÁ¥†ÂÜôÂÖ•ÂíåÊ∑∑ÂêàÔºàËØªÂèñ - ‰øÆÊîπÂÜôÂÖ•ÔºâÊìç‰ΩúÔºåÊ∑±Â∫¶ÁºìÂÜ≤Âå∫ËØªÂèñÂíåÂÜôÂÖ•‰ª•ÂèäÁ∫πÁêÜÊò†Â∞ÑËØªÂèñ‰ª•ÂèäÂëΩ‰ª§ÂíåÂØπË±°È°∂ÁÇπÂíåÂ±ûÊÄßÊï∞ÊçÆËØªÂèñÊûÑÊàê‰∫ÜÂ§ßÈÉ®ÂàÜÂÜÖÂ≠òÊµÅÈáè„ÄÇModern GPUs are highly parallel, as shown in Figure C.2.5. For example, the GeForce 8800 can process 32 pixels per clock, at 600 MHz. Each pixel typically requires a color read and write and a depth read and write of a 4-byte pixel. Usually an average of two or three texels of four bytes each are read to generate the pixel‚Äôs color. So for a typical case, there is a demand of 28 bytes times 32 pixels = 896 bytes per clock. Clearly the bandwidth demand on the memory system is enormous.Áé∞‰ª£GPUÈ´òÂ∫¶Âπ∂Ë°åÔºåÂ¶ÇÂõæC.2.5ÊâÄÁ§∫„ÄÇ ‰æãÂ¶ÇÔºåGeForce 8800ÊØèÊó∂ÈíüÂèØÂ§ÑÁêÜ32‰∏™ÂÉèÁ¥†ÔºåÈ¢ëÁéá‰∏∫600 MHz„ÄÇ ÊØè‰∏™ÂÉèÁ¥†ÈÄöÂ∏∏ÈúÄË¶ÅÈ¢úËâ≤ËØªÂèñÂíåÂÜôÂÖ•‰ª•Âèä4Â≠óËäÇÂÉèÁ¥†ÁöÑÊ∑±Â∫¶ËØªÂèñÂíåÂÜôÂÖ•„ÄÇ ÈÄöÂ∏∏ÔºåËØªÂèñÂπ≥ÂùáÊØè‰∏™Âõõ‰∏™Â≠óËäÇÁöÑ‰∏§‰∏™Êàñ‰∏â‰∏™Á∫πÁ¥†Ôºå‰ª•ÁîüÊàêÂÉèÁ¥†ÁöÑÈ¢úËâ≤„ÄÇ Âõ†Ê≠§ÔºåÂØπ‰∫éÂÖ∏ÂûãÊÉÖÂÜµÔºåÈúÄË¶Å28‰∏™Â≠óËäÇ‰πò‰ª•32‰∏™ÂÉèÁ¥†=ÊØè‰∏™Êó∂Èíü896‰∏™Â≠óËäÇ„ÄÇ ÊòæÁÑ∂ÔºåÂ≠òÂÇ®Á≥ªÁªüÁöÑÂ∏¶ÂÆΩÈúÄÊ±ÇÊòØÂ∑®Â§ßÁöÑ„ÄÇ To supply these requirements, GPU memory systems have the following characteristics:‰∏∫Êª°Ë∂≥Ëøô‰∫õË¶ÅÊ±ÇÔºåGPUÂÜÖÂ≠òÁ≥ªÁªüÂÖ∑Êúâ‰ª•‰∏ãÁâπÂæÅÔºö They are wide, meaning there are a large number of pins to convey data between the GPU and its memory devices, and the memory array itself comprises many DRAM chips to provide the full total data bus width. ÂÆÉ‰ª¨ÂæàÂÆΩÔºåÊÑèÂë≥ÁùÄÊúâÂ§ßÈáèÁöÑÂºïËÑöÂú®GPUÂíåÂÆÉÁöÑÂ≠òÂÇ®Âô®ËÆæÂ§á‰πãÈó¥‰º†ÈÄÅÊï∞ÊçÆÔºåÂπ∂‰∏îÂ≠òÂÇ®Âô®ÈòµÂàóÊú¨Ë∫´ÂåÖÊã¨ËÆ∏Â§öDRAMËäØÁâá‰ª•Êèê‰æõÂÆåÊï¥ÁöÑÊÄªÊï∞ÊçÆÊÄªÁ∫øÂÆΩÂ∫¶„ÄÇ They are fast, meaning aggressive signaling techniques are used to maximize the data rate (bits/second) per pin. ÂÆÉ‰ª¨ÂæàÂø´ÔºåÊÑèÂë≥ÁùÄÁßØÊûÅÁöÑ‰ø°‰ª§ÊäÄÊúØÁî®‰∫éÊúÄÂ§ßÂåñÊØè‰∏™ÂºïËÑöÁöÑÊï∞ÊçÆÈÄüÁéáÔºà‰Ωç/ÁßíÔºâ„ÄÇ GPUs seek to use every available cycle to transfer data to or from the memory array. To achieve this, GPUs specifcally do not aim to minimize latency to the memory system. High throughput (utilization efciency) and short latency are fundamentally in conÔ¨Çict. GPUÂØªÊ±Ç‰ΩøÁî®ÊØè‰∏™ÂèØÁî®Âë®ÊúüÊù•Â∞ÜÊï∞ÊçÆ‰º†ËæìÂà∞Â≠òÂÇ®Âô®ÈòµÂàóÊàñ‰ªéÂ≠òÂÇ®Âô®ÈòµÂàó‰º†ËæìÊï∞ÊçÆ„ÄÇ ‰∏∫ÂÆûÁé∞Ê≠§ÁõÆÁöÑÔºåGPUÁâπÂà´‰∏çÊó®Âú®ÊúÄÂ∞èÂåñÂØπÂ≠òÂÇ®Âô®Á≥ªÁªüÁöÑÂª∂Ëøü„ÄÇ È´òÂêûÂêêÈáèÔºàÂà©Áî®ÊïàÁéáÔºâÂíåÁü≠Âª∂ËøüÂü∫Êú¨‰∏äÊòØÂÜ≤Á™ÅÁöÑ„ÄÇ Compression techniques are used, both lossy, of which the programmer must be aware, and lossless, which is invisible to the application and opportunistic. ‰ΩøÁî®ÂéãÁº©ÊäÄÊúØÔºåÊó¢ÊúâÊçüËÄóÔºåÁ®ãÂ∫èÂëòÂøÖÈ°ªÁü•ÈÅìÔºåÊó†ÊçüÔºåÂØπÂ∫îÁî®Á®ãÂ∫èÂíåÊú∫‰ºö‰∏ª‰πâÊòØ‰∏çÂèØËßÅÁöÑ„ÄÇ Caches and work coalescing structures are used to reduce the amount of offchip traffic needed and to ensure that cycles spent moving data are used as fully as possible. È´òÈÄüÁºìÂ≠òÂíåÂ∑•‰ΩúÂêàÂπ∂ÁªìÊûÑÁî®‰∫éÂáèÂ∞ëÊâÄÈúÄÁöÑÁâáÂ§ñÊµÅÈáèÔºåÂπ∂Á°Æ‰øùÂ∞ΩÂèØËÉΩÂÖÖÂàÜÂú∞‰ΩøÁî®ÁßªÂä®Êï∞ÊçÆÁöÑÂë®Êúü„ÄÇ DRAM Considerations DRAMÊ≥®ÊÑè‰∫ãÈ°πGPUs must take into account the unique characteristics of DRAM. DRAM chips are internally arranged as multiple (typically four to eight) banks, where each bank includes a power-of-2 number of rows (typically around 16,384), and each row contains a power-of-2 number of bits (typically 8192). DRAMs impose a variety of timing requirements on their controlling processor. For example, dozens of cycles are required to activate one row, but once activated, the bits within that row are randomly accessible with a new column address every four clocks. Double-data rate (DDR) synchronous DRAMs transfer data on both rising and falling edges of the interface clock (see Chapter 5). So a 1 GHz clocked DDR DRAM transfers data at 2 gigabits per second per data pin. Graphics DDR DRAMs usually have 32 bidirectional data pins, so eight bytes can be read or written from the DRAM per clock.GPUÂøÖÈ°ªËÄÉËôëDRAMÁöÑÁã¨ÁâπÁâπÊÄß„ÄÇ DRAMËäØÁâáÂÜÖÈÉ®ÊéíÂàó‰∏∫Â§ö‰∏™ÔºàÈÄöÂ∏∏‰∏∫ÂõõÂà∞ÂÖ´‰∏™ÔºâÂ≠òÂÇ®‰ΩìÔºåÂÖ∂‰∏≠ÊØè‰∏™Â≠òÂÇ®‰ΩìÂåÖÊã¨2‰∏™ÂπÇÁöÑË°åÊï∞ÔºàÈÄöÂ∏∏Á∫¶‰∏∫16,384‰∏™ÔºâÔºåÊØèË°åÂåÖÂê´2‰∏™ÂπÇÁöÑ‰ΩçÊï∞ÔºàÈÄöÂ∏∏‰∏∫8192‰∏™ÔºâÔºâ„ÄÇ DRAMÂØπÂÖ∂ÊéßÂà∂Â§ÑÁêÜÂô®ÊñΩÂä†‰∫ÜÂêÑÁßçÊó∂Â∫èË¶ÅÊ±Ç„ÄÇ ‰æãÂ¶ÇÔºåÊøÄÊ¥ª‰∏ÄË°åÈúÄË¶ÅÂá†ÂçÅ‰∏™Âë®ÊúüÔºå‰ΩÜ‰∏ÄÊó¶ÊøÄÊ¥ªÔºåËØ•Ë°å‰∏≠ÁöÑ‰ΩçÂèØÈöèÊú∫ËÆøÈóÆÔºåÊØèÂõõ‰∏™Êó∂Èíü‰ΩøÁî®‰∏Ä‰∏™Êñ∞ÁöÑÂàóÂú∞ÂùÄ„ÄÇ ÂèåÂÄçÊï∞ÊçÆÈÄüÁéáÔºàDDRÔºâÂêåÊ≠•DRAMÂú®Êé•Âè£Êó∂ÈíüÁöÑ‰∏äÂçáÊ≤øÂíå‰∏ãÈôçÊ≤ø‰º†ËæìÊï∞ÊçÆÔºàËßÅÁ¨¨5Á´†Ôºâ„ÄÇ Âõ†Ê≠§Ôºå1 GHzÊó∂ÈíüDDR DRAM‰ª•ÊØè‰∏™Êï∞ÊçÆÂºïËÑöÊØèÁßí2ÂçÉÂÖÜ‰ΩçÁöÑÈÄüÂ∫¶‰º†ËæìÊï∞ÊçÆ„ÄÇ ÂõæÂΩ¢DDR DRAMÈÄöÂ∏∏ÂÖ∑Êúâ32‰∏™ÂèåÂêëÊï∞ÊçÆÂºïËÑöÔºåÂõ†Ê≠§ÊØè‰∏™Êó∂ÈíüÂèØ‰ª•‰ªéDRAMËØªÂèñÊàñÂÜôÂÖ•8‰∏™Â≠óËäÇ„ÄÇ GPUs internally have a large number of generators of memory traffic. Different stages of the logical graphics pipeline each have their own request streams: command and vertex attribute fetch, shader texture fetch and load/store, and pixel depth and color read-write. At each logical stage, there are ofen multiple independent units to deliver the parallel throughput. These are each independent memory requestors. When viewed at the memory system, there are an enormous number of uncorrelated requests in Ô¨Çight. This is a natural mismatch to the reference pattern preferred by the DRAMs. A solution is for the GPU‚Äôs memory controller to maintain separate heaps of trafc bound for diÔ¨Äerent DRAM banks, and wait until enough traffic for a particular DRAM row is pending before activating that row and transferring all the trafc at once. Note that accumulating pending requests, while good for DRAM row locality and thus efcient use of the data bus, leads to longer average latency as seen by the requestors whose requests spend time waiting for others. The design must take care that no particular request waits too long, otherwise some processing units can starve waiting for data and ultimately cause neighboring processors to become idle.GPUÂÜÖÈÉ®ÂÖ∑ÊúâÂ§ßÈáèÂÜÖÂ≠òÊµÅÈáèÁîüÊàêÂô®„ÄÇÈÄªËæëÂõæÂΩ¢ÁÆ°ÈÅìÁöÑ‰∏çÂêåÈò∂ÊÆµÂêÑËá™ÂÖ∑ÊúâÂÖ∂Ëá™Â∑±ÁöÑËØ∑Ê±ÇÊµÅÔºöÂëΩ‰ª§ÂíåÈ°∂ÁÇπÂ±ûÊÄßËé∑ÂèñÔºåÁùÄËâ≤Âô®Á∫πÁêÜËé∑ÂèñÂíåÂä†ËΩΩ/Â≠òÂÇ®Ôºå‰ª•ÂèäÂÉèÁ¥†Ê∑±Â∫¶ÂíåÈ¢úËâ≤ËØªÂÜô„ÄÇÂú®ÊØè‰∏™ÈÄªËæëÈò∂ÊÆµÔºåÊúâÂ§ö‰∏™Áã¨Á´ãÂçïÂÖÉÊù•Êèê‰æõÂπ∂Ë°åÂêûÂêêÈáè„ÄÇËøô‰∫õÊòØÊØè‰∏™Áã¨Á´ãÁöÑÂÜÖÂ≠òËØ∑Ê±ÇËÄÖ„ÄÇÂú®ÂÜÖÂ≠òÁ≥ªÁªü‰∏≠Êü•ÁúãÊó∂ÔºåÈ£ûË°å‰∏≠Â≠òÂú®Â§ßÈáè‰∏çÁõ∏ÂÖ≥ÁöÑËØ∑Ê±Ç„ÄÇËøô‰∏éDRAM‰ºòÈÄâÁöÑÂèÇËÄÉÂõæÊ°àËá™ÁÑ∂‰∏çÂåπÈÖç„ÄÇ‰∏ÄÁßçËß£ÂÜ≥ÊñπÊ°àÊòØGPUÁöÑÂÜÖÂ≠òÊéßÂà∂Âô®Áª¥Êä§ÁªëÂÆöÂà∞‰∏çÂêåDRAMÂ∫ìÁöÑÂçïÁã¨ÁöÑÊµÅÈáèÂ†ÜÔºåÂπ∂Á≠âÂæÖÁâπÂÆöDRAMË°åÁöÑË∂≥Â§üÊµÅÈáèÂæÖÊøÄÊ¥ªÔºåÁÑ∂ÂêéÊøÄÊ¥ªËØ•Ë°åÂπ∂Á´ãÂç≥‰º†ËæìÊâÄÊúâÊµÅÈáè„ÄÇËØ∑Ê≥®ÊÑèÔºåÁ¥ØÁßØÂæÖÂ§ÑÁêÜËØ∑Ê±ÇËôΩÁÑ∂ÊúâÂà©‰∫éDRAMË°å‰ΩçÁΩÆÂπ∂Âõ†Ê≠§ÊúâÊïàÂú∞‰ΩøÁî®Êï∞ÊçÆÊÄªÁ∫øÔºå‰ΩÜ‰ºöÂØºËá¥ËØ∑Ê±ÇËÄÖËä±Ë¥πÊó∂Èó¥Á≠âÂæÖÂÖ∂‰ªñËØ∑Ê±ÇËÄÖÁöÑÂπ≥ÂùáÂª∂ËøüÊó∂Èó¥Êõ¥Èïø„ÄÇËÆæËÆ°ÂøÖÈ°ªÊ≥®ÊÑèÊ≤°ÊúâÁâπÂÆöËØ∑Ê±ÇÁ≠âÂæÖÂ§™ÈïøÊó∂Èó¥ÔºåÂê¶Âàô‰∏Ä‰∫õÂ§ÑÁêÜÂçïÂÖÉÂèØËÉΩ‰ºöÈ•øÊ≠ªÁ≠âÂæÖÊï∞ÊçÆÂπ∂ÊúÄÁªàÂØºËá¥Áõ∏ÈÇªÂ§ÑÁêÜÂô®Âèò‰∏∫Á©∫Èó≤„ÄÇ GPU memory subsystems are arranged as multiple memory partitions, each of which comprises a fully independent memory controller and one or two DRAM devices that are fully and exclusively owned by that partition. To achieve the best load balance and therefore approach the theoretical performance of n partitions, addresses are fnely interleaved evenly across all memory partitions. The partition interleaving stride is typically a block of a few hundred bytes. The number of memory partitions is designed to balance the number of processors and other memory requesters.GPUÂ≠òÂÇ®Âô®Â≠êÁ≥ªÁªüË¢´Â∏ÉÁΩÆ‰∏∫Â§ö‰∏™Â≠òÂÇ®Âô®ÂàÜÂå∫ÔºåÊØè‰∏™Â≠òÂÇ®Âô®ÂàÜÂå∫ÂåÖÊã¨ÂÆåÂÖ®Áã¨Á´ãÁöÑÂ≠òÂÇ®Âô®ÊéßÂà∂Âô®ÂíåÁî±ËØ•ÂàÜÂå∫ÂÆåÂÖ®Âíå‰∏ìÊúâÂú∞Êã•ÊúâÁöÑ‰∏Ä‰∏™Êàñ‰∏§‰∏™DRAMËÆæÂ§á„ÄÇ ‰∏∫‰∫ÜÂÆûÁé∞ÊúÄ‰Ω≥Ë¥üËΩΩÂπ≥Ë°°Âπ∂Âõ†Ê≠§Êé•Ëøën‰∏™ÂàÜÂå∫ÁöÑÁêÜËÆ∫ÊÄßËÉΩÔºåÂú∞ÂùÄÂú®ÊâÄÊúâÂÜÖÂ≠òÂàÜÂå∫‰∏äÂùáÂåÄÂú∞‰∫§Èîô„ÄÇ ÂàÜÂå∫‰∫§ÁªáÊ≠•ÂπÖÈÄöÂ∏∏ÊòØÂá†ÁôæÂ≠óËäÇÁöÑÂùó„ÄÇ ÂÜÖÂ≠òÂàÜÂå∫ÁöÑÊï∞ÈáèÊó®Âú®Âπ≥Ë°°Â§ÑÁêÜÂô®ÂíåÂÖ∂‰ªñÂÜÖÂ≠òËØ∑Ê±ÇËÄÖÁöÑÊï∞Èáè„ÄÇ CachesGPU workloads typically have very large working sets‚Äîon the order of hundreds of megabytes to generate a single graphics frame. Unlike with CPUs, it is not practical to construct caches on chips large enough to hold anything close to the full working set of a graphics application. Whereas CPUs can assume very high cache hit rates (99.9% or more), GPUs experience hit rates closer to 90% and must therefore cope with many misses in Ô¨Çight. While a CPU can reasonably be designed to halt while waiting for a rare cache miss, a GPU needs to proceed with misses and hits intermingled. We call this a streaming cache architecture.GPUÂ∑•‰ΩúË¥üËΩΩÈÄöÂ∏∏ÂÖ∑ÊúâÈùûÂ∏∏Â§ßÁöÑÂ∑•‰ΩúÈõÜ - Â§ßÁ∫¶Êï∞ÁôæÂÖÜÂ≠óËäÇ‰ª•ÁîüÊàêÂçï‰∏™ÂõæÂΩ¢Â∏ß„ÄÇ ‰∏éCPU‰∏çÂêåÔºåÂú®Ë∂≥Â§üÂ§ßÁöÑËäØÁâá‰∏äÊûÑÂª∫ÁºìÂ≠ò‰ª•‰øùÂ≠òÈù†ËøëÂõæÂΩ¢Â∫îÁî®Á®ãÂ∫èÁöÑÂÆåÊï¥Â∑•‰ΩúÈõÜÁöÑ‰ªª‰Ωï‰∏úË•øÈÉΩÊòØ‰∏çÂÆûÈôÖÁöÑ„ÄÇ ËôΩÁÑ∂CPUÂèØ‰ª•ÊâøÊãÖÈùûÂ∏∏È´òÁöÑÁºìÂ≠òÂëΩ‰∏≠ÁéáÔºà99.9ÔºÖÊàñÊõ¥È´òÔºâÔºå‰ΩÜGPUÁöÑÂëΩ‰∏≠ÁéáÊé•Ëøë90ÔºÖÔºåÂõ†Ê≠§ÂøÖÈ°ªÂ∫îÂØπÈ£ûË°å‰∏≠ÁöÑËÆ∏Â§öÊú™ÂëΩ‰∏≠„ÄÇ ËôΩÁÑ∂CPUÂèØ‰ª•ÂêàÁêÜÂú∞ËÆæËÆ°‰∏∫Âú®Á≠âÂæÖÁΩïËßÅÁöÑÈ´òÈÄüÁºìÂ≠òÊú™ÂëΩ‰∏≠Êó∂ÂÅúÊ≠¢Ôºå‰ΩÜÊòØGPUÈúÄË¶ÅÁªßÁª≠ËøõË°åÊú™ÂëΩ‰∏≠ÂíåÂëΩ‰∏≠Ê∑∑Âêà„ÄÇ Êàë‰ª¨Áß∞‰πã‰∏∫ÊµÅÁºìÂ≠òÊû∂ÊûÑ„ÄÇ GPU caches must deliver very high-bandwidth to their clients. Consider the case of a texture cache. A typical texture unit may evaluate two bilinear interpolations for each of four pixels per clock cycle, and a GPU may have many such texture units all operating independently. Each bilinear interpolation requires four separate texels, and each texel might be a 64-bit value. Four 16-bit components are typical. Thus, total bandwidth is 2 x 4 x 4 x 64 = 2048 bits per clock. Each separate 64-bit texel is independently addressed, so the cache needs to handle 32 unique addresses per clock. This naturally favors a multibank and/or multiport arrangement of SRAM arrays.GPUÁºìÂ≠òÂøÖÈ°ª‰∏∫ÂÖ∂ÂÆ¢Êà∑Êèê‰æõÈùûÂ∏∏È´òÁöÑÂ∏¶ÂÆΩ„ÄÇ ËÄÉËôëÁ∫πÁêÜÁºìÂ≠òÁöÑÊÉÖÂÜµ„ÄÇ ÂÖ∏ÂûãÁöÑÁ∫πÁêÜÂçïÂÖÉÂèØ‰ª•ÈíàÂØπÊØè‰∏™Êó∂ÈíüÂë®ÊúüÁöÑÂõõ‰∏™ÂÉèÁ¥†‰∏≠ÁöÑÊØè‰∏Ä‰∏™ËØÑ‰º∞‰∏§‰∏™ÂèåÁ∫øÊÄßÊèíÂÄºÔºåÂπ∂‰∏îGPUÂèØ‰ª•ÂÖ∑ÊúâÂÖ®ÈÉ®Áã¨Á´ãÊìç‰ΩúÁöÑËÆ∏Â§öËøôÊ†∑ÁöÑÁ∫πÁêÜÂçïÂÖÉ„ÄÇ ÊØè‰∏™ÂèåÁ∫øÊÄßÊèíÂÄºÈúÄË¶ÅÂõõ‰∏™ÂçïÁã¨ÁöÑÁ∫πÁ¥†ÔºåÊØè‰∏™Á∫πÁ¥†ÂèØËÉΩÊòØ64‰ΩçÂÄº„ÄÇ ÈÄöÂ∏∏ÊúâÂõõ‰∏™16‰ΩçÁªÑ‰ª∂„ÄÇ Âõ†Ê≠§ÔºåÊÄªÂ∏¶ÂÆΩÊòØÊØèÊó∂Èíü2√ó4√ó4√ó64 = 2048ÊØîÁâπ„ÄÇ ÊØè‰∏™Áã¨Á´ãÁöÑ64‰ΩçÁ∫πÁ¥†ÈÉΩÊòØÁã¨Á´ãÂØªÂùÄÁöÑÔºåÂõ†Ê≠§ÁºìÂ≠òÈúÄË¶ÅÊØè‰∏™Êó∂ÈíüÂ§ÑÁêÜ32‰∏™ÂîØ‰∏ÄÂú∞ÂùÄ„ÄÇ ËøôËá™ÁÑ∂ÊúâÂà©‰∫éSRAMÈòµÂàóÁöÑÂ§öÂ∫ìÂíå/ÊàñÂ§öÁ´ØÂè£Â∏ÉÁΩÆ„ÄÇ MMUModern GPUs are capable of translating virtual addresses to physical addresses. On the GeForce 8800, all processing units generate memory addresses in a 40-bit virtual address space. For computing, load and store thread instructions use 32-bit byte addresses, which are extended to a 40-bit virtual address by adding a 40-bit offset. A memory management unit performs virtual to physical address translation; hardware reads the page tables from local memory to respond to misses on behalf of a hierarchy of translation lookaside buffers spread out among the processors and rendering engines. In addition to physical page bits, GPU page table entries specify the compression algorithm for each page. Page sizes range from 4 to 128 kilobytes.Áé∞‰ª£GPUËÉΩÂ§üÂ∞ÜËôöÊãüÂú∞ÂùÄËΩ¨Êç¢‰∏∫Áâ©ÁêÜÂú∞ÂùÄ„ÄÇ Âú®GeForce 8800‰∏äÔºåÊâÄÊúâÂ§ÑÁêÜÂçïÂÖÉÈÉΩÂú®40‰ΩçËôöÊãüÂú∞ÂùÄÁ©∫Èó¥‰∏≠ÁîüÊàêÂÜÖÂ≠òÂú∞ÂùÄ„ÄÇ ÂØπ‰∫éËÆ°ÁÆóÔºåÂä†ËΩΩÂíåÂ≠òÂÇ®Á∫øÁ®ãÊåá‰ª§Ôºå‰ΩøÁî®32‰ΩçÂ≠óËäÇÂú∞ÂùÄÔºåÈÄöËøáÊ∑ªÂä†40‰ΩçÂÅèÁßªÈáèÂ∞ÜÂÖ∂Êâ©Â±ï‰∏∫40‰ΩçËôöÊãüÂú∞ÂùÄ„ÄÇ Â≠òÂÇ®Âô®ÁÆ°ÁêÜÂçïÂÖÉÊâßË°åËôöÊãüÂà∞Áâ©ÁêÜÂú∞ÂùÄËΩ¨Êç¢; Á°¨‰ª∂‰ªéÊú¨Âú∞Â≠òÂÇ®Âô®ËØªÂèñÈ°µË°®‰ª•‰ª£Ë°®Âú®Â§ÑÁêÜÂô®ÂíåÂëàÁé∞ÂºïÊìé‰πãÈó¥Â±ïÂºÄÁöÑËΩ¨Êç¢ÂêéÂ§áÁºìÂÜ≤Âô®ÁöÑÂ±ÇÊ¨°ÁªìÊûÑÊù•ÂìçÂ∫îÊú™ÂëΩ‰∏≠„ÄÇ Èô§‰∫ÜÁâ©ÁêÜÈ°µÈù¢‰Ωç‰πãÂ§ñÔºåGPUÈ°µË°®Êù°ÁõÆËøò‰∏∫ÊØè‰∏™È°µÈù¢ÊåáÂÆöÂéãÁº©ÁÆóÊ≥ï„ÄÇ È°µÈù¢Â§ßÂ∞èËåÉÂõ¥‰∏∫4Âà∞128ÂçÉÂ≠óËäÇ„ÄÇ Memory SpacesAs introduced in Section C.3, CUDA exposes different memory spaces to allow the programmer to store data values in the most performance-optimal way. For the following discussion, NVIDIA Tesla architecture GPUs are assumed.Â¶ÇC.3ËäÇÊâÄËø∞ÔºåCUDAÂÖ¨ÂºÄ‰∫Ü‰∏çÂêåÁöÑÂÜÖÂ≠òÁ©∫Èó¥Ôºå‰ª•ÂÖÅËÆ∏Á®ãÂ∫èÂëò‰ª•ÊúÄ‰Ω≥ÊÄßËÉΩÊúÄ‰Ω≥ÁöÑÊñπÂºèÂ≠òÂÇ®Êï∞ÊçÆÂÄº„ÄÇ ÂØπ‰∫é‰ª•‰∏ãËÆ®ËÆ∫ÔºåÂÅáËÆæ‰ΩøÁî®NVIDIA TeslaÊû∂ÊûÑGPU„ÄÇ Global memoryGlobal memory is stored in external DRAM; it is not local to any one physical streaming multiprocessor (SM) because it is meant for communication among different CTAs (thread blocks) in different grids. In fact, the many CTAs that reference a location in global memory may not be executing in the GPU at the same time; by design, in CUDA a programmer does not know the relative order in which CTAs are executed. Because the address space is evenly distributed among all memory partitions, there must be a read/write path from any streaming multiprocessor to any DRAM partition. 1399/5000ÂÖ®Â±ÄÂ≠òÂÇ®Âô®Â≠òÂÇ®Âú®Â§ñÈÉ®DRAM‰∏≠;ÂÆÉ‰∏çÊòØ‰ªª‰Ωï‰∏Ä‰∏™Áâ©ÁêÜÊµÅÂ§öÂ§ÑÁêÜÂô®ÔºàSMÔºâÁöÑÊú¨Âú∞ÔºåÂõ†‰∏∫ÂÆÉÁî®‰∫é‰∏çÂêåÁΩëÊ†º‰∏≠‰∏çÂêåCTAÔºàÁ∫øÁ®ãÂùóÔºâ‰πãÈó¥ÁöÑÈÄö‰ø°„ÄÇÂÆûÈôÖ‰∏äÔºåÂºïÁî®ÂÖ®Â±ÄÂ≠òÂÇ®Âô®‰∏≠ÁöÑ‰ΩçÁΩÆÁöÑËÆ∏Â§öCTAÂèØËÉΩ‰∏ç‰ºöÂêåÊó∂Âú®GPU‰∏≠ÊâßË°å;ÊåâÁÖßËÆæËÆ°ÔºåÂú®CUDA‰∏≠ÔºåÁ®ãÂ∫èÂëò‰∏çÁü•ÈÅìÊâßË°åCTAÁöÑÁõ∏ÂØπÈ°∫Â∫è„ÄÇÁî±‰∫éÂú∞ÂùÄÁ©∫Èó¥ÂùáÂåÄÂàÜÂ∏ÉÂú®ÊâÄÊúâÂÜÖÂ≠òÂàÜÂå∫‰∏≠ÔºåÂõ†Ê≠§ÂøÖÈ°ªÂ≠òÂú®‰ªé‰ªª‰ΩïÊµÅÂºèÂ§öÂ§ÑÁêÜÂô®Âà∞‰ªª‰ΩïDRAMÂàÜÂå∫ÁöÑËØª/ÂÜôË∑ØÂæÑ„ÄÇ Access to global memory by different threads (and diÔ¨Äerent processors) is not guaranteed to have sequential consistency. Thread programs see a relaxed memory ordering model. Within a thread, the order of memory reads and writes to the same address is preserved, but the order of accesses to different addresses may not be preserved. Memory reads and writes requested by different threads are unordered. Within a CTA, the barrier synchronization instruction bar.sync can be used to obtain strict memory ordering among the threads of the CTA. The membar thread instruction provides a memory barrier/fence operation that commits prior memory accesses and makes them visible to other threads before proceeding. Threads can also use the atomic memory operations described in Section C.4 to coordinate work on memory they share.‰∏çÂêåÁ∫øÁ®ãÔºàÂíå‰∏çÂêåÁöÑÂ§ÑÁêÜÂô®ÔºâÂØπÂÖ®Â±ÄÂÜÖÂ≠òÁöÑËÆøÈóÆ‰∏ç‰øùËØÅÂÖ∑ÊúâÈ°∫Â∫è‰∏ÄËá¥ÊÄß„ÄÇ Á∫øÁ®ãÁ®ãÂ∫èÁúãÂà∞ÊîæÊùæÁöÑÂÜÖÂ≠òÊéíÂ∫èÊ®°Âûã„ÄÇ Âú®Á∫øÁ®ãÂÜÖÔºå‰øùÁïô‰∫ÜÂØπÂêå‰∏ÄÂú∞ÂùÄÁöÑÂÜÖÂ≠òËØªÂèñÂíåÂÜôÂÖ•È°∫Â∫èÔºå‰ΩÜÂèØËÉΩÊó†Ê≥ï‰øùÁïôÂØπ‰∏çÂêåÂú∞ÂùÄÁöÑËÆøÈóÆÈ°∫Â∫è„ÄÇ ‰∏çÂêåÁ∫øÁ®ãËØ∑Ê±ÇÁöÑÂÜÖÂ≠òËØªÂèñÂíåÂÜôÂÖ•ÊòØÊó†Â∫èÁöÑ„ÄÇ Âú®CTA‰∏≠ÔºåÂ±èÈöúÂêåÊ≠•Êåá‰ª§bar.syncÂèØÁî®‰∫éÂú®CTAÁöÑÁ∫øÁ®ã‰πãÈó¥Ëé∑Âæó‰∏•Ê†ºÁöÑÂÜÖÂ≠òÊéíÂ∫è„ÄÇ membarÁ∫øÁ®ãÊåá‰ª§Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÜÖÂ≠òÂ±èÈöú/Âõ¥Ê†ÖÊìç‰ΩúÔºåÂÆÉÊèê‰∫§ÂÖàÂâçÁöÑÂÜÖÂ≠òËÆøÈóÆÔºåÂπ∂‰ΩøÂÖ∂Âú®ÁªßÁª≠‰πãÂâçÂØπÂÖ∂‰ªñÁ∫øÁ®ãÂèØËßÅ„ÄÇ Á∫øÁ®ãËøòÂèØ‰ª•‰ΩøÁî®Á¨¨C.4ËäÇ‰∏≠ÊèèËø∞ÁöÑÂéüÂ≠êÂÜÖÂ≠òÊìç‰ΩúÊù•ÂçèË∞ÉÂÆÉ‰ª¨ÂÖ±‰∫´ÁöÑÂÜÖÂ≠òÁöÑÂ∑•‰Ωú„ÄÇ Shared memoryPer-CTA shared memory is only visible to the threads that belong to that CTA, and shared memory only occupies storage from the time a CTA is created to the time it terminates. Shared memory can therefore reside on-chip. This approach has many benefits. First, shared memory traffic does not need to compete with limited off-chip bandwidth needed for global memory references. Second, it is practical to build very high-bandwidth memory structures on-chip to support the read/write demands of each streaming multiprocessor. In fact, the shared memory is closely coupled to the streaming multiprocessorPer-CTAÂÖ±‰∫´ÂÜÖÂ≠ò‰ªÖÂØπÂ±û‰∫éËØ•CTAÁöÑÁ∫øÁ®ãÂèØËßÅÔºåÂÖ±‰∫´ÂÜÖÂ≠ò‰ªÖÂç†Áî®‰ªéÂàõÂª∫CTAÂà∞ÁªàÊ≠¢Êó∂ÁöÑÂ≠òÂÇ®„ÄÇ Âõ†Ê≠§ÔºåÂÖ±‰∫´Â≠òÂÇ®Âô®ÂèØ‰ª•È©ªÁïôÂú®ËäØÁâá‰∏ä„ÄÇ ËøôÁßçÊñπÊ≥ïÊúâÂæàÂ§öÂ•ΩÂ§Ñ„ÄÇ È¶ñÂÖàÔºåÂÖ±‰∫´ÂÜÖÂ≠òÊµÅÈáè‰∏çÈúÄË¶Å‰∏éÂÖ®Â±ÄÂÜÖÂ≠òÂºïÁî®ÊâÄÈúÄÁöÑÊúâÈôêÁâáÂ§ñÂ∏¶ÂÆΩÁ´û‰∫â„ÄÇ ÂÖ∂Ê¨°ÔºåÂú®Áâá‰∏äÊûÑÂª∫ÈùûÂ∏∏È´òÂ∏¶ÂÆΩÁöÑÂ≠òÂÇ®Âô®ÁªìÊûÑ‰ª•ÊîØÊåÅÊØè‰∏™ÊµÅÂ§öÂ§ÑÁêÜÂô®ÁöÑËØª/ÂÜôÈúÄÊ±ÇÊòØÂÆûÁî®ÁöÑ„ÄÇ ÂÆûÈôÖ‰∏äÔºåÂÖ±‰∫´Â≠òÂÇ®Âô®‰∏éÊµÅÂºèÂ§öÂ§ÑÁêÜÂô®Á¥ßÂØÜËÄ¶Âêà Each streaming multiprocessor contains eight physical thread processors. During one shared memory clock cycle, each thread processor can process two threads‚Äô worth of instructions, so 16 threads‚Äô worth of shared memory requests must be handled in each clock. Because each thread can generate its own addresses, and the addresses are typically unique, the shared memory is built using 16 independently addressable SRAM banks. For common access patterns, 16 banks are sufcient to maintain throughput, but pathological cases are possible; for example, all 16 threads might happen to access a different address on one SRAM bank. It must be possible to route a request from any thread lane to any bank of SRAM, so a 16-by-16 interconnection network is required.ÊØè‰∏™ÊµÅÂºèÂ§öÂ§ÑÁêÜÂô®ÂåÖÂê´ÂÖ´‰∏™Áâ©ÁêÜÁ∫øÁ®ãÂ§ÑÁêÜÂô®„ÄÇ Âú®‰∏Ä‰∏™ÂÖ±‰∫´ÂÜÖÂ≠òÊó∂ÈíüÂë®ÊúüÂÜÖÔºåÊØè‰∏™Á∫øÁ®ãÂ§ÑÁêÜÂô®ÂèØ‰ª•Â§ÑÁêÜ‰∏§‰∏™Á∫øÁ®ãÁöÑÊåá‰ª§ÔºåÂõ†Ê≠§ÂøÖÈ°ªÂú®ÊØè‰∏™Êó∂Èíü‰∏≠Â§ÑÁêÜ16‰∏™Á∫øÁ®ãÁöÑÂÖ±‰∫´ÂÜÖÂ≠òËØ∑Ê±Ç„ÄÇ Áî±‰∫éÊØè‰∏™Á∫øÁ®ãÈÉΩÂèØ‰ª•ÁîüÊàêËá™Â∑±ÁöÑÂú∞ÂùÄÔºåÂπ∂‰∏îÂú∞ÂùÄÈÄöÂ∏∏ÊòØÂîØ‰∏ÄÁöÑÔºåÂõ†Ê≠§ÂÖ±‰∫´ÂÜÖÂ≠ò‰ΩøÁî®16‰∏™ÂèØÁã¨Á´ãÂØªÂùÄÁöÑSRAM bankÊûÑÂª∫„ÄÇ ÂØπ‰∫éÂ∏∏ËßÅÁöÑËÆøÈóÆÊ®°ÂºèÔºå16‰∏™Èì∂Ë°åË∂≥‰ª•Áª¥ÊåÅÂêûÂêêÈáèÔºå‰ΩÜÁóÖÁêÜÊÉÖÂÜµÊòØÂèØËÉΩÁöÑ; ‰æãÂ¶ÇÔºåÊâÄÊúâ16‰∏™Á∫øÁ®ãÂèØËÉΩÁ¢∞Â∑ßËÆøÈóÆ‰∏Ä‰∏™SRAMÁªÑ‰∏äÁöÑ‰∏çÂêåÂú∞ÂùÄ„ÄÇ ÂøÖÈ°ªÂèØ‰ª•Â∞ÜÊù•Ëá™‰ªª‰ΩïÁ∫øÁ®ãÈÄöÈÅìÁöÑËØ∑Ê±ÇË∑ØÁî±Âà∞‰ªª‰ΩïSRAMÁªÑÔºåÂõ†Ê≠§ÈúÄË¶Å16√ó16ÁöÑ‰∫íËøûÁΩëÁªú„ÄÇ Local MemoryPer-thread local memory is private memory visible only to a single thread. Local memory is architecturally larger than the thread‚Äôs register file, and a program can compute addresses into local memory. To support large allocations of local memory (recall the total allocation is the per-thread allocation times the number of active threads), local memory is allocated in external DRAM. Although global and per-thread local memory reside oÔ¨Ä-chip, they are wellsuited to being cached on-chip.ÊØèÁ∫øÁ®ãÊú¨Âú∞ÂÜÖÂ≠òÊòØ‰ªÖÂØπÂçï‰∏™Á∫øÁ®ãÂèØËßÅÁöÑÁßÅÊúâÂÜÖÂ≠ò„ÄÇ Êú¨Âú∞Â≠òÂÇ®Âô®Âú®‰ΩìÁ≥ªÁªìÊûÑ‰∏äÊØîÁ∫øÁ®ãÁöÑÂØÑÂ≠òÂô®Êñá‰ª∂Â§ßÔºåÂπ∂‰∏îÁ®ãÂ∫èÂèØ‰ª•Â∞ÜÂú∞ÂùÄËÆ°ÁÆóÂà∞Êú¨Âú∞Â≠òÂÇ®Âô®‰∏≠„ÄÇ ‰∏∫‰∫ÜÊîØÊåÅÊú¨Âú∞ÂÜÖÂ≠òÁöÑÂ§ßÈáèÂàÜÈÖçÔºàË∞ÉÁî®ÊÄªÂàÜÈÖçÊòØÊØèÁ∫øÁ®ãÂàÜÈÖç‰πò‰ª•Ê¥ªÂä®Á∫øÁ®ãÊï∞ÔºâÔºåÊú¨Âú∞ÂÜÖÂ≠òÂàÜÈÖçÂú®Â§ñÈÉ®DRAM‰∏≠„ÄÇ ËôΩÁÑ∂ÂÖ®Â±ÄÂíåÊØèÁ∫øÁ®ãÊú¨Âú∞ÂÜÖÂ≠òÈ©ªÁïôÂú®ËäØÁâá‰∏äÔºå‰ΩÜÂÆÉ‰ª¨ÈùûÂ∏∏ÈÄÇÂêàÂú®Áâá‰∏äÁºìÂ≠ò„ÄÇ Constant MemoryConstant memory is read-only to a program running on the SM (it can be written via commands to the GPU). It is stored in external DRAM and cached in the SM. Because commonly most or all threads in a SIMT warp read from the same address in constant memory, a single address lookup per clock is sufcient. The constant cache is designed to broadcast scalar values to threads in each warp.Â∏∏ÈáèÂ≠òÂÇ®Âô®ÂØπSM‰∏äËøêË°åÁöÑÁ®ãÂ∫èÊòØÂè™ËØªÁöÑÔºàÂèØ‰ª•ÈÄöËøáÂëΩ‰ª§ÂÜôÂÖ•GPUÔºâ„ÄÇ ÂÆÉÂ≠òÂÇ®Âú®Â§ñÈÉ®DRAM‰∏≠Âπ∂ÁºìÂ≠òÂú®SM‰∏≠„ÄÇ Âõ†‰∏∫SIMT warp‰∏≠ÁöÑÂ§ßÂ§öÊï∞ÊàñÊâÄÊúâÁ∫øÁ®ãÈÄöÂ∏∏‰ªéÂ∏∏ÈáèÂ≠òÂÇ®Âô®‰∏≠ÁöÑÁõ∏ÂêåÂú∞ÂùÄËØªÂèñÔºåÊâÄ‰ª•ÊØè‰∏™Êó∂ÈíüÁöÑÂçï‰∏™Âú∞ÂùÄÊü•ÊâæÊòØË∂≥Â§üÁöÑ„ÄÇ Â∏∏ÈáèÁºìÂ≠òÊó®Âú®ÂêëÊØè‰∏™warp‰∏≠ÁöÑÁ∫øÁ®ãÂπøÊí≠Ê†áÈáèÂÄº„ÄÇ Texture MemoryTexture memory holds large read-only arrays of data. Textures for computing have the same attributes and capabilities as textures used with 3D graphics. Although textures are commonly two-dimensional images (2D arrays of pixel values), 1D (linear) and 3D (volume) textures are also available.Á∫πÁêÜÂÜÖÂ≠òÂåÖÂê´Â§ßÈáèÂè™ËØªÊï∞ÊçÆÊï∞ÁªÑ„ÄÇ Áî®‰∫éËÆ°ÁÆóÁöÑÁ∫πÁêÜÂÖ∑Êúâ‰∏éÁî®‰∫é3DÂõæÂΩ¢ÁöÑÁ∫πÁêÜÁõ∏ÂêåÁöÑÂ±ûÊÄßÂíåËÉΩÂäõ„ÄÇ ËôΩÁÑ∂Á∫πÁêÜÈÄöÂ∏∏ÊòØ‰∫åÁª¥ÂõæÂÉèÔºàÂÉèÁ¥†ÂÄºÁöÑ2DÈòµÂàóÔºâÔºå‰ΩÜ‰πüÂèØ‰ª•‰ΩøÁî®1DÔºàÁ∫øÊÄßÔºâÂíå3DÔºà‰ΩìÁßØÔºâÁ∫πÁêÜ„ÄÇ A compute program references a texture using a tex instruction. Operands include an identifer to name the texture, and 1, 2, or 3 coordinates based on the texture dimensionality. Te Ô¨Çoating-point coordinates include a fractional portion that specifes a sample location, ofen in between texel locations. Noninteger coordinates invoke a bilinear weighted interpolation of the four closest values (for a 2D texture) before the result is returned to the program.ËÆ°ÁÆóÁ®ãÂ∫è‰ΩøÁî®texÊåá‰ª§ÂºïÁî®Á∫πÁêÜ„ÄÇ Êìç‰ΩúÊï∞ÂåÖÊã¨Áî®‰∫éÂëΩÂêçÁ∫πÁêÜÁöÑÊ†áËØÜÁ¨¶Ôºå‰ª•ÂèäÂü∫‰∫éÁ∫πÁêÜÁª¥Â∫¶ÁöÑ1,2Êàñ3‰∏™ÂùêÊ†á„ÄÇ ÊµÆÁÇπÂùêÊ†áÂåÖÊã¨ÊåáÂÆöÊ†∑Êú¨‰ΩçÁΩÆÁöÑÂ∞èÊï∞ÈÉ®ÂàÜÔºå‰Ωç‰∫éÁ∫πÁ¥†‰ΩçÁΩÆ‰πãÈó¥„ÄÇ Âú®Â∞ÜÁªìÊûúËøîÂõûÂà∞Á®ãÂ∫è‰πãÂâçÔºåÈùûÊï¥Êï∞ÂùêÊ†áË∞ÉÁî®Âõõ‰∏™ÊúÄÊé•ËøëÂÄºÔºàÂØπ‰∫é2DÁ∫πÁêÜÔºâÁöÑÂèåÁ∫øÊÄßÂä†ÊùÉÊèíÂÄº„ÄÇ Texture fetches are cached in a streaming cache hierarchy designed to optimize throughput of texture fetches from thousands of concurrent threads. Some programs use texture fetches as a way to cache global memory.Á∫πÁêÜÊèêÂèñÁºìÂ≠òÂú®ÊµÅÁºìÂ≠òÂ±ÇÊ¨°ÁªìÊûÑ‰∏≠ÔºåÊó®Âú®‰ºòÂåñÊù•Ëá™Êï∞ÂçÉ‰∏™Âπ∂ÂèëÁ∫øÁ®ãÁöÑÁ∫πÁêÜÊèêÂèñÁöÑÂêûÂêêÈáè„ÄÇ ‰∏Ä‰∫õÁ®ãÂ∫è‰ΩøÁî®Á∫πÁêÜÊèêÂèñ‰Ωú‰∏∫ÁºìÂ≠òÂÖ®Â±ÄÂÜÖÂ≠òÁöÑÊñπÊ≥ï„ÄÇ SurfacesSurface is a generic term for a one-dimensional, two-dimensional, or threedimensional array of pixel values and an associated format. A variety of formats are defned; for example, a pixel may be defned as four 8-bit RGBA integer components, or four 16-bit Ô¨Çoating-point components. A program kernel does not need to know the surface type. A tex instruction recasts its result values as Ô¨Çoating-point, depending on the surface format. Load/Store AccessLoad/store instructions with integer byte addressing enable the writing and compiling of programs in conventional languages like C and C++. CUDA programs use load/store instructions to access memory.Â∏¶ÊúâÊï¥Êï∞Â≠óËäÇÂØªÂùÄÁöÑÂä†ËΩΩ/Â≠òÂÇ®Êåá‰ª§ÂèØ‰ª•Áî®CÂíåC ++Á≠â‰º†ÁªüËØ≠Ë®ÄÁºñÂÜôÂíåÁºñËØëÁ®ãÂ∫è„ÄÇ CUDAÁ®ãÂ∫è‰ΩøÁî®Âä†ËΩΩ/Â≠òÂÇ®Êåá‰ª§Êù•ËÆøÈóÆÂÜÖÂ≠ò„ÄÇ To improve memory bandwidth and reduce overhead, the local and global load/store instructions coalesce individual parallel thread requests from the same warp together into a single memory block request when the addresses fall in the same block and meet alignment criteria. Coalescing individual small memory requests into large block requests provides a signifcant performance boost over separate requests. Te large thread count, together with support for many outstanding load requests, helps cover load-to-use latency for local and global memory implemented in external DRAM.‰∏∫‰∫ÜÊîπÂñÑÂ≠òÂÇ®Âô®Â∏¶ÂÆΩÂπ∂ÂáèÂ∞ëÂºÄÈîÄÔºåÂΩìÂú∞ÂùÄËêΩÂÖ•Âêå‰∏ÄÂùóÂπ∂Êª°Ë∂≥ÂØπÈΩêÊ†áÂáÜÊó∂ÔºåÊú¨Âú∞ÂíåÂÖ®Â±ÄÂä†ËΩΩ/Â≠òÂÇ®Êåá‰ª§Â∞ÜÊù•Ëá™Áõ∏ÂêåwarpÁöÑÂêÑ‰∏™Âπ∂Ë°åÁ∫øÁ®ãËØ∑Ê±ÇÂêàÂπ∂‰∏∫Âçï‰∏™Â≠òÂÇ®Âô®ÂùóËØ∑Ê±Ç„ÄÇ Â∞ÜÂçï‰∏™Â∞èÂÜÖÂ≠òËØ∑Ê±ÇÂêàÂπ∂Âà∞Â§ßÂùóËØ∑Ê±Ç‰∏≠ÂèØ‰ª•ÊòæÁùÄÊèêÂçáÂçïÁã¨ËØ∑Ê±ÇÁöÑÊÄßËÉΩ„ÄÇ Â§ßÁ∫øÁ®ãÊï∞‰ª•ÂèäÂØπËÆ∏Â§öÊú™ÂÆåÊàêÁöÑË¥üËΩΩËØ∑Ê±ÇÁöÑÊîØÊåÅÊúâÂä©‰∫éË¶ÜÁõñÂ§ñÈÉ®DRAM‰∏≠ÂÆûÁé∞ÁöÑÊú¨Âú∞ÂíåÂÖ®Â±ÄÂÜÖÂ≠òÁöÑË¥üËΩΩ‰ΩøÁî®Âª∂Ëøü„ÄÇ ROPAs shown in Figure C.2.5, NVIDIA Tesla architecture GPUs comprise a scalable streaming processor array (SPA), which performs all of the GPU‚Äôs programmabl calculations, and a scalable memory system, which comprises external DRAM control and fxed function Raster Operation Processors (ROPs) that perform color and depth framebuffer operations directly on memory. Each ROP unit is paired with a specifc memory partition. ROP partitions are fed from the SMs via an interconnection network. Each ROP is responsible for depth and stencil tests and updates, as well as color blending. The ROP and memory controllers cooperate to implement lossless color and depth compression (up to 8:1) to reduce external bandwidth demand. ROP units also perform atomic operations on memory.Â¶ÇÂõæC.2.5ÊâÄÁ§∫ÔºåNVIDIA TeslaÊû∂ÊûÑGPUÂåÖÊã¨‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÊµÅÂ§ÑÁêÜÂô®ÈòµÂàóÔºàSPAÔºâÔºåÂÆÉÊâßË°åÊâÄÊúâGPUÁöÑprogrammablËÆ°ÁÆóÔºå‰ª•Âèä‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÂ≠òÂÇ®Âô®Á≥ªÁªüÔºåÂåÖÊã¨Â§ñÈÉ®DRAMÊéßÂà∂ÂíåÂõ∫ÂÆöÂäüËÉΩÂÖâÊ†ÖÊìç‰ΩúÂ§ÑÁêÜÂô®ÔºàROPÔºâ ÔºâÁõ¥Êé•Âú®ÂÜÖÂ≠ò‰∏äÊâßË°åÈ¢úËâ≤ÂíåÊ∑±Â∫¶Â∏ßÁºìÂÜ≤Êìç‰Ωú„ÄÇ ÊØè‰∏™ROPÂçïÂÖÉ‰∏éÁâπÂÆöÁöÑÂÜÖÂ≠òÂàÜÂå∫ÈÖçÂØπ„ÄÇ ROPÂàÜÂå∫ÈÄöËøá‰∫íËøûÁΩëÁªú‰ªéSMÈ¶àÈÄÅ„ÄÇ ÊØè‰∏™ROPË¥üË¥£Ê∑±Â∫¶ÂíåÊ®°ÊùøÊµãËØïÂíåÊõ¥Êñ∞Ôºå‰ª•ÂèäÈ¢úËâ≤Ê∑∑Âêà„ÄÇ ROPÂíåÂÜÖÂ≠òÊéßÂà∂Âô®ÂçèÂêåÂ∑•‰ΩúÔºåÂÆûÁé∞Êó†ÊçüËâ≤ÂΩ©ÂíåÊ∑±Â∫¶ÂéãÁº©ÔºàÈ´òËææ8Ôºö1ÔºâÔºå‰ª•ÂáèÂ∞ëÂ§ñÈÉ®Â∏¶ÂÆΩÈúÄÊ±Ç„ÄÇ ROPÂçïÂÖÉËøòÂØπÂÜÖÂ≠òÊâßË°åÂéüÂ≠êÊìç‰Ωú„ÄÇ]]></content>
      <categories>
        <category>GPUÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÁøªËØë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4_Multithreaded_Multiprocessor_Architecture]]></title>
    <url>%2F2019%2F01%2F27%2FMultithreaded_Multiprocessor_Architecture%2F</url>
    <content type="text"><![CDATA[To address different market segments, GPUs implement scalable numbers of multiprocessors‚Äîin fact, GPUs are multiprocessors composed of multiprocessors. Furthermore, each multiprocessor is highly multithreaded to execute many fine-grained vertex and pixel shader threads efficiently. A quality basic GPU has two to four multiprocessors, while a gaming enthusiast‚Äôs GPU or computing platform has dozens of them. This section looks at the architecture of one such multithreaded multiprocessor, a simplifed version of the NVIDIA Tesla streaming multiprocessor (SM) described in Section C.7.‰∏∫‰∫ÜËß£ÂÜ≥‰∏çÂêåÁöÑÁªÜÂàÜÂ∏ÇÂú∫ÔºåGPUÂÆûÁé∞‰∫ÜÂèØÊâ©Â±ïÊï∞ÈáèÁöÑÂ§öÂ§ÑÁêÜÂô® - ÂÆûÈôÖ‰∏äÔºåGPUÊòØÁî±Â§öÂ§ÑÁêÜÂô®ÁªÑÊàêÁöÑÂ§öÂ§ÑÁêÜÂô®„ÄÇ Ê≠§Â§ñÔºåÊØè‰∏™Â§öÂ§ÑÁêÜÂô®ÈÉΩÊòØÈ´òÂ∫¶Â§öÁ∫øÁ®ãÁöÑÔºåÂèØ‰ª•ÊúâÊïàÂú∞ÊâßË°åËÆ∏Â§öÁªÜÁ≤íÂ∫¶ÁöÑÈ°∂ÁÇπÂíåÂÉèÁ¥†ÁùÄËâ≤Âô®Á∫øÁ®ã„ÄÇ È´òË¥®ÈáèÁöÑÂü∫Êú¨GPUÊúâ‰∏§Âà∞Âõõ‰∏™Â§öÂ§ÑÁêÜÂô®ÔºåËÄåÊ∏∏ÊàèÁà±Â•ΩËÄÖÁöÑGPUÊàñËÆ°ÁÆóÂπ≥Âè∞ÊúâÂá†ÂçÅ‰∏™„ÄÇ Êú¨ËäÇ‰ªãÁªç‰∏Ä‰∏™ËøôÊ†∑ÁöÑÂ§öÁ∫øÁ®ãÂ§öÂ§ÑÁêÜÂô®ÁöÑ‰ΩìÁ≥ªÁªìÊûÑÔºåËøôÊòØÁ¨¨C.7ËäÇ‰∏≠ÊèèËø∞ÁöÑNVIDIA TeslaÊµÅÂ§öÂ§ÑÁêÜÂô®ÔºàSMÔºâÁöÑÁÆÄÂåñÁâàÊú¨„ÄÇ Why use a multiprocessor, rather than several independent processors? The parallelism within each multiprocessor provides localized high performance and supports extensive multithreading for the fine-grained parallel programming models described in Section C.3. The individual threads of a thread block execute together within a multiprocessor to share data. The multithreaded multiprocessor design we describe here has eight scalar processor cores in a tightly coupled architecture, and executes up to 512 threads (the SM described in Section C.7 executes up to 768 threads). For area and power effciency, the multiprocessor shares large complex units among the eight processor cores, including the instruction cache, the multithreaded instruction unit, and the shared memory RAM.‰∏∫‰ªÄ‰πàË¶Å‰ΩøÁî®Â§öÂ§ÑÁêÜÂô®ÔºåËÄå‰∏çÊòØÂá†‰∏™Áã¨Á´ãÁöÑÂ§ÑÁêÜÂô®Ôºü ÊØè‰∏™Â§öÂ§ÑÁêÜÂô®ÂÜÖÁöÑÂπ∂Ë°åÊÄßÊèê‰æõ‰∫ÜÊú¨Âú∞ÂåñÁöÑÈ´òÊÄßËÉΩÔºåÂπ∂ÊîØÊåÅC.3ËäÇ‰∏≠ÊèèËø∞ÁöÑÁªÜÁ≤íÂ∫¶Âπ∂Ë°åÁºñÁ®ãÊ®°ÂûãÁöÑÂπøÊ≥õÂ§öÁ∫øÁ®ã„ÄÇ Á∫øÁ®ãÂùóÁöÑÂêÑ‰∏™Á∫øÁ®ãÂú®Â§öÂ§ÑÁêÜÂô®ÂÜÖ‰∏ÄËµ∑ÊâßË°å‰ª•ÂÖ±‰∫´Êï∞ÊçÆ„ÄÇ Êàë‰ª¨Âú®ËøôÈáåÊèèËø∞ÁöÑÂ§öÁ∫øÁ®ãÂ§öÂ§ÑÁêÜÂô®ËÆæËÆ°Âú®Á¥ßÂØÜËÄ¶Âêà(tightly coupled)ÁöÑÊû∂ÊûÑ‰∏≠ÊúâÂÖ´‰∏™Ê†áÈáèÂ§ÑÁêÜÂô®ÂÜÖÊ†∏ÔºåÂπ∂ÊâßË°åÂ§öËææ512‰∏™Á∫øÁ®ãÔºàC.7ËäÇ‰∏≠ÊèèËø∞ÁöÑSMÊâßË°åÂ§öËææ768‰∏™Á∫øÁ®ãÔºâ„ÄÇ ÂØπ‰∫éÈù¢ÁßØÂíåÂäüÁéáÊïàÁéáÔºåÂ§öÂ§ÑÁêÜÂô®Âú®ÂÖ´‰∏™Â§ÑÁêÜÂô®ÂÜÖÊ†∏‰πãÈó¥ÂÖ±‰∫´Â§ßÂûãÂ§çÊùÇÂçïÂÖÉÔºåÂåÖÊã¨Êåá‰ª§È´òÈÄüÁºìÂ≠òÔºåÂ§öÁ∫øÁ®ãÊåá‰ª§ÂçïÂÖÉÂíåÂÖ±‰∫´ÂÜÖÂ≠òRAM„ÄÇ Massive MultithreadingGPU processors are highly multithreaded to achieve several goals:GPUÂ§ÑÁêÜÂô®ÊòØÈ´òÂ∫¶Â§öÁ∫øÁ®ãÁöÑÔºå‰ª•ÂÆûÁé∞Âá†‰∏™ÁõÆÊ†áÔºö Cover the latency of memory loads and texture fetches from DRAM Ë¶ÜÁõñDRAM‰∏≠Â≠òÂÇ®Âô®Âä†ËΩΩÂíåÁ∫πÁêÜÊèêÂèñÁöÑÂª∂Ëøü Support fine-grained parallel graphics shader programming models ÊîØÊåÅÁªÜÁ≤íÂ∫¶Âπ∂Ë°åÂõæÂΩ¢ÁùÄËâ≤Âô®ÁºñÁ®ãÊ®°Âûã Support fine-grained parallel computing programming models ÊîØÊåÅÁªÜÁ≤íÂ∫¶Âπ∂Ë°åËÆ°ÁÆóÁºñÁ®ãÊ®°Âûã Virtualize the physical processors as threads and thread blocks to provide transparent scalability Â∞ÜÁâ©ÁêÜÂ§ÑÁêÜÂô®ËôöÊãüÂåñ‰∏∫Á∫øÁ®ãÂíåÁ∫øÁ®ãÂùóÔºå‰ª•Êèê‰æõÈÄèÊòéÁöÑÂèØ‰º∏Áº©ÊÄß Simplify the parallel programming model to writing a serial program for one thread ÁÆÄÂåñÂπ∂Ë°åÁºñÁ®ãÊ®°ÂûãÔºå‰∏∫‰∏Ä‰∏™Á∫øÁ®ãÁºñÂÜô‰∏≤Ë°åÁ®ãÂ∫è Memory and texture fetch latency can require hundreds of processor clocks, because GPUs typically have small streaming caches rather than large working-set caches like CPUs. A fetch request generally requires a full DRAM access latency plus interconnect and buÔ¨Äering latency. Multithreading helps cover the latency with useful computing‚Äîwhile one thread is waiting for a load or texture fetch to complete, the processor can execute another thread. The fne-grained parallel programming models provide literally thousands of independent threads that can keep many processors busy despite the long memory latency seen by individual threads.ÂÜÖÂ≠òÂíåÁ∫πÁêÜÊèêÂèñÂª∂ËøüÂèØËÉΩÈúÄË¶ÅÊï∞Áôæ‰∏™Â§ÑÁêÜÂô®Êó∂ÈíüÔºåÂõ†‰∏∫GPUÈÄöÂ∏∏ÂÖ∑ÊúâÂ∞èÂûãÊµÅÁºìÂ≠òÔºåËÄå‰∏çÊòØÂÉèCPUÈÇ£Ê†∑ÁöÑÂ§ßÂûãÂ∑•‰ΩúÈõÜÁºìÂ≠ò„ÄÇ Ëé∑ÂèñËØ∑Ê±ÇÈÄöÂ∏∏ÈúÄË¶ÅÂÆåÊï¥ÁöÑDRAMËÆøÈóÆÂª∂Ëøü‰ª•Âèä‰∫íËøûÂíåÁºìÂÜ≤Âª∂Ëøü„ÄÇ Â§öÁ∫øÁ®ãÊúâÂä©‰∫éÈÄöËøáÊúâÁî®ÁöÑËÆ°ÁÆóÊù•Âº•Ë°•Âª∂Ëøü - ÂΩì‰∏Ä‰∏™Á∫øÁ®ãÊ≠£Âú®Á≠âÂæÖÂä†ËΩΩÊàñÁ∫πÁêÜÊèêÂèñÂÆåÊàêÊó∂ÔºåÂ§ÑÁêÜÂô®ÂèØ‰ª•ÊâßË°åÂè¶‰∏Ä‰∏™Á∫øÁ®ã„ÄÇ Ëøô‰∫õÁªÜÁ≤íÂ∫¶ÁöÑÂπ∂Ë°åÁºñÁ®ãÊ®°ÂûãÊèê‰æõ‰∫ÜÊï∞ÂçÉ‰∏™Áã¨Á´ãÁöÑÁ∫øÁ®ãÔºåÂç≥‰ΩøÂêÑ‰∏™Á∫øÁ®ãÁúãÂà∞ÁöÑÂÜÖÂ≠òÂª∂ËøüÂæàÈïøÔºå‰πüÂèØ‰ª•‰ΩøËÆ∏Â§öÂ§ÑÁêÜÂô®‰øùÊåÅÂøôÁ¢åÁä∂ÊÄÅ„ÄÇ A graphics vertex or pixel shader program is a program for a single thread that processes a vertex or a pixel. Similarly, a CUDA program is a C program for a single thread that computes a result. Graphics and computing programs instantiate many parallel threads to render complex images and compute large result arrays. To dynamically balance shifting vertex and pixel shader thread workloads, each multiprocessor concurrently executes multiple different thread programs and different types of shader programs.ÂõæÂΩ¢È°∂ÁÇπÊàñÂÉèÁ¥†ÁùÄËâ≤Âô®Á®ãÂ∫èÊòØÁî®‰∫éÂ§ÑÁêÜÈ°∂ÁÇπÊàñÂÉèÁ¥†ÁöÑÂçï‰∏™Á∫øÁ®ãÁöÑÁ®ãÂ∫è„ÄÇ Á±ª‰ººÂú∞ÔºåCUDAÁ®ãÂ∫èÊòØÁî®‰∫éËÆ°ÁÆóÁªìÊûúÁöÑÂçï‰∏™Á∫øÁ®ãÁöÑCÁ®ãÂ∫è„ÄÇ ÂõæÂΩ¢ÂíåËÆ°ÁÆóÁ®ãÂ∫èÂÆû‰æãÂåñËÆ∏Â§öÂπ∂Ë°åÁ∫øÁ®ã‰ª•Ê∏≤ÊüìÂ§çÊùÇÂõæÂÉèÂπ∂ËÆ°ÁÆóÂ§ßÂûãÁªìÊûúÊï∞ÁªÑ„ÄÇ ‰∏∫‰∫ÜÂä®ÊÄÅÂπ≥Ë°°Áßª‰ΩçÈ°∂ÁÇπÂíåÂÉèÁ¥†ÁùÄËâ≤Âô®Á∫øÁ®ãÂ∑•‰ΩúË¥üËΩΩÔºåÊØè‰∏™Â§öÂ§ÑÁêÜÂô®ÂêåÊó∂ÊâßË°åÂ§ö‰∏™‰∏çÂêåÁöÑÁ∫øÁ®ãÁ®ãÂ∫èÂíå‰∏çÂêåÁ±ªÂûãÁöÑÁùÄËâ≤Âô®Á®ãÂ∫è„ÄÇ To support the independent vertex, primitive, and pixel programming model of graphics shading languages and the single-thread programming model of CUDA C/C++, each GPU thread has its own private registers, private per-thread memory, program counter, and thread execution state, and can execute an independent code path. To efciently execute hundreds of concurrent lightweight threads, the GPU multiprocessor is hardware multithreaded‚Äîit manages and executes hundreds of concurrent threads in hardware without scheduling overhead. Concurrent threads within thread blocks can synchronize at a barrier with a single instruction. Lightweight thread creation, zero-overhead thread scheduling, and fast barrier synchronization effciently support very fne-grained parallelism.‰∏∫‰∫ÜÊîØÊåÅÂõæÂΩ¢ÁùÄËâ≤ËØ≠Ë®ÄÁöÑÁã¨Á´ãÈ°∂ÁÇπÔºåÂéüÂßãÂíåÂÉèÁ¥†ÁºñÁ®ãÊ®°Âûã‰ª•ÂèäCUDA C / C ++ÁöÑÂçïÁ∫øÁ®ãÁºñÁ®ãÊ®°ÂûãÔºåÊØè‰∏™GPUÁ∫øÁ®ãÈÉΩÊúâËá™Â∑±ÁöÑÁßÅÊúâÂØÑÂ≠òÂô®ÔºåÁßÅÊúâÊØèÁ∫øÁ®ãÂÜÖÂ≠òÔºåÁ®ãÂ∫èËÆ°Êï∞Âô®ÂíåÁ∫øÁ®ãÊâßË°åÁä∂ÊÄÅÔºåÂèØ‰ª•ÊâßË°åÁã¨Á´ãÁöÑ‰ª£Á†ÅË∑ØÂæÑ„ÄÇ ‰∏∫‰∫ÜÊúâÊïàÂú∞ÊâßË°åÊï∞Áôæ‰∏™Âπ∂ÂèëËΩªÈáèÁ∫ßÁ∫øÁ®ãÔºåGPUÂ§öÂ§ÑÁêÜÂô®ÊòØÁ°¨‰ª∂Â§öÁ∫øÁ®ãÁöÑ - ÂÆÉÂú®Á°¨‰ª∂‰∏≠ÁÆ°ÁêÜÂíåÊâßË°åÊï∞Áôæ‰∏™Âπ∂ÂèëÁ∫øÁ®ãÔºåËÄåÊó†ÈúÄË∞ÉÂ∫¶ÂºÄÈîÄ„ÄÇ Á∫øÁ®ãÂùóÂÜÖÁöÑÂπ∂ÂèëÁ∫øÁ®ãÂèØ‰ª•‰ΩøÁî®Âçï‰∏™Êåá‰ª§Âú®Â±èÈöú‰∏äÂêåÊ≠•„ÄÇ ËΩªÈáèÁ∫ßÁ∫øÁ®ãÂàõÂª∫ÔºåÈõ∂ÂºÄÈîÄÁ∫øÁ®ãË∞ÉÂ∫¶ÂíåÂø´ÈÄüÂ±èÈöúÂêåÊ≠•ÊúâÊïàÂú∞ÊîØÊåÅÈùûÂ∏∏ÁªÜÁ≤íÂ∫¶ÁöÑÂπ∂Ë°åÊÄß„ÄÇ Multiprocessor Architecture Â§öÂ§ÑÁêÜÂô®Êû∂ÊûÑA unified graphics and computing multiprocessor executes vertex, geometry, and pixel fragment shader programs, and parallel computing programs. As Figure C.4.1 shows, the example multiprocessor consists of eight scalar processor (SP) cores each with a large multithreaded register fle (RF), two special function units (SFUs), a multithreaded instruction unit, an instruction cache, a read-only constant cache,and a shared memory.Áªü‰∏ÄÁöÑÂõæÂΩ¢ÂíåËÆ°ÁÆóÂ§öÂ§ÑÁêÜÂô®ÊâßË°åÈ°∂ÁÇπÔºåÂá†‰ΩïÂíåÂÉèÁ¥†ÁâáÊÆµÁùÄËâ≤Âô®Á®ãÂ∫è‰ª•ÂèäÂπ∂Ë°åËÆ°ÁÆóÁ®ãÂ∫è„ÄÇ Â¶ÇÂõæC.4.1ÊâÄÁ§∫ÔºåÁ§∫‰æãÂ§öÂ§ÑÁêÜÂô®Áî±8‰∏™Ê†áÈáèÂ§ÑÁêÜÂô®ÔºàSPÔºâÂÜÖÊ†∏ÁªÑÊàêÔºåÊØè‰∏™ÂÜÖÊ†∏ÂÖ∑Êúâ‰∏Ä‰∏™Â§ßÂûãÂ§öÁ∫øÁ®ãÂØÑÂ≠òÂô®ÔºàRFÔºâÔºå‰∏§‰∏™ÁâπÊÆäÂäüËÉΩÂçïÂÖÉÔºàSFUÔºâÔºå‰∏Ä‰∏™Â§öÁ∫øÁ®ãÊåá‰ª§ÂçïÂÖÉÔºå‰∏Ä‰∏™Êåá‰ª§È´òÈÄüÁºìÂ≠òÔºå‰∏Ä‰∏™ËØªÂèñÂô®„ÄÇ Âè™ÊúâÂ∏∏ÈáèÁºìÂ≠òÂíåÂÖ±‰∫´ÂÜÖÂ≠ò„ÄÇ The 16 KB shared memory holds graphics data buÔ¨Äers and shared computing data. CUDA variables declared as __shared__ reside in the shared memory. To map the logical graphics pipeline workload through the multiprocessor multiple times, as shown in Section C.2, vertex, geometry, and pixel threads have independent input and output buffers, and workloads arrive and depart independently of thread execution.16 KBÂÖ±‰∫´ÂÜÖÂ≠òÂèØÂ≠òÂÇ®ÂõæÂΩ¢Êï∞ÊçÆÁºìÂÜ≤Âå∫ÂíåÂÖ±‰∫´ËÆ°ÁÆóÊï∞ÊçÆ„ÄÇ Â£∞Êòé‰∏∫__shared__ ÁöÑCUDAÂèòÈáèÈ©ªÁïôÂú®ÂÖ±‰∫´ÂÜÖÂ≠ò‰∏≠„ÄÇ Ëã•Ë¶ÅÂ§öÊ¨°Êò†Â∞ÑÈÄªËæëÂõæÂΩ¢ÁÆ°ÈÅìÂ∑•‰ΩúË¥üËΩΩÈÄöËøáÂ§öÂ§ÑÁêÜÂô®ÔºåÂ¶ÇÁ¨¨C.2ËäÇÊâÄÁ§∫ÔºåÈ°∂ÁÇπÔºåÂá†‰ΩïÂíåÂÉèÁ¥†Á∫øÁ®ãÂÖ∑ÊúâÁã¨Á´ãÁöÑËæìÂÖ•ÂíåËæìÂá∫ÁºìÂÜ≤Âå∫ÔºåÂπ∂‰∏îÂ∑•‰ΩúË¥üËΩΩÁã¨Á´ã‰∫éÁ∫øÁ®ãÊâßË°åËÄåÂà∞ËææÂíåÁ¶ªÂºÄ„ÄÇ Each SP core contains scalar integer and Ô¨Çoating-point arithmetic units that execute most instructions. The SP is hardware multithreaded, supporting up to 64 threads. Each pipelined SP core executes one scalar instruction per thread per clock, which ranges from 1.2 GHz to 1.6 GHz in diÔ¨Äerent GPU products. Each SP core has a large RF of 1024 general-purpose 32-bit registers, partitioned among its assigned threads. Programs declare their register demand, typically 16 to 64 scalar 32-bit registers per thread. The SP can concurrently run many threads that use a few registers or fewer threads that use more registers. The compiler optimizes register allocation to balance the cost of spilling registers versus the cost of fewer threads. Pixel shader programs ofen use 16 or fewer registers, enabling each SP to run up to 64 pixel shader threads to cover long-latency texture fetches. Compiled CUDA programs ofen need 32 registers per thread, limiting each SP to 32 threads, which limits such a kernel program to 256 threads per thread block on this example multiprocessor, rather than its maximum of 512 threads.ÊØè‰∏™SPÂÜÖÊ†∏ÂåÖÂê´ÊâßË°åÂ§ßÂ§öÊï∞Êåá‰ª§ÁöÑÊ†áÈáèÊï¥Êï∞ÂíåÊµÆÁÇπËøêÁÆóÂçïÂÖÉ„ÄÇ SPÊòØÁ°¨‰ª∂Â§öÁ∫øÁ®ãÔºåÊúÄÂ§öÊîØÊåÅ64‰∏™Á∫øÁ®ã„ÄÇÊØè‰∏™ÊµÅÊ∞¥Á∫øSPÊ†∏ÂøÉÊØè‰∏™Êó∂ÈíüÊØè‰∏™Á∫øÁ®ãÊâßË°å‰∏Ä‰∏™Ê†áÈáèÊåá‰ª§ÔºåÂú®‰∏çÂêåÁöÑGPU‰∫ßÂìÅ‰∏≠ÔºåËåÉÂõ¥‰ªé1.2 GHzÂà∞1.6 GHz„ÄÇÊØè‰∏™SPÂÜÖÊ†∏ÈÉΩÊúâ‰∏Ä‰∏™1024‰∏™ÈÄöÁî®32‰ΩçÂØÑÂ≠òÂô®ÁöÑÂ§ßRFÔºåÂú®ÂÖ∂ÂàÜÈÖçÁöÑÁ∫øÁ®ã‰πãÈó¥ËøõË°åÂàÜÂå∫„ÄÇÁ®ãÂ∫èÂ£∞ÊòéÂÆÉ‰ª¨ÁöÑÂØÑÂ≠òÂô®ÈúÄÊ±ÇÔºåÈÄöÂ∏∏ÊØè‰∏™Á∫øÁ®ãÊúâ16Âà∞64‰∏™Ê†áÈáè32‰ΩçÂØÑÂ≠òÂô®„ÄÇ SPÂèØ‰ª•ÂêåÊó∂ËøêË°åËÆ∏Â§öÁ∫øÁ®ãÔºåËøô‰∫õÁ∫øÁ®ã‰ΩøÁî®Â∞ëÈáèÂØÑÂ≠òÂô®ÊàñÊõ¥Â∞ë‰ΩøÁî®Êõ¥Â§öÂØÑÂ≠òÂô®ÁöÑÁ∫øÁ®ã„ÄÇÁºñËØëÂô®‰ºòÂåñÂØÑÂ≠òÂô®ÂàÜÈÖç‰ª•Âπ≥Ë°°Ê∫¢Âá∫ÂØÑÂ≠òÂô®ÁöÑÊàêÊú¨‰∏éÊõ¥Â∞ëÁ∫øÁ®ãÁöÑÊàêÊú¨„ÄÇÂÉèÁ¥†ÁùÄËâ≤Âô®Á®ãÂ∫è‰ΩøÁî®16‰∏™ÊàñÊõ¥Â∞ëÁöÑÂØÑÂ≠òÂô®Ôºå‰ΩøÊØè‰∏™SPËÉΩÂ§üËøêË°åÂ§öËææ64‰∏™ÂÉèÁ¥†ÁùÄËâ≤Âô®Á∫øÁ®ãÔºå‰ª•Ë¶ÜÁõñÈïøÂª∂ËøüÁ∫πÁêÜÊèêÂèñ„ÄÇÁºñËØëÁöÑCUDAÁ®ãÂ∫èÊØè‰∏™Á∫øÁ®ãÈúÄË¶Å32‰∏™ÂØÑÂ≠òÂô®ÔºåÂ∞ÜÊØè‰∏™SPÈôêÂà∂‰∏∫32‰∏™Á∫øÁ®ãÔºåËøôÂú®Ëøô‰∏™Á§∫‰æãÂ§öÂ§ÑÁêÜÂô®‰∏äÂ∞ÜÊØè‰∏™Á∫øÁ®ãÂùóÁöÑÂÜÖÊ†∏Á®ãÂ∫èÈôêÂà∂‰∏∫256‰∏™Á∫øÁ®ãÔºåËÄå‰∏çÊòØÊúÄÂ§ö512‰∏™Á∫øÁ®ã„ÄÇ The pipelined SFUs execute thread instructions that compute special functions and interpolate pixel attributes from primitive vertex attributes. These instructions can execute concurrently with instructions on the SPs. The SFU is described later.ÊµÅÊ∞¥Á∫øSFUÊâßË°åËÆ°ÁÆóÁâπÊÆäÂáΩÊï∞ÁöÑÁ∫øÁ®ãÊåá‰ª§ÔºåÂπ∂‰ªéÂéüÂßãÈ°∂ÁÇπÂ±ûÊÄßÊèíÂÖ•ÂÉèÁ¥†Â±ûÊÄß„ÄÇ Ëøô‰∫õÊåá‰ª§ÂèØ‰ª•‰∏éSP‰∏äÁöÑÊåá‰ª§ÂêåÊó∂ÊâßË°å„ÄÇ SFUÂ∞ÜÂú®ÂêéÈù¢ÊèèËø∞„ÄÇ The multiprocessor executes texture fetch instructions on the texture unit via the texture interface, and uses the memory interface for external memory load, store, and atomic access instructions. These instructions can execute concurrently with instructions on the SPs. Shared memory access uses a low-latency interconnection network between the SP processors and the shared memory banks.Â§öÂ§ÑÁêÜÂô®ÈÄöËøáÁ∫πÁêÜÊé•Âè£Âú®Á∫πÁêÜÂçïÂÖÉ‰∏äÊâßË°åÁ∫πÁêÜËé∑ÂèñÊåá‰ª§ÔºåÂπ∂‰ΩøÁî®Â≠òÂÇ®Âô®Êé•Âè£ËøõË°åÂ§ñÈÉ®Â≠òÂÇ®Âô®Âä†ËΩΩÔºåÂ≠òÂÇ®ÂíåÂéüÂ≠êËÆøÈóÆÊåá‰ª§„ÄÇ Ëøô‰∫õÊåá‰ª§ÂèØ‰ª•‰∏éSP‰∏äÁöÑÊåá‰ª§ÂêåÊó∂ÊâßË°å„ÄÇ ÂÖ±‰∫´ÂÜÖÂ≠òËÆøÈóÆ‰ΩøÁî®SPÂ§ÑÁêÜÂô®ÂíåÂÖ±‰∫´ÂÜÖÂ≠òÂ∫ì‰πãÈó¥ÁöÑ‰ΩéÂª∂Ëøü‰∫íËøûÁΩëÁªú„ÄÇ Single-Instruction Multiple-Thread (SIMT)To manage and execute hundreds of threads running several different programs effciently, the multiprocessor employs a single-instruction multiple-thread (SIMT) architecture. It creates, manages, schedules, and executes concurrent threads in groups of parallel threads called warps. The term warp originates from weaving, the first parallel thread technology. The photograph in Figure C.4.2 shows a warp of parallel threads emerging from a loom. This example multiprocessor uses a SIMT warp size of 32 threads, executing four threads in each of the eight SP cores over four clocks. The Tesla SM multiprocessor described in Section C.7 also uses a warp size of 32 parallel threads, executing four threads per SP core for effciency on plentiful pixel threads and computing threads. Thread blocks consist of one or more warps.‰∏∫‰∫ÜÊúâÊïàÂú∞ÁÆ°ÁêÜÂíåÊâßË°åËøêË°åÂ§ö‰∏™‰∏çÂêåÁ®ãÂ∫èÁöÑÊï∞Áôæ‰∏™Á∫øÁ®ãÔºåÂ§öÂ§ÑÁêÜÂô®ÈááÁî®ÂçïÊåá‰ª§Â§öÁ∫øÁ®ãÔºàSIMTÔºâÊû∂ÊûÑ„ÄÇ ÂÆÉÂú®Áß∞‰∏∫warpsÁöÑÂπ∂Ë°åÁ∫øÁ®ãÁªÑ‰∏≠ÂàõÂª∫ÔºåÁÆ°ÁêÜÔºåË∞ÉÂ∫¶ÂíåÊâßË°åÂπ∂ÂèëÁ∫øÁ®ã„ÄÇ ÊúØËØ≠warpÊ∫ê‰∫éÁºñÁªáÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Âπ∂Ë°åÁ∫øÁ®ãÊäÄÊúØ„ÄÇ ÂõæC.4.2‰∏≠ÁöÑÁÖßÁâáÊòæÁ§∫‰∫Ü‰ªéÁªáÊú∫‰∏≠Âá∫Áé∞ÁöÑÂπ≥Ë°åÁ∫øÁöÑÁøòÊõ≤„ÄÇ Ê≠§Á§∫‰æãÂ§öÂ§ÑÁêÜÂô®‰ΩøÁî®32‰∏™Á∫øÁ®ãÁöÑSIMT warpÂ§ßÂ∞èÔºåÂú®Âõõ‰∏™Êó∂ÈíüÂÜÖÁöÑÂÖ´‰∏™SPÂÜÖÊ†∏‰∏≠ÁöÑÊØè‰∏Ä‰∏™‰∏≠ÊâßË°åÂõõ‰∏™Á∫øÁ®ã„ÄÇ Á¨¨C.7ËäÇ‰∏≠ÊèèËø∞ÁöÑTesla SMÂ§öÂ§ÑÁêÜÂô®Ëøò‰ΩøÁî®32‰∏™Âπ∂Ë°åÁ∫øÁ®ãÁöÑwarpÂ§ßÂ∞èÔºåÊØè‰∏™SPÊ†∏ÂøÉÊâßË°åÂõõ‰∏™Á∫øÁ®ãÔºå‰ª•‰æøÂú®‰∏∞ÂØåÁöÑÂÉèÁ¥†Á∫øÁ®ãÂíåËÆ°ÁÆóÁ∫øÁ®ã‰∏äÂÆûÁé∞ÊïàÁéá„ÄÇ Á∫øÁ®ãÂùóÁî±‰∏Ä‰∏™ÊàñÂ§ö‰∏™warpÁªÑÊàê„ÄÇ ÂõæC.4.2 SIMTÂ§öÁ∫øÁ®ãwarpË∞ÉÂ∫¶Ôºö Ë∞ÉÂ∫¶Á®ãÂ∫èÈÄâÊã©Â∞±Áª™warpÂπ∂ÂêåÊ≠•ÂêëÁªÑÊàêwarpÁöÑÂπ∂Ë°åÁ∫øÁ®ãÂèëÂá∫Êåá‰ª§„ÄÇ Áî±‰∫éwarpÊòØÁã¨Á´ãÁöÑÔºåÂõ†Ê≠§Ë∞ÉÂ∫¶Á®ãÂ∫èÂèØ‰ª•ÊØèÊ¨°ÈÄâÊã©‰∏çÂêåÁöÑwarp„ÄÇ This example SIMT multiprocessor manages a pool of 16 warps, a total of 512 threads. Individual parallel threads composing a warp are the same type and start together at the same program address, but are otherwise free to branch and execute independently. At each instruction issue time, the SIMT multithreaded instruction unit selects a warp that is ready to execute its next instruction, and then issues that instruction to the active threads of that warp. A SIMT instruction is broadcast synchronously to the active parallel threads of a warp; individual threads may be inactive due to independent branching or predication. In this multiprocessor, each SP scalar processor core executes an instruction for four individual threads of a warp using four clocks, reÔ¨Çecting the 4:1 ratio of warp threads to cores. single-instruction multiple-thread (SIMT): A processor architecture that applies one instruction to multiple independent threads in parallel. ‰∏ÄÁßçÂ§ÑÁêÜÂô®‰ΩìÁ≥ªÁªìÊûÑÔºåÂèØÂ∞Ü‰∏ÄÊù°Êåá‰ª§Âπ∂Ë°åÂ∫îÁî®‰∫éÂ§ö‰∏™Áã¨Á´ãÁ∫øÁ®ã„ÄÇ warp: The set of parallel threads that execute the same instruction together in a SIMT architecture. Âú®SIMT‰ΩìÁ≥ªÁªìÊûÑ‰∏≠‰∏ÄËµ∑ÊâßË°åÁõ∏ÂêåÊåá‰ª§ÁöÑÂπ∂Ë°åÁ∫øÁ®ãÈõÜ„ÄÇ SIMT processor architecture is akin to single-instruction multiple data (SIMD) design, which applies one instruction to multiple data lanes, but diÔ¨Äers in that SIMT applies one instruction to multiple independent threads in parallel, not just to multiple data lanes. An instruction for a SIMD processor controls a vector ofmultiple data lanes together, whereas an instruction for a SIMT processor controls an individual thread, and the SIMT instruction unit issues an instruction to a warp of independent parallel threads for efciency. Te SIMT processor fnds data-level parallelism among threads at runtime, analogous to the way a superscalar processor finds instruction-level parallelism among instructions at runtime.Ê≠§Á§∫‰æãSIMTÂ§öÂ§ÑÁêÜÂô®ÁÆ°ÁêÜ‰∏Ä‰∏™ÂåÖÂê´16‰∏™warpÁöÑÊ±†ÔºåÊÄªÂÖ±512‰∏™Á∫øÁ®ã„ÄÇ ÁªÑÊàêwarpÁöÑÂêÑ‰∏™Âπ∂Ë°åÁ∫øÁ®ãÊòØÁõ∏ÂêåÁöÑÁ±ªÂûãÔºåÂπ∂‰∏îÂú®Áõ∏ÂêåÁöÑÁ®ãÂ∫èÂú∞ÂùÄÂ§Ñ‰∏ÄËµ∑ÂºÄÂßãÔºå‰ΩÜÊòØÂèØ‰ª•Áã¨Á´ãÂú∞ÂàÜÊîØÂíåÊâßË°å„ÄÇ Âú®ÊØè‰∏™Êåá‰ª§ÂèëÂ∏ÉÊó∂ÔºåSIMTÂ§öÁ∫øÁ®ãÊåá‰ª§ÂçïÂÖÉÈÄâÊã©ÂáÜÂ§áÊâßË°åÂÖ∂‰∏ã‰∏ÄÊù°Êåá‰ª§ÁöÑwarpÔºåÁÑ∂ÂêéÂ∞ÜËØ•Êåá‰ª§ÂèëÂ∏ÉÂà∞ËØ•warpÁöÑÊ¥ªÂä®Á∫øÁ®ã„ÄÇ SIMTÊåá‰ª§‰∏éwarpÁöÑÊ¥ªÂä®Âπ∂Ë°åÁ∫øÁ®ãÂêåÊ≠•ÂπøÊí≠; Áî±‰∫éÁã¨Á´ãÁöÑÂàÜÊîØÊàñÈ¢ÑÊµãÔºåÂêÑ‰∏™Á∫øÁ®ãÂèØËÉΩ‰∏çÊ¥ªÂä®„ÄÇ Âú®Ëøô‰∏™Â§öÂ§ÑÁêÜÂô®‰∏≠ÔºåÊØè‰∏™SPÊ†áÈáèÂ§ÑÁêÜÂô®ÂÜÖÊ†∏‰ΩøÁî®Âõõ‰∏™Êó∂ÈíüÊâßË°å‰∏Ä‰∏™ÁªèÁ∫øÁöÑÂõõ‰∏™ÂçïÁã¨Á∫øÁ®ãÁöÑÊåá‰ª§ÔºåÂ∞ÜÁªèÁ∫øÁöÑ4Ôºö1ÊØîÁéáÂèçÊò†Âà∞ÂÜÖÊ†∏„ÄÇ A SIMT processor realizes full efficiency and performance when all threads of a warp take the same execution path. If threads of a warp diverge via a datadependent conditional branch, execution serializes for each branch path taken, and when all paths complete, the threads converge to the same execution path. For equal length paths, a divergent if-else code block is 50% efficient. The multiprocessor uses a branch synchronization stack to manage independent threads that diverge and converge. DiÔ¨Äerent warps execute independently at full speed regardless of whether they are executing common or disjoint code paths. As a result, SIMT GPUs are dramatically more efcient and Ô¨Çexible on branching code than earlier GPUs, as their warps are much narrower than the SIMD width of prior GPUs.ÂΩìwarpÁöÑÊâÄÊúâÁ∫øÁ®ãÈááÁî®Áõ∏ÂêåÁöÑÊâßË°åË∑ØÂæÑÊó∂ÔºåSIMTÂ§ÑÁêÜÂô®ÂÆûÁé∞ÂÖ®ÈÉ®ÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ Â¶ÇÊûúwarpÁöÑÁ∫øÁ®ãÈÄöËøáÊï∞ÊçÆÁõ∏ÂÖ≥ÁöÑÊù°‰ª∂ÂàÜÊîØÂèëÊï£ÔºåÂàôÊâßË°å‰∏∫ÊâÄÈááÁî®ÁöÑÊØè‰∏™ÂàÜÊîØË∑ØÂæÑËøõË°åÂ∫èÂàóÂåñÔºåÂπ∂‰∏îÂΩìÊâÄÊúâË∑ØÂæÑÂÆåÊàêÊó∂ÔºåÁ∫øÁ®ã‰ºöËÅöÂà∞Áõ∏ÂêåÁöÑÊâßË°åË∑ØÂæÑ„ÄÇ ÂØπ‰∫éÁ≠âÈïøË∑ØÂæÑÔºåÂèëÊï£ÁöÑif-else‰ª£Á†ÅÂùóÊïàÁéá‰∏∫50ÔºÖ„ÄÇ Â§öÂ§ÑÁêÜÂô®‰ΩøÁî®ÂàÜÊîØÂêåÊ≠•Â†ÜÊ†àÊù•ÁÆ°ÁêÜÂàÜÊï£ÂíåËÅöÂêàÁöÑÁã¨Á´ãÁ∫øÁ®ã„ÄÇ ‰∏çÂêåÁöÑwarpÂÖ®ÈÄüÁã¨Á´ãÊâßË°åÔºåÊó†ËÆ∫ÂÆÉ‰ª¨ÊòØÊâßË°åÂÖ¨ÂÖ±ËøòÊòØ‰∏çÁõ∏‰∫§ÁöÑ‰ª£Á†ÅË∑ØÂæÑ„ÄÇ Âõ†Ê≠§ÔºåSIMT GPUÂú®ÂàÜÊîØ‰ª£Á†Å‰∏äÊØîÊó©ÊúüGPUÊõ¥Âä†È´òÊïàÂíåÁÅµÊ¥ªÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑÁªèÁ∫øÊØîÂÖàÂâçGPUÁöÑSIMDÂÆΩÂ∫¶Á™ÑÂæóÂ§ö„ÄÇ In contrast with SIMD vector architectures, SIMT enables programmers to write thread-level parallel code for individual independent threads, as well as data-parallel code for many coordinated threads. For program correctness, the programmer can essentially ignore the SIMT execution attributes of warps; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge. In practice, this is analogous to the role of cache lines in traditional codes: cache line size can be safely ignored when designing for correctness but must be considered in the code structure when designing for peak performance.‰∏éSIMDÂêëÈáè‰ΩìÁ≥ªÁªìÊûÑÁõ∏ÊØîÔºåSIMT‰ΩøÁ®ãÂ∫èÂëòËÉΩÂ§ü‰∏∫ÂêÑ‰∏™Áã¨Á´ãÁ∫øÁ®ãÁºñÂÜôÁ∫øÁ®ãÁ∫ßÂπ∂Ë°å‰ª£Á†ÅÔºåÂπ∂‰∏∫ËÆ∏Â§öÂçèË∞ÉÁ∫øÁ®ãÁºñÂÜôÊï∞ÊçÆÂπ∂Ë°å‰ª£Á†Å„ÄÇ ÂØπ‰∫éÁ®ãÂ∫èÁöÑÊ≠£Á°ÆÊÄßÔºåÁ®ãÂ∫èÂëòÂü∫Êú¨‰∏äÂèØ‰ª•ÂøΩÁï•warpÁöÑSIMTÊâßË°åÂ±ûÊÄß; ÁÑ∂ËÄåÔºåÈÄöËøáÊ≥®ÊÑè‰ª£Á†ÅÂæàÂ∞ëÈúÄË¶ÅÁªèÁ∫ø‰∏≠ÁöÑÁ∫øÁ®ãÂèëÊï£ÔºåÂèØ‰ª•ÂÆûÁé∞ÊòæÁùÄÁöÑÊÄßËÉΩÊîπËøõ„ÄÇ ÂÆûÈôÖ‰∏äÔºåËøôÁ±ª‰ºº‰∫é‰º†Áªü‰ª£Á†Å‰∏≠ÁºìÂ≠òË°åÁöÑ‰ΩúÁî®ÔºöÂú®ËÆæËÆ°Ê≠£Á°ÆÊÄßÊó∂ÂèØ‰ª•ÂÆâÂÖ®Âú∞ÂøΩÁï•ÁºìÂ≠òË°åÂ§ßÂ∞èÔºå‰ΩÜÂú®ËÆæËÆ°Â≥∞ÂÄºÊÄßËÉΩÊó∂ÂøÖÈ°ªÂú®‰ª£Á†ÅÁªìÊûÑ‰∏≠ËÄÉËôë„ÄÇ SIMT Warp Execution and Divergence Ë∞ÅËÉΩÂëäËØâÊàëËøô‰∏™ÊÄé‰πàÁøªËØëÔºüThe SIMT approach of scheduling independent warps is more Ô¨Çexible than the scheduling of previous GPU architectures. A warp comprises parallel threads of the same type: vertex, geometry, pixel, or compute. The basic unit of pixel fragment shader processing is the 2-by-2 pixel quad implemented as four pixel shader threads. The multiprocessor controller packs the pixel quads into a warp. It similarly groups vertices and primitives into warps, and packs computing threads into a warp. A thread block comprises one or more warps. The SIMT design shares the instruction fetch and issue unit efciently across parallel threads of a warp, but requires a full warp of active threads to get full performance effciency.Ë∞ÉÂ∫¶Áã¨Á´ãwarpÁöÑSIMTÊñπÊ≥ïÊØîÂÖàÂâçGPUÊû∂ÊûÑÁöÑË∞ÉÂ∫¶Êõ¥ÁÅµÊ¥ª„ÄÇ Êâ≠Êõ≤ÂåÖÊã¨Áõ∏ÂêåÁ±ªÂûãÁöÑÂπ∂Ë°åÁ∫øÁ®ãÔºöÈ°∂ÁÇπÔºåÂá†‰ΩïÔºåÂÉèÁ¥†ÊàñËÆ°ÁÆó„ÄÇ ÂÉèÁ¥†ÁâáÊÆµÁùÄËâ≤Âô®Â§ÑÁêÜÁöÑÂü∫Êú¨Âçï‰ΩçÊòØÂÆûÁé∞‰∏∫Âõõ‰∏™ÂÉèÁ¥†ÁùÄËâ≤Âô®Á∫øÁ®ãÁöÑ2√ó2ÂÉèÁ¥†ÂõõËæπÂΩ¢„ÄÇ Â§öÂ§ÑÁêÜÂô®ÊéßÂà∂Âô®Â∞ÜÂÉèÁ¥†ÂõõËæπÂΩ¢ÊâìÂåÖÊàêÊâ≠Êõ≤„ÄÇ ÂÆÉÁ±ª‰ººÂú∞Â∞ÜÈ°∂ÁÇπÂíåÂü∫ÂÖÉÂàÜÁªÑ‰∏∫warpÔºåÂπ∂Â∞ÜËÆ°ÁÆóÁ∫øÁ®ãÊâìÂåÖÊàêwarp„ÄÇ Á∫øÁ®ãÂùóÂåÖÊã¨‰∏Ä‰∏™ÊàñÂ§ö‰∏™warp„ÄÇ SIMTËÆæËÆ°Âú®warpÁöÑÂπ∂Ë°åÁ∫øÁ®ã‰πãÈó¥ÊúâÊïàÂú∞ÂÖ±‰∫´Êåá‰ª§Ëé∑ÂèñÂíåÂèëÂ∏ÉÂçïÂÖÉÔºå‰ΩÜÈúÄË¶ÅÂÆåÊï¥ÁöÑÊ¥ªÂä®Á∫øÁ®ãwarpÊâçËÉΩËé∑ÂæóÂÆåÂÖ®ÁöÑÊÄßËÉΩÊïàÁéá„ÄÇ This unifed multiprocessor schedules and executes multiple warp types concurrently, allowing it to concurrently execute vertex and pixel warps. Its warp scheduler operates at less than the processor clock rate, because there are four thread lanes per processor core. During each scheduling cycle, it selects a warp to execute a SIMT warp instruction, as shown in Figure C.4.2. An issued warp-instruction executes as four sets of eight threads over four processor cycles of throughput. The processor pipeline uses several clocks of latency to complete each instruction. If the number of active warps times the clocks per warp exceeds the pipeline latency, the programmer can ignore the pipeline latency. For this multiprocessor, a round-robin schedule of eight warps has a period of 32 cycles between successive instructions for the same warp. If the program can keep 256 threads active per multiprocessor, instruction latencies up to 32 cycles can be hidden from an individual sequential thread. However, with few active warps, the processor pipeline depth becomes visible and may cause processors to stall.Ëøô‰∏™Áªü‰∏ÄÁöÑÂ§öÂ§ÑÁêÜÂô®ÂêåÊó∂Ë∞ÉÂ∫¶ÂíåÊâßË°åÂ§ö‰∏™warpÁ±ªÂûãÔºåÂÖÅËÆ∏ÂÆÉÂêåÊó∂ÊâßË°åÈ°∂ÁÇπÂíåÂÉèÁ¥†warp„ÄÇÂÆÉÁöÑwarpË∞ÉÂ∫¶Á®ãÂ∫è‰ª•‰Ωé‰∫éÂ§ÑÁêÜÂô®Êó∂ÈíüÈÄüÁéáËøêË°åÔºåÂõ†‰∏∫ÊØè‰∏™Â§ÑÁêÜÂô®ÂÜÖÊ†∏ÊúâÂõõ‰∏™Á∫øÁ®ãÈÄöÈÅì„ÄÇÂú®ÊØè‰∏™Ë∞ÉÂ∫¶Âë®Êúü‰∏≠ÔºåÂÆÉÈÄâÊã©‰∏Ä‰∏™warpÊù•ÊâßË°åSIMT warpÊåá‰ª§ÔºåÂ¶ÇÂõæC.4.2ÊâÄÁ§∫„ÄÇÂèëÂá∫ÁöÑwarp-instructionÂú®ÂêûÂêêÈáèÁöÑÂõõ‰∏™Â§ÑÁêÜÂô®Âë®ÊúüÂÜÖ‰Ωú‰∏∫ÂõõÁªÑÂÖ´‰∏™Á∫øÁ®ãÊâßË°å„ÄÇÂ§ÑÁêÜÂô®ÊµÅÊ∞¥Á∫ø‰ΩøÁî®Âá†‰∏™Âª∂ËøüÊó∂ÈíüÊù•ÂÆåÊàêÊØèÊù°Êåá‰ª§„ÄÇÂ¶ÇÊûúÊØè‰∏™warpÁöÑÊó∂ÈíüÁöÑÊ¥ªÂä®warpÊï∞‰πò‰ª•ÁÆ°ÈÅìÂª∂ËøüÔºåÂàôÁ®ãÂ∫èÂëòÂèØ‰ª•ÂøΩÁï•ÁÆ°ÈÅìÂª∂Ëøü„ÄÇÂØπ‰∫éËøôÁßçÂ§öÂ§ÑÁêÜÂô®ÔºåÂÖ´‰∏™warpÁöÑÂæ™ÁéØË∞ÉÂ∫¶Âú®Áõ∏ÂêåwarpÁöÑËøûÁª≠Êåá‰ª§‰πãÈó¥ÂÖ∑Êúâ32‰∏™Âë®ÊúüÁöÑÂë®Êúü„ÄÇÂ¶ÇÊûúÁ®ãÂ∫èÂèØ‰ª•Âú®ÊØè‰∏™Â§öÂ§ÑÁêÜÂô®‰∏≠‰øùÊåÅ256‰∏™Á∫øÁ®ãÂ§Ñ‰∫éÊ¥ªÂä®Áä∂ÊÄÅÔºåÂàôÂèØ‰ª•‰ªéÂçï‰∏™È°∫Â∫èÁ∫øÁ®ã‰∏≠ÈöêËóèÊúÄÂ§ö32‰∏™Âë®ÊúüÁöÑÊåá‰ª§Âª∂Ëøü„ÄÇ‰ΩÜÊòØÔºåÁî±‰∫éÂá†‰πéÊ≤°ÊúâÊ¥ªÂä®warpÔºåÂ§ÑÁêÜÂô®ÁÆ°ÈÅìÊ∑±Â∫¶ÂèòÂæóÂèØËßÅÔºåÂπ∂ÂèØËÉΩÂØºËá¥Â§ÑÁêÜÂô®ÂÅúÊ≠¢„ÄÇ A challenging design problem is implementing zero-overhead warp scheduling for a dynamic mix of diÔ¨Äerent warp programs and program types. The instruction scheduler must select a warp every four clocks to issue one instruction per clock per thread, equivalent to an IPC of 1.0 per processor core. Because warps are independent, the only dependences are among sequential instructions from the same warp. The scheduler uses a register dependency scoreboard to qualify warps whose active threads are ready to execute an instruction. It prioritizes all such ready warps and selects the highest priority one for issue. Prioritization must consider warp type, instruction type, and the desire to be fair to all active warps.‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑËÆæËÆ°ÈóÆÈ¢òÊòØ‰∏∫‰∏çÂêåÁöÑwarpÁ®ãÂ∫èÂíåÁ®ãÂ∫èÁ±ªÂûãÁöÑÂä®ÊÄÅÁªÑÂêàÂÆûÁé∞Èõ∂ÂºÄÈîÄwarpË∞ÉÂ∫¶„ÄÇ Êåá‰ª§Ë∞ÉÂ∫¶Á®ãÂ∫èÂøÖÈ°ªÊØèÂõõ‰∏™Êó∂ÈíüÈÄâÊã©‰∏Ä‰∏™warpÔºåÊØè‰∏™Á∫øÁ®ãÊØè‰∏™Êó∂ÈíüÂèëÂá∫‰∏ÄÊù°Êåá‰ª§ÔºåÁõ∏ÂΩì‰∫éÊØè‰∏™Â§ÑÁêÜÂô®ÂÜÖÊ†∏ÁöÑIPC‰∏∫1.0„ÄÇ Áî±‰∫éwarpÊòØÁã¨Á´ãÁöÑÔºåÂîØ‰∏ÄÁöÑ‰æùËµñÊòØÊù•Ëá™Âêå‰∏ÄwarpÁöÑÈ°∫Â∫èÊåá‰ª§„ÄÇ Ë∞ÉÂ∫¶Á®ãÂ∫è‰ΩøÁî®ÂØÑÂ≠òÂô®‰æùËµñÊÄßËÆ∞ÂàÜÊùøÊù•ÈôêÂÆöÂÖ∂Ê¥ªÂä®Á∫øÁ®ãÂ∑≤ÂáÜÂ§áÂ•ΩÊâßË°åÊåá‰ª§ÁöÑwarp„ÄÇ ÂÆÉ‰ºòÂÖàËÄÉËôëÊâÄÊúâËøô‰∫õÂáÜÂ§áÂ•ΩÁöÑwarpÂπ∂ÈÄâÊã©ÊúÄ‰ºòÂÖàÁöÑwarp„ÄÇ ‰ºòÂÖàÁ∫ßÂøÖÈ°ªËÄÉËôëwarpÁ±ªÂûãÔºåÊåá‰ª§Á±ªÂûã‰ª•ÂèäÂØπÊâÄÊúâÊ¥ªÂä®warpÂÖ¨Âπ≥ÁöÑÊÑøÊúõ„ÄÇ Managing Threads and Thread Blocks ÁÆ°ÁêÜÁ∫øÁ®ãÂíåÁ∫øÁ®ãÂùóThe multiprocessor controller and instruction unit manage threads and thread blocks. The controller accepts work requests and input data and arbitrates access to shared resources, including the texture unit, memory access path, and I/O paths. For graphics workloads, it creates and manages three types of graphics threads concurrently: vertex, geometry, and pixel. Each of the graphics work types has independent input and output paths. It accumulates and packs each of these input work types into SIMT warps of parallel threads executing the same thread program. It allocates a free warp, allocates registers for the warp threads, and starts warp execution in the multiprocessor. Every program declares its perthread register demand; the controller starts a warp only when it can allocate the requested register count for the warp threads. When all the threads of the warp exit, the controller unpacks the results and frees the warp registers and resources.Â§öÂ§ÑÁêÜÂô®ÊéßÂà∂Âô®ÂíåÊåá‰ª§ÂçïÂÖÉÁÆ°ÁêÜÁ∫øÁ®ãÂíåÁ∫øÁ®ãÂùó„ÄÇ ÊéßÂà∂Âô®Êé•ÂèóÂ∑•‰ΩúËØ∑Ê±ÇÂíåËæìÂÖ•Êï∞ÊçÆÔºåÂπ∂‰ª≤Ë£ÅÂØπÂÖ±‰∫´ËµÑÊ∫êÁöÑËÆøÈóÆÔºåÂåÖÊã¨Á∫πÁêÜÂçïÂÖÉÔºåÂÜÖÂ≠òËÆøÈóÆË∑ØÂæÑÂíåI / OË∑ØÂæÑ„ÄÇ ÂØπ‰∫éÂõæÂΩ¢Â∑•‰ΩúË¥üËΩΩÔºåÂÆÉÂêåÊó∂ÂàõÂª∫ÂíåÁÆ°ÁêÜ‰∏âÁßçÁ±ªÂûãÁöÑÂõæÂΩ¢Á∫øÁ®ãÔºöÈ°∂ÁÇπÔºåÂá†‰ΩïÂíåÂÉèÁ¥†„ÄÇ ÊØè‰∏™ÂõæÂΩ¢Â∑•‰ΩúÁ±ªÂûãÈÉΩÊúâÁã¨Á´ãÁöÑËæìÂÖ•ÂíåËæìÂá∫Ë∑ØÂæÑ„ÄÇ ÂÆÉÂ∞ÜËøô‰∫õËæìÂÖ•Â∑•‰ΩúÁ±ªÂûã‰∏≠ÁöÑÊØè‰∏Ä‰∏™Á¥ØÁßØÂπ∂ÊâìÂåÖÂà∞ÊâßË°åÁõ∏ÂêåÁ∫øÁ®ãÁ®ãÂ∫èÁöÑÂπ∂Ë°åÁ∫øÁ®ãÁöÑSIMT warp‰∏≠„ÄÇ ÂÆÉÂàÜÈÖç‰∏Ä‰∏™Ëá™Áî±warpÔºå‰∏∫warpÁ∫øÁ®ãÂàÜÈÖçÂØÑÂ≠òÂô®ÔºåÂπ∂Âú®Â§öÂ§ÑÁêÜÂô®‰∏≠ÂêØÂä®warpÊâßË°å„ÄÇ ÊØè‰∏™Á®ãÂ∫èÈÉΩÂ£∞ÊòéÂÖ∂ÂØÑÂ≠òÂô®ÈúÄÊ±Ç; Âè™ÊúâÂΩìÊéßÂà∂Âô®ÂèØ‰ª•‰∏∫ÁªèÁ∫øÂàÜÈÖçÊâÄËØ∑Ê±ÇÁöÑÂØÑÂ≠òÂô®ËÆ°Êï∞Êó∂ÔºåÊéßÂà∂Âô®Êâç‰ºöÂêØÂä®warp„ÄÇ ÂΩìwarpÁöÑÊâÄÊúâÁ∫øÁ®ãÈÄÄÂá∫Êó∂ÔºåÊéßÂà∂Âô®Â∞ÜËß£ÂéãÁº©ÁªìÊûúÂπ∂ÈáäÊîæwarpÂØÑÂ≠òÂô®ÂíåËµÑÊ∫ê„ÄÇ The controller creates cooperative thread arrays (CTAs) which implement CUDA thread blocks as one or more warps of parallel threads. It creates a CTA when it can create all CTA warps and allocate all CTA resources. In addition to threads and registers, a CTA requires allocating shared memory and barriers. The program declares the required capacities, and the controller waits until it can allocate those amounts before launching the CTA. Then it creates CTA warps at the warp scheduling rate, so that a CTA program starts executing immediately at full multiprocessor performance. The controller monitors when all threads of a CTA have exited, and frees the CTA shared resources and its warp resources.ÊéßÂà∂Âô®ÂàõÂª∫Âçè‰ΩúÁ∫øÁ®ãÈòµÂàóÔºàCTAÔºâÔºåÂÖ∂Â∞ÜCUDAÁ∫øÁ®ãÂùóÂÆûÁé∞‰∏∫‰∏Ä‰∏™ÊàñÂ§ö‰∏™Âπ∂Ë°åÁ∫øÁ®ãÁöÑwarp„ÄÇ ÂÆÉÂèØ‰ª•Âú®ÂàõÂª∫ÊâÄÊúâCTA warpÂπ∂ÂàÜÈÖçÊâÄÊúâCTAËµÑÊ∫êÊó∂ÂàõÂª∫CTA„ÄÇ Èô§Á∫øÁ®ãÂíåÂØÑÂ≠òÂô®Â§ñÔºåCTAËøòÈúÄË¶ÅÂàÜÈÖçÂÖ±‰∫´ÂÜÖÂ≠òÂíåÈöúÁ¢ç„ÄÇ Á®ãÂ∫èÂ£∞ÊòéÊâÄÈúÄÁöÑÂÆπÈáèÔºåÊéßÂà∂Âô®Á≠âÂæÖÔºåÁõ¥Âà∞ÂÆÉÂèØ‰ª•Âú®ÂêØÂä®CTA‰πãÂâçÂàÜÈÖçËøô‰∫õÊï∞Èáè„ÄÇ ÁÑ∂ÂêéÂÆÉ‰ª•warpË∞ÉÂ∫¶ÈÄüÁéáÂàõÂª∫CTA warpÔºå‰ª•‰æøCTAÁ®ãÂ∫èÂú®ÂÆåÂÖ®Â§öÂ§ÑÁêÜÂô®ÊÄßËÉΩÊó∂Á´ãÂç≥ÂºÄÂßãÊâßË°å„ÄÇ TeÊéßÂà∂Âô®ÁõëËßÜCTAÁöÑÊâÄÊúâÁ∫øÁ®ã‰ΩïÊó∂ÈÄÄÂá∫ÔºåÂπ∂ÈáäÊîæCTAÂÖ±‰∫´ËµÑÊ∫êÂèäÂÖ∂warpËµÑÊ∫ê„ÄÇ cooperative thread array (CTA) : A set of concurrent threads that executes the same thread program and may cooperate to compute a result. A GPU CTA implements a CUDA thread block. ‰∏ÄÁªÑÂπ∂ÂèëÁ∫øÁ®ãÔºåÂÆÉ‰ª¨ÊâßË°åÁõ∏ÂêåÁöÑÁ∫øÁ®ãÁ®ãÂ∫èÂπ∂ÂèØÂçè‰ΩúËÆ°ÁÆóÁªìÊûú„ÄÇ GPU CTAÂÆûÁé∞‰∫ÜCUDAÁ∫øÁ®ãÂùó„ÄÇ Thread Instructions Á∫øÁ®ãÊåá‰ª§The SP thread processors execute scalar instructions for individual threads, unlike earlier GPU vector instruction architectures, which executed four-component vector instructions for each vertex or pixel shader program. Vertex programs generally compute (x, y, z, w) position vectors, while pixel shader programs compute (red, green, blue, alpha) color vectors. However, shader programs are becoming longer and more scalar, and it is increasingly difcult to fully occupy even two components of a legacy GPU four-component vector architecture. In effect, the SIMT architecture parallelizes across 32 independent pixel threads, rather than parallelizing the four vector components within a pixel. CUDA C/C++ programs have predominantly scalar code per thread. Previous GPUs employed vector packing (e.g., combining subvectors of work to gain efciency) but that complicated the scheduling hardware as well as the compiler. Scalar instructions are simpler and compiler friendly. Texture instructions remain vector based, taking a source coordinate vector and returning a filtered color vector.SPÁ∫øÁ®ãÂ§ÑÁêÜÂô®ÊâßË°åÂêÑ‰∏™Á∫øÁ®ãÁöÑÊ†áÈáèÊåá‰ª§Ôºå‰∏çÂÉèÊó©ÊúüÁöÑGPUÁü¢ÈáèÊåá‰ª§Êû∂ÊûÑÔºåÂêéËÄÖ‰∏∫ÊØè‰∏™È°∂ÁÇπÊàñÂÉèÁ¥†ÁùÄËâ≤Âô®Á®ãÂ∫èÊâßË°åÂõõÂàÜÈáèÁü¢ÈáèÊåá‰ª§„ÄÇÈ°∂ÁÇπÁ®ãÂ∫èÈÄöÂ∏∏ËÆ°ÁÆóÔºàxÔºåyÔºåzÔºåwÔºâ‰ΩçÁΩÆÁü¢ÈáèÔºåËÄåÂÉèÁ¥†ÁùÄËâ≤Âô®Á®ãÂ∫èËÆ°ÁÆóÔºàÁ∫¢Ëâ≤ÔºåÁªøËâ≤ÔºåËìùËâ≤ÔºåalphaÔºâÈ¢úËâ≤Áü¢Èáè„ÄÇÁÑ∂ËÄåÔºåÁùÄËâ≤Âô®Á®ãÂ∫èÂèòÂæóË∂äÊù•Ë∂äÈïøÂπ∂‰∏îË∂äÊù•Ë∂äÊ†áÈáèÂåñÔºåÂπ∂‰∏îÁîöËá≥ÂÆåÂÖ®Âç†ÊçÆ‰º†ÁªüGPUÂõõÂàÜÈáèÁü¢Èáè‰ΩìÁ≥ªÁªìÊûÑÁöÑ‰∏§‰∏™ÁªÑ‰ª∂‰πüÂèòÂæóË∂äÊù•Ë∂äÂõ∞Èöæ„ÄÇÂÆûÈôÖ‰∏äÔºåSIMTÊû∂ÊûÑÂú®32‰∏™Áã¨Á´ãÂÉèÁ¥†Á∫øÁ®ã‰πãÈó¥Âπ∂Ë°åÂåñÔºåËÄå‰∏çÊòØÂπ∂Ë°åÂåñÂÉèÁ¥†ÂÜÖÁöÑÂõõ‰∏™Áü¢ÈáèÂàÜÈáè„ÄÇ CUDA C / C ++Á®ãÂ∫èÊØè‰∏™Á∫øÁ®ã‰∏ªË¶ÅÊúâÊ†áÈáè‰ª£Á†Å„ÄÇÂÖàÂâçÁöÑGPUÈááÁî®ÂêëÈáèÊâìÂåÖÔºà‰æãÂ¶ÇÔºåÁªÑÂêàÂ∑•‰ΩúÁöÑÂ≠êÂêëÈáè‰ª•Ëé∑ÂæóÊïàÁéáÔºâ‰ΩÜÊòØ‰ΩøË∞ÉÂ∫¶Á°¨‰ª∂‰ª•ÂèäÁºñËØëÂô®Â§çÊùÇÂåñ„ÄÇÊ†áÈáèÊåá‰ª§Êõ¥ÁÆÄÂçïÔºåÁºñËØëÂèãÂ•Ω„ÄÇÁ∫πÁêÜÊåá‰ª§‰øùÊåÅÂü∫‰∫éÁü¢ÈáèÔºåÈááÁî®Ê∫êÂùêÊ†áÂêëÈáèÂπ∂ËøîÂõûÊª§Ê≥¢ÂêéÁöÑÈ¢úËâ≤ÂêëÈáè„ÄÇ To support multiple GPUs with diÔ¨Äerent binary microinstruction formats, highlevel graphics and computing language compilers generate intermediate assemblerlevel instructions (e.g., Direct3D vector instructions or PTX scalar instructions), which are then optimized and translated to binary GPU microinstructions. The NVIDIA PTX (parallel thread execution) instruction set defnition [2007] provides a stable target ISA for compilers, and provides compatibility over several generations of GPUs with evolving binary microinstruction-set architectures. The optimizer readily expands Direct3D vector instructions to multiple scalar binary microinstructions. PTX scalar instructions translate nearly one to one with scalar binary microinstructions, although some PTX instructions expand to multiple binary microinstructions, and multiple PTX instructions may fold into one binary microinstruction. Because the intermediate assembler-level instructions use virtual registers, the optimizer analyzes data dependencies and allocates real registers. The optimizer eliminates dead code, folds instructions together when feasible, and optimizes SIMT branch diverge and converge points.‰∏∫‰∫ÜÊîØÊåÅÂÖ∑Êúâ‰∏çÂêå‰∫åËøõÂà∂ÂæÆÊåá‰ª§Ê†ºÂºèÁöÑÂ§ö‰∏™GPUÔºåÈ´òÁ∫ßÂõæÂΩ¢ÂíåËÆ°ÁÆóËØ≠Ë®ÄÁºñËØëÂô®ÁîüÊàê‰∏≠Èó¥Ê±áÁºñÁ∫ßÊåá‰ª§Ôºà‰æãÂ¶ÇÔºåDirect3DÂêëÈáèÊåá‰ª§ÊàñPTXÊ†áÈáèÊåá‰ª§ÔºâÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂‰ºòÂåñÂπ∂ËΩ¨Êç¢‰∏∫‰∫åËøõÂà∂GPUÂæÆÊåá‰ª§„ÄÇ NVIDIA PTXÔºàÂπ∂Ë°åÁ∫øÁ®ãÊâßË°åÔºâÊåá‰ª§ÈõÜÂÆö‰πâ[2007]‰∏∫ÁºñËØëÂô®Êèê‰æõ‰∫ÜÁ®≥ÂÆöÁöÑÁõÆÊ†áISAÔºåÂπ∂Êèê‰æõ‰∫ÜÂá†‰ª£GPU‰∏é‰∏çÊñ≠ÂèëÂ±ïÁöÑ‰∫åËøõÂà∂ÂæÆÊåá‰ª§ÈõÜÊû∂ÊûÑÁöÑÂÖºÂÆπÊÄß„ÄÇ‰ºòÂåñÂô®ÂæàÂÆπÊòìÂ∞ÜDirect3DÂêëÈáèÊåá‰ª§Êâ©Â±ï‰∏∫Â§ö‰∏™Ê†áÈáè‰∫åËøõÂà∂ÂæÆÊåá‰ª§„ÄÇÂ∞ΩÁÆ°‰∏Ä‰∫õPTXÊåá‰ª§Êâ©Â±ïÂà∞Â§ö‰∏™‰∫åËøõÂà∂ÂæÆÊåá‰ª§Ôºå‰ΩÜPTXÊ†áÈáèÊåá‰ª§Âá†‰πé‰∏ÄÂØπ‰∏ÄÂú∞ËΩ¨Êç¢‰∏∫Ê†áÈáè‰∫åËøõÂà∂ÂæÆÊåá‰ª§ÔºåÂπ∂‰∏îÂ§ö‰∏™PTXÊåá‰ª§ÂèØÊäòÂè†Êàê‰∏Ä‰∏™‰∫åËøõÂà∂ÂæÆÊåá‰ª§„ÄÇÁî±‰∫é‰∏≠Èó¥Ê±áÁºñÁ®ãÂ∫èÁ∫ßÊåá‰ª§‰ΩøÁî®ËôöÊãüÂØÑÂ≠òÂô®Ôºå‰ºòÂåñÁ®ãÂ∫è‰ºöÂàÜÊûêÊï∞ÊçÆ‰æùËµñÊÄßÂπ∂ÂàÜÈÖçÂÆûÈôÖÂØÑÂ≠òÂô®„ÄÇ‰ºòÂåñÂô®Ê∂àÈô§‰∫ÜÊ≠ª‰ª£Á†ÅÔºåÂú®ÂèØË°åÊó∂Â∞ÜÊåá‰ª§ÊäòÂè†Âú®‰∏ÄËµ∑ÔºåÂπ∂‰ºòÂåñSIMTÂàÜÊîØÂèëÊï£ÂíåÊî∂ÊïõÁÇπ„ÄÇ Instruction Set Architecture (ISA) Êåá‰ª§ÈõÜÊû∂ÊûÑThe thread ISA described here is a simplifed version of the Tesla architecture PTX ISA, a register-based scalar instruction set comprising Ô¨Çoating-point, integer, logical, conversion, special functions, Ô¨Çow control, memory access, and texture operations. Figure C.4.3 lists the basic PTX GPU thread instructions; see the NVIDIA PTX specifcation [2007] for details. The instruction format is:ËøôÈáåÊèèËø∞ÁöÑÁ∫øÁ®ãISAÊòØÁâπÊñØÊãâÊû∂ÊûÑPTX ISAÁöÑÁÆÄÂåñÁâàÊú¨ÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÂØÑÂ≠òÂô®ÁöÑÊ†áÈáèÊåá‰ª§ÈõÜÔºåÂåÖÊã¨ÊµÆÁÇπÔºåÊï¥Êï∞ÔºåÈÄªËæëÔºåËΩ¨Êç¢ÔºåÁâπÊÆäÂáΩÊï∞ÔºåÊµÅÊéßÂà∂ÔºåÂ≠òÂÇ®Âô®ËÆøÈóÆÂíåÁ∫πÁêÜÊìç‰Ωú„ÄÇ ÂõæC.4.3ÂàóÂá∫‰∫ÜÂü∫Êú¨ÁöÑPTX GPUÁ∫øÁ®ãÊåá‰ª§; ÊúâÂÖ≥ËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖNVIDIA PTXËßÑËåÉ[2007]„ÄÇ Êåá‰ª§Ê†ºÂºè‰∏∫Ôºö 1opcode.type d, a, b, c; where d is the destination operand, a, b, c are source operands, and .type is one of:ÂÖ∂‰∏≠d ÊòØÁõÆÊ†áÊìç‰ΩúÊï∞ÔºåaÔºåbÔºåcÊòØÊ∫êÊìç‰ΩúÊï∞ÔºåËÄå.typeÊòØ‰ª•‰∏ã‰πã‰∏ÄÔºö Type .type Specifer Untyped bits 8, 16, 32, and 64 bits .b8, .b16, .b32, .b64 Unsigned integer 8, 16, 32, and 64 bits .u8, .u16, .u32, .u64 Signed integer 8, 16, 32, and 64 bits .s8, .s16, .s32, .s64 Floating-point 16, 32, and 64 bits .f16, .f32, .f64 Source operands are scalar 32-bit or 64-bit values in registers, an immediate value, or a constant; predicate operands are 1-bit Boolean values. Destinations are registers, except for store to memory. Instructions are predicated by prefxing them with @p or @!p, where p is a predicate register. Memory and texture instructions transfer scalars or vectors of two to four components, up to 128 bits in total. PTX instructions specify the behavior of one thread.Ê∫êÊìç‰ΩúÊï∞ÊòØÂØÑÂ≠òÂô®‰∏≠ÁöÑÊ†áÈáè32‰ΩçÊàñ64‰ΩçÂÄºÔºåÁ´ãÂç≥ÂÄºÊàñÂ∏∏Èáè; Ë∞ìËØçÊìç‰ΩúÊï∞ÊòØ1‰ΩçÂ∏ÉÂ∞îÂÄº„ÄÇ ÁõÆÊ†áÊòØÂØÑÂ≠òÂô®ÔºåÈô§‰∫ÜÂ≠òÂÇ®Âà∞Â≠òÂÇ®Âô®„ÄÇ ÈÄöËøá‰ΩøÁî®@pÊàñ@ÔºÅpÂØπÂÆÉ‰ª¨ËøõË°åÈ¢ÑÂ§ÑÁêÜÊù•È¢ÑÊµãÊåá‰ª§ÔºåÂÖ∂‰∏≠pÊòØË∞ìËØçÂØÑÂ≠òÂô®„ÄÇ ÂÜÖÂ≠òÂíåÁ∫πÁêÜÊåá‰ª§‰º†Ëæì‰∏§Âà∞Âõõ‰∏™ÁªÑ‰ª∂ÁöÑÊ†áÈáèÊàñÂêëÈáèÔºåÊÄªÂÖ±ÊúÄÂ§ö128‰Ωç„ÄÇ PTXÊåá‰ª§ÊåáÂÆö‰∏Ä‰∏™Á∫øÁ®ãÁöÑË°å‰∏∫„ÄÇ The PTX arithmetic instructions operate on 32-bit and 64-bit Ô¨Çoating-point, signed integer, and unsigned integer types. Recent GPUs support 64-bit double precision Ô¨Çoating-point; see Section C.6. On current GPUs, PTX 64-bit integer and logical instructions are translated to two or more binary microinstructions that perform 32-bit operations. The GPU special function instructions are limited to 32-bit Ô¨Çoating-point. The thread control Ô¨Çow instructions are conditional branch, function call and return, thread exit, and bar.sync (barrier synchronization). The conditional branch instruction @p bra target uses a predicate register p (or !p) previously set by a compare and set predicate setp instruction to determine whether the thread takes the branch or not. Other instructions can also be predicated on a predicate register being true or false.PTXÁÆóÊúØÊåá‰ª§ÂØπ32‰ΩçÂíå64‰ΩçÊµÆÁÇπÔºåÊúâÁ¨¶Âè∑Êï¥Êï∞ÂíåÊó†Á¨¶Âè∑Êï¥Êï∞Á±ªÂûãËøõË°åÊìç‰Ωú„ÄÇ ÊúÄËøëÁöÑGPUÊîØÊåÅ64‰ΩçÂèåÁ≤æÂ∫¶ÊµÆÁÇπ; ËßÅC.6ËäÇ„ÄÇ Âú®ÂΩìÂâçÁöÑGPU‰∏äÔºåPTX 64‰ΩçÊï¥Êï∞ÂíåÈÄªËæëÊåá‰ª§Ë¢´ËΩ¨Êç¢‰∏∫‰∏§‰∏™ÊàñÊõ¥Â§ö‰∏™ÊâßË°å32‰ΩçÊìç‰ΩúÁöÑ‰∫åËøõÂà∂ÂæÆÊåá‰ª§„ÄÇ GPUÁâπÊÆäÂäüËÉΩÊåá‰ª§‰ªÖÈôê‰∫é32‰ΩçÊµÆÁÇπ„ÄÇ Á∫øÁ®ãÊéßÂà∂ÊµÅÁ®ãÊåá‰ª§ÊòØÊù°‰ª∂ÂàÜÊîØÔºåÂáΩÊï∞Ë∞ÉÁî®ÂíåËøîÂõûÔºåÁ∫øÁ®ãÈÄÄÂá∫Âíåbar.syncÔºàÂ±èÈöúÂêåÊ≠•Ôºâ„ÄÇ Êù°‰ª∂ÂàÜÊîØÊåá‰ª§@p bra target‰ΩøÁî®ÂÖàÂâçÁî±compareÂíåset predicate setpÊåá‰ª§ËÆæÁΩÆÁöÑË∞ìËØçÂØÑÂ≠òÂô®pÔºàÊàñÔºÅpÔºâÊù•Á°ÆÂÆöÁ∫øÁ®ãÊòØÂê¶Êé•ÂèóÂàÜÊîØ„ÄÇ ÂÖ∂‰ªñÊåá‰ª§‰πüÂèØ‰ª•Âú®Ë∞ìËØçÂØÑÂ≠òÂô®‰∏∫ÁúüÊàñÂÅáÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÈ¢ÑÊµã„ÄÇ Memory Access Instructions ÂÜÖÂ≠òËÆøÈóÆÊåá‰ª§The tex instruction fetches and filters texture samples from 1D, 2D, and 3D texture arrays in memory via the texture subsystem. Texture fetches generally use interpolated Ô¨Çoating-point coordinates to address a texture. Once a graphics pixel shader thread computes its pixel fragment color, the raster operations processor blends it with the pixel color at its assigned (x, y) pixel position and writes the final color to memory.texÊåá‰ª§ÈÄöËøáÁ∫πÁêÜÂ≠êÁ≥ªÁªü‰ªéÂÜÖÂ≠ò‰∏≠ÁöÑ1DÔºå2DÂíå3DÁ∫πÁêÜÊï∞ÁªÑ‰∏≠ÊèêÂèñÂíåËøáÊª§Á∫πÁêÜÊ†∑Êú¨„ÄÇ Á∫πÁêÜÊèêÂèñÈÄöÂ∏∏‰ΩøÁî®ÊèíÂÄºÁöÑÊµÆÁÇπÂùêÊ†áÊù•ÂØªÂùÄÁ∫πÁêÜ„ÄÇ ‰∏ÄÊó¶ÂõæÂΩ¢ÂÉèÁ¥†ÁùÄËâ≤Âô®Á∫øÁ®ãËÆ°ÁÆóÂÖ∂ÂÉèÁ¥†ÁâáÊÆµÈ¢úËâ≤ÔºåÂÖâÊ†ÖÊìç‰ΩúÂ§ÑÁêÜÂô®Â∞ÜÂÖ∂‰∏éÂÖ∂ÊåáÂÆöÁöÑÔºàxÔºåyÔºâÂÉèÁ¥†‰ΩçÁΩÆÂ§ÑÁöÑÂÉèÁ¥†È¢úËâ≤Ê∑∑ÂêàÔºåÂπ∂Â∞ÜÊúÄÁªàÈ¢úËâ≤ÂÜôÂÖ•Â≠òÂÇ®Âô®„ÄÇTo support computing and C/C++ language needs, the Tesla PTX ISA implements memory load/store instructions. It uses integer byte addressing with register plus oÔ¨Äset address arithmetic to facilitate conventional compiler code optimizations. Memory load/store instructions are common in processors, but are a signifcant new capability in the Tesla architecture GPUs, as prior GPUs provided only the texture and pixel accesses required by the graphics APIs.‰∏∫‰∫ÜÊîØÊåÅËÆ°ÁÆóÂíåC / C ++ËØ≠Ë®ÄÈúÄÊ±ÇÔºåTesla PTX ISAÂÆûÁé∞‰∫ÜÂÜÖÂ≠òÂä†ËΩΩ/Â≠òÂÇ®Êåá‰ª§„ÄÇ ÂÆÉ‰ΩøÁî®Êï¥Êï∞Â≠óËäÇÂØªÂùÄÂíåÂØÑÂ≠òÂô®Âä†o ffËÆæÁΩÆÂú∞ÂùÄÁÆóÊ≥ïÊù•‰øÉËøõ‰º†ÁªüÁöÑÁºñËØëÂô®‰ª£Á†Å‰ºòÂåñ„ÄÇ ÂÜÖÂ≠òÂä†ËΩΩ/Â≠òÂÇ®Êåá‰ª§Âú®Â§ÑÁêÜÂô®‰∏≠ÂæàÂ∏∏ËßÅÔºå‰ΩÜÂú®TeslaÊû∂ÊûÑGPU‰∏≠ÊòØ‰∏ÄÈ°πÈáçË¶ÅÁöÑÊñ∞ÂäüËÉΩÔºåÂõ†‰∏∫‰πãÂâçÁöÑGPU‰ªÖÊèê‰æõÂõæÂΩ¢APIÊâÄÈúÄÁöÑÁ∫πÁêÜÂíåÂÉèÁ¥†ËÆøÈóÆ„ÄÇFor computing, the load/store instructions access three read/write memory spaces that implement the corresponding CUDA memory spaces in Section C.3:ÂØπ‰∫éËÆ°ÁÆóÔºåÂä†ËΩΩ/Â≠òÂÇ®Êåá‰ª§ËÆøÈóÆÂú®C.3ËäÇ‰∏≠ÂÆûÁé∞Áõ∏Â∫îCUDAÂ≠òÂÇ®Á©∫Èó¥ÁöÑ‰∏â‰∏™ËØª/ÂÜôÂ≠òÂÇ®Á©∫Èó¥Ôºö Local memory for per-thread private addressable temporary data (implemented in external DRAM) ÊØèÁ∫øÁ®ã‰∏ìÁî®ÂèØÂØªÂùÄ‰∏¥Êó∂Êï∞ÊçÆÁöÑÊú¨Âú∞ÂÜÖÂ≠òÔºàÂú®Â§ñÈÉ®DRAM‰∏≠ÂÆûÁé∞Ôºâ Shared memory for low-latency access to data shared by cooperating threads in the same CTA/thread block (implemented in on-chip SRAM) ÂÖ±‰∫´ÂÜÖÂ≠òÔºåÁî®‰∫éÂØπÂêå‰∏ÄCTA /Á∫øÁ®ãÂùó‰∏≠ÁöÑÂçè‰ΩúÁ∫øÁ®ãÂÖ±‰∫´ÁöÑÊï∞ÊçÆËøõË°å‰ΩéÂª∂ËøüËÆøÈóÆÔºàÂú®Áâá‰∏äSRAM‰∏≠ÂÆûÁé∞Ôºâ Global memory for large data sets shared by all threads of a computing application (implemented in external DRAM) ËÆ°ÁÆóÂ∫îÁî®Á®ãÂ∫èÁöÑÊâÄÊúâÁ∫øÁ®ãÂÖ±‰∫´ÁöÑÂ§ßÂûãÊï∞ÊçÆÈõÜÁöÑÂÖ®Â±ÄÂÜÖÂ≠òÔºàÂú®Â§ñÈÉ®DRAM‰∏≠ÂÆûÁé∞Ôºâ The memory load/store instructions ld.global, st.global, ld.shared, st.shared, ld.local, and st.local access the global, shared, and local memory spaces. Computing programs use the fast barrier synchronization instruction bar.sync to synchronize threads within a CTA/thread block that communicate with each other via shared and global memory.ÂÜÖÂ≠òÂä†ËΩΩ/Â≠òÂÇ®Êåá‰ª§ld.globalÔºåst.globalÔºåld.sharedÔºåst.sharedÔºåld.localÂíåst.localËÆøÈóÆÂÖ®Â±ÄÔºåÂÖ±‰∫´ÂíåÊú¨Âú∞ÂÜÖÂ≠òÁ©∫Èó¥„ÄÇ ËÆ°ÁÆóÁ®ãÂ∫è‰ΩøÁî®Âø´ÈÄüÂ±èÈöúÂêåÊ≠•Êåá‰ª§bar.syncÊù•ÂêåÊ≠•CTA /Á∫øÁ®ãÂùóÂÜÖÁöÑÁ∫øÁ®ãÔºåËøô‰∫õÁ∫øÁ®ãÈÄöËøáÂÖ±‰∫´ÂíåÂÖ®Â±ÄÂÜÖÂ≠òÁõ∏‰∫íÈÄö‰ø°„ÄÇ To improve memory bandwidth and reduce overhead, the local and global load/store instructions coalesce individual parallel thread requests from the same SIMT warp together into a single memory block request when the addresses fall in the same block and meet alignment criteria. Coalescing memory requests provides a signifcant performance boost over separate requests from individual threads. The multiprocessor‚Äôs large thread count, together with support for many outstanding load requests, helps cover load-to-use latency for local and global memory implemented in external DRAM.‰∏∫‰∫ÜÊîπÂñÑÂ≠òÂÇ®Âô®Â∏¶ÂÆΩÂπ∂ÂáèÂ∞ëÂºÄÈîÄÔºåÂΩìÂú∞ÂùÄËêΩÂú®Âêå‰∏ÄÂùó‰∏≠Âπ∂Êª°Ë∂≥ÂØπÈΩêÊ†áÂáÜÊó∂ÔºåÊú¨Âú∞ÂíåÂÖ®Â±ÄÂä†ËΩΩ/Â≠òÂÇ®Êåá‰ª§Â∞ÜÊù•Ëá™Áõ∏ÂêåSIMT warpÁöÑÂêÑ‰∏™Âπ∂Ë°åÁ∫øÁ®ãËØ∑Ê±ÇÂêàÂπ∂‰∏∫Âçï‰∏™Â≠òÂÇ®Âô®ÂùóËØ∑Ê±Ç„ÄÇ ÂêàÂπ∂ÂÜÖÂ≠òËØ∑Ê±ÇÁõ∏ÂØπ‰∫éÊù•Ëá™ÂêÑ‰∏™Á∫øÁ®ãÁöÑÂçïÁã¨ËØ∑Ê±ÇÊèê‰æõ‰∫ÜÊòæÁùÄÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ Â§öÂ§ÑÁêÜÂô®ÁöÑÂ§ßÁ∫øÁ®ãÊï∞‰ª•ÂèäÂØπËÆ∏Â§öÊú™ÂÆåÊàêÁöÑË¥üËΩΩËØ∑Ê±ÇÁöÑÊîØÊåÅÊúâÂä©‰∫éË¶ÜÁõñÂ§ñÈÉ®DRAM‰∏≠ÂÆûÁé∞ÁöÑÊú¨Âú∞ÂíåÂÖ®Â±ÄÂÜÖÂ≠òÁöÑË¥üËΩΩ‰ΩøÁî®Âª∂Ëøü„ÄÇ The latest Tesla architecture GPUs also provide efcient atomic memory operations on memory with the atom.op.u32 instructions, including integer operations add, min, max, and, or, xor, exchange, and cas (compare-and-swap) operations, facilitating parallel reductions and parallel data structure management.ÊúÄÊñ∞ÁöÑTeslaÊû∂ÊûÑGPUËøòÈÄöËøáatom.op.u32Êåá‰ª§Âú®ÂÜÖÂ≠ò‰∏äÊèê‰æõÊúâÊïàÁöÑÂéüÂ≠êÂÜÖÂ≠òÊìç‰ΩúÔºåÂåÖÊã¨Êï¥Êï∞ËøêÁÆóaddÔºåminÔºåmaxÂíåÔºåÊàñËÄÖxorÔºåexchangeÂíåcasÔºàÊØîËæÉÂíå‰∫§Êç¢ÔºâÊìç‰ΩúÔºå ‰øÉËøõÂπ∂Ë°åÂáèÂ∞ëÂíåÂπ∂Ë°åÊï∞ÊçÆÁªìÊûÑÁÆ°ÁêÜ„ÄÇ Barrier Synchronization for Thread Communication Á∫øÁ®ãÈÄö‰ø°ÁöÑÂ±èÈöúÂêåÊ≠•Fast barrier synchronization permits CUDA programs to communicate frequently via shared memory and global memory by simply calling __syncthreads(); as part of each interthread communication step. The synchronization intrinsic function generates a single bar.sync instruction. However, implementing fast barrier synchronization among up to 512 threads per CUDA thread block is a challenge.Âø´ÈÄüÂ±èÈöúÂêåÊ≠•ÂÖÅËÆ∏CUDAÁ®ãÂ∫èÈÄöËøáÁÆÄÂçïÂú∞Ë∞ÉÁî®__syncthreadsÔºàÔºâ;Êù•ÁªèÂ∏∏ÈÄöËøáÂÖ±‰∫´ÂÜÖÂ≠òÂíåÂÖ®Â±ÄÂÜÖÂ≠òËøõË°åÈÄö‰ø°„ÄÇ ‰Ωú‰∏∫ÊØè‰∏™Á∫øÁ®ã‰∫§ÊµÅÊ≠•È™§ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ ÂêåÊ≠•ÂÜÖÈÉ®ÂáΩÊï∞ÁîüÊàêÂçï‰∏™bar.syncÊåá‰ª§„ÄÇ ‰ΩÜÊòØÔºåÂú®ÊØè‰∏™CUDAÁ∫øÁ®ãÂùó‰∏≠ÊúÄÂ§ö512‰∏™Á∫øÁ®ã‰πãÈó¥ÂÆûÁé∞Âø´ÈÄüÂ±èÈöúÂêåÊ≠•ÊòØ‰∏ÄÈ°πÊåëÊàò„ÄÇGrouping threads into SIMT warps of 32 threads reduces the synchronization difculty by a factor of 32. Treads wait at a barrier in the SIMT thread scheduler so they do not consume any processor cycles while waiting. When a thread executes a bar.sync instruction, it increments the barrier‚Äôs thread arrival counter and the scheduler marks the thread as waiting at the barrier. Once all the CTA threads arrive, the barrier counter matches the expected terminal count, and the scheduler releases all the threads waiting at the barrier and resumes executing threads.Â∞ÜÁ∫øÁ®ãÂàÜÁªÑ‰∏∫32‰∏™Á∫øÁ®ãÁöÑSIMT warpÂèØÂ∞ÜÂêåÊ≠•Âõ∞ÈöæÂáèÂ∞ë32ÂÄç.TreadÂú®SIMTÁ∫øÁ®ãË∞ÉÂ∫¶Á®ãÂ∫è‰∏≠Á≠âÂæÖÈöúÁ¢çÔºå‰ª•‰æøÂÆÉ‰ª¨Âú®Á≠âÂæÖÊó∂‰∏çÊ∂àËÄó‰ªª‰ΩïÂ§ÑÁêÜÂô®Âë®Êúü„ÄÇ ÂΩì‰∏Ä‰∏™Á∫øÁ®ãÊâßË°åbar.syncÊåá‰ª§Êó∂ÔºåÂÆÉ‰ºöÈÄíÂ¢ûÂ±èÈöúÁöÑÁ∫øÁ®ãÂà∞ËææËÆ°Êï∞Âô®ÔºåÂπ∂‰∏îË∞ÉÂ∫¶Á®ãÂ∫èÂ∞ÜËØ•Á∫øÁ®ãÊ†áËÆ∞‰∏∫Âú®Â±èÈöúÂ§ÑÁ≠âÂæÖ„ÄÇ ‰∏ÄÊó¶ÊâÄÊúâCTAÁ∫øÁ®ãÂà∞ËææÔºåÂ±èÈöúËÆ°Êï∞Âô®ÂåπÈÖçÈ¢ÑÊúüÁöÑÁªàÁ´ØËÆ°Êï∞ÔºåÂπ∂‰∏îË∞ÉÂ∫¶Á®ãÂ∫èÈáäÊîæÂú®Â±èÈöúÂ§ÑÁ≠âÂæÖÁöÑÊâÄÊúâÁ∫øÁ®ãÂπ∂ÁªßÁª≠ÊâßË°åÁ∫øÁ®ã„ÄÇ Streaming Processor (SP) ÊµÅÂ§ÑÁêÜÂô®ÔºàSPÔºâThe multithreaded streaming processor (SP) core is the primary thread instruction processor in the multiprocessor. Its register fle (RF) provides 1024 scalar 32-bit registers for up to 64 threads. It executes all the fundamental Ô¨Çoating-point operations, including add.f32, mul.f32, mad.f32 (Ô¨Çoating multiply-add), min.f32, max.f32, and setp.f32 (Ô¨Çoating compare and set predicate). Te Ô¨Çoatingpoint add and multiply operations are compatible with the IEEE 754 standard for single precision FP numbers, including not-a-number (NaN) and infnity values. Te SP core also implements all of the 32-bit and 64-bit integer arithmetic, comparison, conversion, and logical PTX instructions shown in Figure C.4.3.Â§öÁ∫øÁ®ãÊµÅÂ§ÑÁêÜÂô®ÔºàSPÔºâÂÜÖÊ†∏ÊòØÂ§öÂ§ÑÁêÜÂô®‰∏≠ÁöÑ‰∏ªË¶ÅÁ∫øÁ®ãÊåá‰ª§Â§ÑÁêÜÂô®„ÄÇ ÂÖ∂ÂØÑÂ≠òÂô®Êñá‰ª∂ÔºàRFÔºâÊèê‰æõ1024‰∏™Ê†áÈáè32‰ΩçÂØÑÂ≠òÂô®ÔºåÊúÄÂ§öÂèØÊîØÊåÅ64‰∏™Á∫øÁ®ã„ÄÇ ÂÆÉÊâßË°åÊâÄÊúâÂü∫Êú¨ÁöÑÊµÆÁÇπËøêÁÆóÔºåÂåÖÊã¨add.f32Ôºåmul.f32Ôºåmad.f32ÔºàÊµÆÂä®‰πòÊ≥ï - Âä†Ê≥ïÔºâÔºåmin.f32Ôºåmax.f32Âíåsetp.f32ÔºàÊµÆÁÇπÊï∞ÊØîËæÉÂíåËÆæÁΩÆË∞ìËØçÔºâ„ÄÇ ÂØπ‰∫éÂçïÁ≤æÂ∫¶FPÁºñÂè∑ÔºåÂåÖÊã¨ÈùûÊï∞Â≠óÔºàNaNÔºâÂíåÊó†Á©∑Â§ßÂÄºÔºåTe floatingpointÂä†Ê≥ïÂíå‰πòÊ≥ïËøêÁÆó‰∏éIEEE 754Ê†áÂáÜÂÖºÂÆπ„ÄÇ Te SPÂÜÖÊ†∏ËøòÂÆûÁé∞‰∫ÜÂõæC.4.3‰∏≠ÊâÄÁ§∫ÁöÑÊâÄÊúâ32‰ΩçÂíå64‰ΩçÊï¥Êï∞ËøêÁÆóÔºåÊØîËæÉÔºåËΩ¨Êç¢ÂíåÈÄªËæëPTXÊåá‰ª§„ÄÇ The Ô¨Çoating-point add and mul operations employ IEEE round-to-nearest-even as the default rounding mode. Te mad.f32 Ô¨Çoating-point multiply-add operation performs a multiplication with truncation, followed by an addition with roundto-nearest-even. The SP Ô¨Çushes input denormal operands to sign-preserved-zero. Results that underÔ¨Çow the target output exponent range are Ô¨Çushed to signpreserved-zero after rounding.ÊµÆÁÇπÂä†Ê≥ïÂíåmulËøêÁÆóÈááÁî®IEEEËàçÂÖ•Âà∞ÊúÄËøë - ÁîöËá≥‰Ωú‰∏∫ÈªòËÆ§ËàçÂÖ•Ê®°Âºè„ÄÇ Te mad.f32ÊµÆÁÇπ‰πòÊ≥ïÂä†Ê≥ïËøêÁÆóÊâßË°å‰∏éÊà™Êñ≠ÁöÑ‰πòÊ≥ïËøêÁÆóÔºåÁÑ∂Âêé‰ΩøÁî®roundto-nearest-evenËøõË°åÂä†Ê≥ïËøêÁÆó„ÄÇ SPÂ∞ÜËæìÂÖ•ÈùûÊ≠£ËßÑÊìç‰ΩúÊï∞Áî®‰∫éÁ¨¶Âè∑‰øùÁïô‰∏∫Èõ∂„ÄÇ Âú®ËàçÂÖ•‰πãÂêéÔºåÂ∞ÜÁõÆÊ†áËæìÂá∫ÊåáÊï∞ËåÉÂõ¥‰∏ãÁöÑÁªìÊûúÊµÆÂä®Âà∞Á¨¶Âè∑‰øùÁïô‰∏∫Èõ∂ Special Function Unit (SFU) ÁâπÊÆäÂäüËÉΩÂçïÂÖÉÔºàSFUÔºâCertain thread instructions can execute on the SFUs, concurrently with other thread instructions executing on the SPs. The SFU implements the special function instructions of Figure C.4.3, which compute 32-bit Ô¨Çoating-point approximations to reciprocal, reciprocal square root, and key transcendental functions. It also implements 32-bit Ô¨Çoating-point planar attribute interpolation for pixel shaders, providing accurate interpolation of attributes such as color, depth, and texture coordinates.Êüê‰∫õÁ∫øÁ®ãÊåá‰ª§ÂèØ‰ª•Âú®SFU‰∏äÊâßË°åÔºå‰∏éÂú®SP‰∏äÊâßË°åÁöÑÂÖ∂‰ªñÁ∫øÁ®ãÊåá‰ª§ÂêåÊó∂ÊâßË°å„ÄÇ SFUÂÆûÁé∞‰∫ÜÂõæC.4.3‰∏≠ÁöÑÁâπÊÆäÂáΩÊï∞Êåá‰ª§ÔºåÂÆÉ‰ª¨ËÆ°ÁÆóÂÄíÊï∞ÔºåÂÄíÊï∞Âπ≥ÊñπÊ†πÂíåÂÖ≥ÈîÆË∂ÖË∂äÂáΩÊï∞ÁöÑ32‰ΩçÊµÆÁÇπËøë‰ºº„ÄÇ ÂÆÉËøò‰∏∫ÂÉèÁ¥†ÁùÄËâ≤Âô®ÂÆûÁé∞‰∫Ü32‰ΩçÊµÆÁÇπÂπ≥Èù¢Â±ûÊÄßÊèíÂÄºÔºåÊèê‰æõ‰∫ÜÈ¢úËâ≤ÔºåÊ∑±Â∫¶ÂíåÁ∫πÁêÜÂùêÊ†áÁ≠âÂ±ûÊÄßÁöÑÁ≤æÁ°ÆÊèíÂÄº„ÄÇ Each pipelined SFU generates one 32-bit Ô¨Çoating-point special function result per cycle; the two SFUs per multiprocessor execute special function instructions at a quarter the simple instruction rate of the eight SPs. The SFUs also execute the mul.f32 multiply instruction concurrently with the eight SPs, increasing the peakcomputation rate up to 50% for threads with a suitable instruction mixture.ÊØè‰∏™ÊµÅÊ∞¥Á∫øSFUÂú®ÊØè‰∏™Âë®Êúü‰∫ßÁîü‰∏Ä‰∏™32‰ΩçÊµÆÁÇπÁâπÊÆäÂäüËÉΩÁªìÊûú; ÊØè‰∏™Â§öÂ§ÑÁêÜÂô®ÁöÑ‰∏§‰∏™SFU‰ª•ÂÖ´‰∏™SPÁöÑÁÆÄÂçïÊåá‰ª§ÈÄüÁéáÁöÑÂõõÂàÜ‰πã‰∏ÄÊâßË°åÁâπÊÆäÂäüËÉΩÊåá‰ª§„ÄÇ SFUËøò‰∏é8‰∏™SPÂêåÊó∂ÊâßË°åmul.f32‰πòÊ≥ïÊåá‰ª§ÔºåÂØπ‰∫éÂÖ∑ÊúâÂêàÈÄÇÊåá‰ª§Ê∑∑ÂêàÁöÑÁ∫øÁ®ãÔºåÂ≥∞ÂÄºËÆ°ÁÆóÈÄüÁéáÊèêÈ´ò‰∫Ü50ÔºÖ„ÄÇ For functional evaluation, the Tesla architecture SFU employs quadratic interpolation based on enhanced minimax approximations for approximating the reciprocal, reciprocal square-root, log2x, 2x, and sin/cos functions. Te accuracy of the function estimates ranges from 22 to 24 mantissa bits. See Section C.6 for more details on SFU arithmetic.ÂØπ‰∫éÂäüËÉΩËØÑ‰º∞ÔºåÁâπÊñØÊãâÊû∂ÊûÑSFUÈááÁî®Âü∫‰∫éÂ¢ûÂº∫ÁöÑÊûÅÂ∞èÊûÅÂ§ßËøë‰ººÁöÑ‰∫åÊ¨°ÊèíÂÄºÊù•Ëøë‰ººÂÄíÊï∞ÔºåÂÄíÊï∞Âπ≥ÊñπÊ†πÔºålog2xÔºå2xÂíåÊ≠£Âº¶/‰ΩôÂº¶ÂáΩÊï∞„ÄÇ ÂäüËÉΩ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÂ∫¶ËåÉÂõ¥‰ªé22Âà∞24‰∏™Â∞æÊï∞‰Ωç„ÄÇ ÊúâÂÖ≥SFUÁÆóÊ≥ïÁöÑÊõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇËßÅÁ¨¨C.6ËäÇ„ÄÇ Comparing with Other Multiprocessors ‰∏éÂÖ∂‰ªñÂ§öÂ§ÑÁêÜÂô®ÊØîËæÉCompared with SIMD vector architectures such as x86 SSE, the SIMT multiprocessor can execute individual threads independently, rather than always executing them together in synchronous groups. SIMT hardware fnds data parallelism among independent threads, whereas SIMD hardware requires the sofware to express data parallelism explicitly in each vector instruction. A SIMT machine executes a warp of 32 threads synchronously when the threads take the same execution path, yet can execute each thread independently when they diverge. Te advantage is signifcant because SIMT programs and instructions simply describe the behavior of a single independent thread, rather than a SIMD data vector of four or more data lanes. Yet the SIMT multiprocessor has SIMD-like efciency, spreading the area and cost of one instruction unit across the 32 threads of a warp and across the eight streaming processor cores. SIMT provides the performance of SIMD together with the productivity of multithreading, avoiding the need to explicitly code SIMD vectors for edge conditions and partial divergence.‰∏éSIM86Áü¢Èáè‰ΩìÁ≥ªÁªìÊûÑÔºàÂ¶Çx86 SSEÔºâÁõ∏ÊØîÔºåSIMTÂ§öÂ§ÑÁêÜÂô®ÂèØ‰ª•Áã¨Á´ãÊâßË°åÂêÑ‰∏™Á∫øÁ®ãÔºåËÄå‰∏çÊòØÂßãÁªàÂú®ÂêåÊ≠•ÁªÑ‰∏≠‰∏ÄËµ∑ÊâßË°åÂÆÉ‰ª¨„ÄÇ SIMTÁ°¨‰ª∂ÊîØÊåÅÁã¨Á´ãÁ∫øÁ®ã‰πãÈó¥ÁöÑÊï∞ÊçÆÂπ∂Ë°åÊÄßÔºåËÄåSIMDÁ°¨‰ª∂Ë¶ÅÊ±ÇËΩØ‰ª∂Âú®ÊØè‰∏™ÂêëÈáèÊåá‰ª§‰∏≠ÊòéÁ°ÆË°®ËææÊï∞ÊçÆÂπ∂Ë°åÊÄß„ÄÇÂΩìÁ∫øÁ®ãÈááÁî®Áõ∏ÂêåÁöÑÊâßË°åË∑ØÂæÑÊó∂ÔºåSIMTÊú∫Âô®ÂêåÊ≠•ÊâßË°å32‰∏™Á∫øÁ®ãÁöÑwarpÔºå‰ΩÜÊòØÂΩìÂÆÉ‰ª¨ÂèëÊï£Êó∂ÂèØ‰ª•Áã¨Á´ãÂú∞ÊâßË°åÊØè‰∏™Á∫øÁ®ã„ÄÇ‰ºòÁÇπÊòØÊòæÁùÄÁöÑÔºåÂõ†‰∏∫SIMTÁ®ãÂ∫èÂíåÊåá‰ª§ÁÆÄÂçïÂú∞ÊèèËø∞‰∫ÜÂçï‰∏™Áã¨Á´ãÁ∫øÁ®ãÁöÑË°å‰∏∫ÔºåËÄå‰∏çÊòØÂõõ‰∏™ÊàñÊõ¥Â§öÊï∞ÊçÆÈÄöÈÅìÁöÑSIMDÊï∞ÊçÆÂêëÈáè„ÄÇÁÑ∂ËÄåÔºåSIMTÂ§öÂ§ÑÁêÜÂô®ÂÖ∑ÊúâÁ±ª‰ººSIMDÁöÑÊïàÁéáÔºåÂú®‰∏Ä‰∏™Êâ≠Êõ≤ÁöÑ32‰∏™Á∫øÁ®ãÂíåÂÖ´‰∏™ÊµÅÂ§ÑÁêÜÂô®ÂÜÖÊ†∏‰πãÈó¥Êâ©Â±ï‰∫Ü‰∏Ä‰∏™Êåá‰ª§ÂçïÂÖÉÁöÑÈù¢ÁßØÂíåÊàêÊú¨„ÄÇ SIMTÊèê‰æõSIMDÁöÑÊÄßËÉΩ‰ª•ÂèäÂ§öÁ∫øÁ®ãÁöÑÁîü‰∫ßÁéáÔºåÊó†ÈúÄ‰∏∫ËæπÁºòÊù°‰ª∂ÂíåÈÉ®ÂàÜÂèëÊï£ÊòéÁ°ÆÁºñÁ†ÅSIMDÂêëÈáè„ÄÇ The SIMT multiprocessor imposes little overhead because it is hardware multithreaded with hardware barrier synchronization. That allows graphics shaders and CUDA threads to express very fne-grained parallelism. Graphics and CUDA programs use threads to express fne-grained data parallelism in a perthread program, rather than forcing the programmer to express it as SIMD vector instructions. It is simpler and more productive to develop scalar single-thread code than vector code, and the SIMT multiprocessor executes the code with SIMD-like effciency.SIMTÂ§öÂ§ÑÁêÜÂô®ÁöÑÂºÄÈîÄÂæàÂ∞èÔºåÂõ†‰∏∫ÂÆÉÊòØÂÖ∑ÊúâÁ°¨‰ª∂Â±èÈöúÂêåÊ≠•ÁöÑÁ°¨‰ª∂Â§öÁ∫øÁ®ã„ÄÇ ËøôÂÖÅËÆ∏ÂõæÂΩ¢ÁùÄËâ≤Âô®ÂíåCUDAÁ∫øÁ®ãË°®ËææÈùûÂ∏∏ÁªÜËá¥ÁöÑÂπ∂Ë°åÊÄß„ÄÇ ÂõæÂΩ¢ÂíåCUDAÁ®ãÂ∫è‰ΩøÁî®Á∫øÁ®ãÂú®perthreadÁ®ãÂ∫è‰∏≠Ë°®ËææÁªÜÁ≤íÂ∫¶Êï∞ÊçÆÂπ∂Ë°åÊÄßÔºåËÄå‰∏çÊòØÂº∫Ëø´Á®ãÂ∫èÂëòÂ∞ÜÂÖ∂Ë°®Ëææ‰∏∫SIMDÂêëÈáèÊåá‰ª§„ÄÇ ÂºÄÂèëÊ†áÈáèÂçïÁ∫øÁ®ã‰ª£Á†ÅÊØîÁü¢Èáè‰ª£Á†ÅÊõ¥ÁÆÄÂçïÔºåÊõ¥È´òÊïàÔºåËÄåSIMTÂ§öÂ§ÑÁêÜÂô®‰ª•Á±ª‰ººSIMDÁöÑÊïàÁéáÊâßË°å‰ª£Á†Å„ÄÇ Coupling eight streaming processor cores together closely into a multiprocessor and then implementing a scalable number of such multiprocessors makes a twolevel multiprocessor composed of multiprocessors. The CUDA programming model exploits the two-level hierarchy by providing individual threads for fne-grained parallel computations, and by providing grids of thread blocks for coarse-grained parallel operations. The same thread program can provide both fine-grained and coarse-grained operations. In contrast, CPUs with SIMD vector instructions must use two diÔ¨Äerent programming models to provide fne-grained and coarse-grained operations: coarse-grained parallel threads on different cores, and SIMD vector instructions for fne-grained data parallelism.Â∞ÜÂÖ´‰∏™ÊµÅÂ§ÑÁêÜÂô®Ê†∏ÂøÉÁ¥ßÂØÜÂú∞ËÄ¶ÂêàÂà∞Â§öÂ§ÑÁêÜÂô®‰∏≠ÔºåÁÑ∂ÂêéÂÆûÁé∞ÂèØÊâ©Â±ïÊï∞ÈáèÁöÑËøôÁßçÂ§öÂ§ÑÁêÜÂô®Ôºå‰ΩøÂæóÁî±Â§öÂ§ÑÁêÜÂô®ÁªÑÊàêÁöÑ‰∏§Á∫ßÂ§öÂ§ÑÁêÜÂô®Êàê‰∏∫ÂèØËÉΩ„ÄÇ CUDAÁºñÁ®ãÊ®°ÂûãÈÄöËøá‰∏∫ÁªÜÁ≤íÂ∫¶Âπ∂Ë°åËÆ°ÁÆóÊèê‰æõÂçïÁã¨ÁöÑÁ∫øÁ®ãÔºåÂπ∂ÈÄöËøá‰∏∫Á≤óÁ≤íÂ∫¶Âπ∂Ë°åÊìç‰ΩúÊèê‰æõÁ∫øÁ®ãÂùóÁΩëÊ†ºÊù•Âà©Áî®‰∏§Á∫ßÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇ Áõ∏ÂêåÁöÑÁ∫øÁ®ãÁ®ãÂ∫èÂèØ‰ª•Êèê‰æõÁªÜÁ≤íÂ∫¶ÂíåÁ≤óÁ≤íÂ∫¶Êìç‰Ωú„ÄÇ Áõ∏ÂèçÔºåÂÖ∑ÊúâSIMDÂêëÈáèÊåá‰ª§ÁöÑCPUÂøÖÈ°ª‰ΩøÁî®‰∏§Áßç‰∏çÂêåÁöÑÁºñÁ®ãÊ®°ÂûãÊù•Êèê‰æõÁªÜÁ≤íÂ∫¶ÂíåÁ≤óÁ≤íÂ∫¶Êìç‰ΩúÔºö‰∏çÂêåÂÜÖÊ†∏‰∏äÁöÑÁ≤óÁ≤íÂ∫¶Âπ∂Ë°åÁ∫øÁ®ãÔºå‰ª•ÂèäÁî®‰∫éÁªÜÁ≤íÂ∫¶Êï∞ÊçÆÂπ∂Ë°åÊÄßÁöÑSIMDÂêëÈáèÊåá‰ª§„ÄÇ Multithreaded Multiprocessor Conclusion Â§öÁ∫øÁ®ãÂ§öÂ§ÑÁêÜÂô®ÁªìËÆ∫The example GPU multiprocessor based on the Tesla architecture is highly multithreaded, executing a total of up to 512 lightweight threads concurrently to support fne-grained pixel shaders and CUDA threads. It uses a variation on SIMD architecture and multithreading called SIMT (single-instruction multiple-thread) to effciently broadcast one instruction to a warp of 32 parallel threads, while permitting each thread to branch and execute independently. Each thread executes its instruction stream on one of the eight streaming processor (SP) cores, which are multithreaded up to 64 threads.Âü∫‰∫éTeslaÊû∂ÊûÑÁöÑÁ§∫‰æãGPUÂ§öÂ§ÑÁêÜÂô®ÊòØÈ´òÂ∫¶Â§öÁ∫øÁ®ãÁöÑÔºåÂêåÊó∂ÊâßË°åÊÄªÂÖ±Â§öËææ512‰∏™ËΩªÈáèÁ∫ßÁ∫øÁ®ãÔºå‰ª•ÊîØÊåÅÁªÜÁ≤íÂ∫¶ÂÉèÁ¥†ÁùÄËâ≤Âô®ÂíåCUDAÁ∫øÁ®ã„ÄÇ ÂÆÉ‰ΩøÁî®SIMDÊû∂ÊûÑÁöÑÂèò‰ΩìÂíåÁß∞‰∏∫SIMTÔºàÂçïÊåá‰ª§Â§öÁ∫øÁ®ãÔºâÁöÑÂ§öÁ∫øÁ®ãÊù•ÊúâÊïàÂú∞Â∞Ü‰∏ÄÊù°Êåá‰ª§ÂπøÊí≠Âà∞32‰∏™Âπ∂Ë°åÁ∫øÁ®ãÁöÑwarpÔºåÂêåÊó∂ÂÖÅËÆ∏ÊØè‰∏™Á∫øÁ®ãÁã¨Á´ãÂú∞ÂàÜÊîØÂíåÊâßË°å„ÄÇ ÊØè‰∏™Á∫øÁ®ãÂú®ÂÖ´‰∏™ÊµÅÂ§ÑÁêÜÂô®ÔºàSPÔºâÂÜÖÊ†∏‰πã‰∏Ä‰∏äÊâßË°åÂÖ∂Êåá‰ª§ÊµÅÔºåËøô‰∫õÂÜÖÊ†∏ÊòØÂ§öÁ∫øÁ®ãÁöÑÔºåÊúÄÂ§ö64‰∏™Á∫øÁ®ã„ÄÇ The PTX ISA is a register-based load/store scalar ISA that describes the execution of a single thread. Because PTX instructions are optimized and translated to binary microinstructions for a specifc GPU, the hardware instructions can evolve rapidly without disrupting compilers and sofware tools that generate PTX instructions.PTX ISAÊòØ‰∏Ä‰∏™Âü∫‰∫éÂØÑÂ≠òÂô®ÁöÑÂä†ËΩΩ/Â≠òÂÇ®Ê†áÈáèISAÔºåÁî®‰∫éÊèèËø∞Âçï‰∏™Á∫øÁ®ãÁöÑÊâßË°å„ÄÇ Áî±‰∫éPTXÊåá‰ª§ÁªèËøá‰ºòÂåñÂπ∂ËΩ¨Êç¢‰∏∫ÁâπÂÆöGPUÁöÑ‰∫åËøõÂà∂ÂæÆÊåá‰ª§ÔºåÂõ†Ê≠§Á°¨‰ª∂Êåá‰ª§ÂèØ‰ª•Âø´ÈÄüÂèëÂ±ïÔºåËÄå‰∏ç‰ºö‰∏≠Êñ≠ÁîüÊàêPTXÊåá‰ª§ÁöÑÁºñËØëÂô®ÂíåËΩØ‰ª∂Â∑•ÂÖ∑]]></content>
      <categories>
        <category>GPUÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÁøªËØë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3_Programming_GPUs]]></title>
    <url>%2F2019%2F01%2F24%2FProgramming_GPUs%2F</url>
    <content type="text"><![CDATA[Programming multiprocessor GPUs is qualitatively different than programming other multiprocessors like multicore CPUs. GPUs provide two to three orders of magnitude more thread and data parallelism than CPUs, scaling to hundreds of processor cores and tens of thousands of concurrent threads. GPUs continue to increase their parallelism, doubling it about every 12 to 18 months, enabled by Moore‚Äôs law [1965] of increasing integrated circuit density and by improving architectural efciency. To span the wide price and performance range of different market segments, different GPU products implement widely varying numbers of processors and threads. Yet users expect games, graphics, imaging, and computing applications to work on any GPU, regardless of how many parallel threads it executes or how many parallel processor cores it has, and they expect more expensive GPUs (with more threads and cores) to run applications faster. As a result, GPU programming models and application programs are designed to scale transparently to a wide range of parallelism.ÁºñÁ®ãÂ§öÂ§ÑÁêÜÂô®GPU‰∏éÁºñÁ®ãÂÖ∂‰ªñÂ§öÂ§ÑÁêÜÂô®ÔºàÂ¶ÇÂ§öÊ†∏CPUÔºâÁöÑË¥®Èáè‰∏çÂêå„ÄÇ GPUÊØîCPUÊèê‰æõ‰∏§Âà∞‰∏â‰∏™Êï∞ÈáèÁ∫ßÁöÑÁ∫øÁ®ãÂíåÊï∞ÊçÆÂπ∂Ë°åÊÄßÔºåÂèØÊâ©Â±ïÂà∞Êï∞Áôæ‰∏™Â§ÑÁêÜÂô®ÂÜÖÊ†∏ÂíåÊï∞‰∏á‰∏™Âπ∂ÂèëÁ∫øÁ®ã„ÄÇÈöèÁùÄÈõÜÊàêÁîµË∑ØÂØÜÂ∫¶ÁöÑÊèêÈ´òÂíåÊèêÈ´òÊû∂ÊûÑÊïàÁéáÁöÑÊë©Â∞îÂÆöÂæã[1965]ÔºåGPUÊØè12Âà∞18‰∏™ÊúàÂ∞±‰ºöÁªßÁª≠Â¢ûÂä†Âπ∂Ë°åÊÄßÔºåÂπ∂‰∏î‰ºöÂ¢ûÂä†‰∏ÄÂÄç„ÄÇ‰∏∫‰∫ÜË∑®Ë∂ä‰∏çÂêåÁªÜÂàÜÂ∏ÇÂú∫ÁöÑÂπøÊ≥õ‰ª∑Ê†ºÂíåÊÄßËÉΩËåÉÂõ¥Ôºå‰∏çÂêåÁöÑGPU‰∫ßÂìÅÂÆûÁé∞‰∫ÜÂ§ßÈáè‰∏çÂêåÊï∞ÈáèÁöÑÂ§ÑÁêÜÂô®ÂíåÁ∫øÁ®ã„ÄÇÁÑ∂ËÄåÔºåÁî®Êà∑ÊúüÊúõÊ∏∏ÊàèÔºåÂõæÂΩ¢ÔºåÊàêÂÉèÂíåËÆ°ÁÆóÂ∫îÁî®Á®ãÂ∫èÂèØ‰ª•Âú®‰ªª‰ΩïGPU‰∏äËøêË°åÔºåÊó†ËÆ∫ÂÆÉÊâßË°åÂ§öÂ∞ëÂπ∂Ë°åÁ∫øÁ®ãÊàñÂÆÉÊúâÂ§öÂ∞ëÂπ∂Ë°åÂ§ÑÁêÜÂô®Ê†∏ÂøÉÔºå‰ªñ‰ª¨ÊúüÊúõÊõ¥Â§öÊòÇË¥µÁöÑGPUÔºàÂÖ∑ÊúâÊõ¥Â§öÁ∫øÁ®ãÂíåÊ†∏ÂøÉÔºâÂèØ‰ª•Êõ¥Âø´Âú∞ËøêË°åÂ∫îÁî®Á®ãÂ∫è„ÄÇÂõ†Ê≠§ÔºåGPUÁºñÁ®ãÊ®°ÂûãÂíåÂ∫îÁî®Á®ãÂ∫èÊó®Âú®ÈÄèÊòéÂú∞Êâ©Â±ïÂà∞ÂêÑÁßçÂπ∂Ë°åÊÄß„ÄÇ The driving force behind the large number of parallel threads and cores in a GPU is real-time graphics performance‚Äîthe need to render complex 3D scenes with high resolution at interactive frame rates, at least 60 frames per second. Correspondingly, the scalable programming models of graphics shading languages such as Cg (C for graphics) and HLSL (high-level shading language) are designed to exploit large degrees of parallelism via many independent parallel threads and to scale to any number of processor cores. The CUDA scalable parallel programming model similarly enables general parallel computing applications to leverage large numbers of parallel threads and scale to any number of parallel processor cores, transparently to the application.GPU‰∏≠Â§ßÈáèÂπ∂Ë°åÁ∫øÁ®ãÂíåÊ†∏ÂøÉËÉåÂêéÁöÑÈ©±Âä®ÂäõÊòØÂÆûÊó∂ÂõæÂΩ¢ÊÄßËÉΩ - ÈúÄË¶Å‰ª•‰∫§‰∫íÂºèÂ∏ßÈÄüÁéáÔºàËá≥Â∞ëÊØèÁßí60Â∏ßÔºâÊ∏≤ÊüìÂÖ∑ÊúâÈ´òÂàÜËæ®ÁéáÁöÑÂ§çÊùÇ3DÂú∫ÊôØ„ÄÇ Áõ∏Â∫îÂú∞ÔºåÂõæÂΩ¢ÁùÄËâ≤ËØ≠Ë®ÄÔºàÂ¶ÇCgÔºàÂõæÂΩ¢Áî®CÔºâÂíåHLSLÔºàÈ´òÁ∫ßÁùÄËâ≤ËØ≠Ë®ÄÔºâÔºâÁöÑÂèØ‰º∏Áº©ÁºñÁ®ãÊ®°ÂûãÊó®Âú®ÈÄöËøáËÆ∏Â§öÁã¨Á´ãÁöÑÂπ∂Ë°åÁ∫øÁ®ãÂà©Áî®Â§ßÁ®ãÂ∫¶ÁöÑÂπ∂Ë°åÊÄßÔºåÂπ∂Êâ©Â±ïÂà∞‰ªªÊÑèÊï∞ÈáèÁöÑÂ§ÑÁêÜÂô®ÂÜÖÊ†∏„ÄÇ CUDAÂèØÊâ©Â±ïÂπ∂Ë°åÁºñÁ®ãÊ®°ÂûãÂêåÊ†∑‰ΩøÈÄöÁî®Âπ∂Ë°åËÆ°ÁÆóÂ∫îÁî®Á®ãÂ∫èËÉΩÂ§üÂà©Áî®Â§ßÈáèÂπ∂Ë°åÁ∫øÁ®ãÔºåÂπ∂Êâ©Â±ïÂà∞‰ªªÊÑèÊï∞ÈáèÁöÑÂπ∂Ë°åÂ§ÑÁêÜÂô®ÂÜÖÊ†∏ÔºåÂØπÂ∫îÁî®Á®ãÂ∫èÈÄèÊòé„ÄÇ In these scalable programming models, the programmer writes code for a single thread, and the GPU runs myriad thread instances in parallel. Programs thus scale transparently over a wide range of hardware parallelism. Tis simple paradigm arose from graphics APIs and shading languages that describe how to shade one vertex or one pixel. It has remained an effective paradigm as GPUs have rapidly increased their parallelism and performance since the late 1990s.Âú®Ëøô‰∫õÂèØ‰º∏Áº©ÁöÑÁºñÁ®ãÊ®°Âûã‰∏≠ÔºåÁ®ãÂ∫èÂëò‰∏∫Âçï‰∏™Á∫øÁ®ãÁºñÂÜô‰ª£Á†ÅÔºåGPUÂπ∂Ë°åËøêË°åÊó†Êï∞ÁöÑÁ∫øÁ®ãÂÆû‰æã„ÄÇ Âõ†Ê≠§ÔºåÁ®ãÂ∫èÂú®ÂπøÊ≥õÁöÑÁ°¨‰ª∂Âπ∂Ë°åÊÄß‰∏äÈÄèÊòéÂú∞Êâ©Â±ï„ÄÇ ËøôÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑËåÉ‰æãÔºåÂÆÉÊù•Ëá™ÊèèËø∞Â¶Ç‰ΩïÈÅÆËîΩ‰∏Ä‰∏™È°∂ÁÇπÊàñ‰∏Ä‰∏™ÂÉèÁ¥†ÁöÑÂõæÂΩ¢APIÂíåÁùÄËâ≤ËØ≠Ë®Ä„ÄÇ Ëá™‰ªé20‰∏ñÁ∫™90Âπ¥‰ª£Êú´‰ª•Êù•ÔºåGPU‰∏ÄÁõ¥Âú®ËøÖÈÄüÊèêÈ´òÂÖ∂Âπ∂Ë°åÊÄßÂíåÊÄßËÉΩÔºåËøô‰ªçÁÑ∂ÊòØ‰∏ÄÁßçÊúâÊïàÁöÑËåÉ‰æã„ÄÇThis section brieÔ¨Çy describes programming GPUs for real-time graphics applications using graphics APIs and programming languages. It then describes programming GPUs for visual computing and general parallel computing applications using the C language and the CUDA programming model.Êú¨ËäÇÁÆÄË¶Å‰ªãÁªçÂ¶Ç‰Ωï‰ΩøÁî®ÂõæÂΩ¢APIÂíåÁºñÁ®ãËØ≠Ë®Ä‰∏∫ÂÆûÊó∂ÂõæÂΩ¢Â∫îÁî®Á®ãÂ∫èÁºñÁ®ãGPU„ÄÇ ÁÑ∂ÂêéÔºåÂÆÉÊèèËø∞‰∫Ü‰ΩøÁî®CËØ≠Ë®ÄÂíåCUDAÁºñÁ®ãÊ®°ÂûãÁºñÁ®ãÁî®‰∫éÂèØËßÜËÆ°ÁÆóÂíåÈÄöÁî®Âπ∂Ë°åËÆ°ÁÆóÂ∫îÁî®Á®ãÂ∫èÁöÑGPU„ÄÇ Programming Real-Time Graphics ÂÆûÊó∂ÂõæÂΩ¢ÁºñÁ®ãAPIs have played an important role in the rapid, successful development of GPUs and processors. There are two primary standard graphics APIs: OpenGL and Direct3D, one of the Microsoft DirectX multimedia programming interfaces. OpenGL, an open standard, was originally proposed and defined by Silicon Graphics Incorporated. Te ongoing development and extension of the OpenGL standard [Segal and Akeley, 2006], [Kessenich, 2006] is managed by Khronos, an industry consortium. Direct3D [Blythe, 2006], a de facto standard, is definedand evolved forward by Microsoft and partners. OpenGL and Direct3D are similarly structured, and continue to evolve rapidly with GPU hardware advances. They defne a logical graphics processing pipeline that is mapped onto the GPU hardware and processors, along with programming models and languages for the programmable pipeline stages.APIÂú®GPUÂíåÂ§ÑÁêÜÂô®ÁöÑÂø´ÈÄüËÄåÊàêÂäüÁöÑÂèëÂ±ï‰∏≠ÂèëÊå•‰∫ÜÈáçË¶Å‰ΩúÁî®„ÄÇ Áé∞Âú®Êúâ‰∏§‰∏™‰∏ªË¶ÅÁöÑÊ†áÂáÜÂõæÂΩ¢APIÔºöOpenGL Âíå Direct3D„ÄÇ Direct3DÊòØÂæÆËΩØ DirectX Â§öÂ™í‰ΩìÁºñÁ®ãÊé•Âè£‰πã‰∏Ä„ÄÇ OpenGLÊòØ‰∏ÄÁßçÂºÄÊîæÊ†áÂáÜÔºåÊúÄÂàùÊòØÁî±Silicon Graphics IncorporatedÊèêÂá∫Âπ∂ÂÆö‰πâÁöÑ„ÄÇ OpenGLÊ†áÂáÜÁöÑÊåÅÁª≠ÂèëÂ±ïÂíåÊâ©Â±ï[SegalÂíåAkeleyÔºå2006]Ôºå[KessenichÔºå2006]Áî±Ë°å‰∏öÂçè‰ºöKhronosÁÆ°ÁêÜ„ÄÇ Direct3D [BlytheÔºå2006]ÊòØ‰∫ãÂÆû‰∏äÁöÑÊ†áÂáÜÔºåÁî±ÂæÆËΩØÂíåÂêà‰Ωú‰ºô‰º¥ÂÆö‰πâÂíåÂèëÂ±ï„ÄÇ OpenGLÂíåDirect3DÁªìÊûÑÁõ∏‰ººÔºåÂπ∂‰∏îÈöèÁùÄGPUÁ°¨‰ª∂ÁöÑÂèëÂ±ïËÄå‰∏çÊñ≠ÂèëÂ±ï„ÄÇ ÂÆÉ‰ª¨ÂÆö‰πâ‰∫ÜÊò†Â∞ÑÂà∞GPUÁ°¨‰ª∂ÂíåÂ§ÑÁêÜÂô®ÁöÑÈÄªËæëÂõæÂΩ¢Â§ÑÁêÜÊµÅÊ∞¥Á∫øÔºå‰ª•ÂèäÂèØÁºñÁ®ãÊµÅÊ∞¥Á∫øÁ∫ßÁöÑÁºñÁ®ãÊ®°ÂûãÂíåËØ≠Ë®Ä„ÄÇ OpenGL :An open standard graphics API. Direct3D A graphics API defined by Microsoft and partners. ‰∏Ä‰∏™ÂºÄÊîæÊ†áÂáÜÁöÑÂõæÂΩ¢API„ÄÇ Direct3D: A graphics API defined by Microsof and partners. Áî±MicrosoftÂíåÂêà‰Ωú‰ºô‰º¥ÂÆö‰πâÁöÑÂõæÂΩ¢API„ÄÇ ËØëÊ≥®ÔºöDirectX‰∏≠ Direct3DÊé•Âè£Ë¥üË¥£3DÊïàÊûúÊòæÁ§∫ÔºåDirectDrawË¥üË¥£2DÂõæÂÉèÂä†ÈÄü Logical Graphics Pipeline ÈÄªËæëÂõæÂΩ¢ÁÆ°ÈÅìFigure C.3.1 illustrates the Direct3D 10 logical graphics pipeline. OpenGL has a similar graphics pipeline structure. The API and logical pipeline provide a streaming dataÔ¨Çow infrastructure and plumbing for the programmable shader stages, shown in blue. The 3D application sends the GPU a sequence of vertices grouped into geometric primitives‚Äîpoints, lines, triangles, and polygons. The input assembler collects vertices and primitives. The vertex shader program executes per-vertex processing including transforming the vertex 3D position into a screen position and lighting the vertex to determine its color. The geometry shader program executes per-primitive processing and can add or drop primitives. The setup and rasterizer unit generates pixel fragments (fragments are potential contributions to pixels) that are covered by a geometric primitive. The pixel shader program performs per-fragment processing, including interpolating per-fragment parameters, texturing, and coloring. Pixel shaders make extensive use of sampled and filtered lookups into large 1D, 2D, or 3D arrays called textures, using interpolated Ô¨Çoating-point coordinates. Shaders use texture accesses for maps, functions, decals, images, and data. The raster operations processing (or output merger) stage performs Z-buffer depth testing and stencil testing, which may discard a hidden pixel fragment or replace the pixel‚Äôs depth with the fragment‚Äôs depth, and performs a color blending operation that combines the fragment color with the pixel color and writes the pixel with the blended color.ÂõæC.3.1ËØ¥Êòé‰∫ÜDirect3D 10ÁöÑÈÄªËæëÂõæÂΩ¢ÁÆ°ÈÅì„ÄÇ OpenGLÂÖ∑ÊúâÁ±ª‰ººÁöÑÂõæÂΩ¢ÁÆ°ÈÅìÁªìÊûÑ„ÄÇ APIÂíåÈÄªËæëÁÆ°ÈÅì‰∏∫ÂèØÁºñÁ®ãÁùÄËâ≤Âô®Èò∂ÊÆµÊèê‰æõÊµÅÊï∞ÊçÆÊµÅÂü∫Á°ÄÁªìÊûÑÂíåÁÆ°ÈÅìÔºå‰ª•ËìùËâ≤ÊòæÁ§∫„ÄÇ 3DÂ∫îÁî®Á®ãÂ∫èÂêëGPUÂèëÈÄÅ‰∏ÄÁ≥ªÂàóÈ°∂ÁÇπÔºåËøô‰∫õÈ°∂ÁÇπË¢´ÂàÜÁªÑ‰∏∫Âá†‰ΩïÂõæÂÖÉ - ÁÇπÔºåÁ∫øÔºå‰∏âËßíÂΩ¢ÂíåÂ§öËæπÂΩ¢„ÄÇËæìÂÖ•Ê±áÁºñÁ®ãÂ∫èÊî∂ÈõÜÈ°∂ÁÇπÂíåÂü∫ÂÖÉ„ÄÇÈ°∂ÁÇπÁùÄËâ≤Âô®Á®ãÂ∫èÊâßË°åÊØèÈ°∂ÁÇπÂ§ÑÁêÜÔºåÂåÖÊã¨Â∞ÜÈ°∂ÁÇπ3D‰ΩçÁΩÆÂèòÊç¢‰∏∫Â±èÂπï‰ΩçÁΩÆÂπ∂ÁÇπ‰∫ÆÈ°∂ÁÇπ‰ª•Á°ÆÂÆöÂÖ∂È¢úËâ≤„ÄÇÂá†‰ΩïÁùÄËâ≤Âô®Á®ãÂ∫èÊâßË°åÊØè‰∏™Âü∫ÂÖÉÂ§ÑÁêÜÔºåÂπ∂ÂèØ‰ª•Ê∑ªÂä†ÊàñÂà†Èô§Âü∫ÂÖÉ„ÄÇËÆæÁΩÆÂíåÂÖâÊ†ÖÂåñÂô®ÂçïÂÖÉÁîüÊàêÁî±Âá†‰ΩïÂõæÂÖÉË¶ÜÁõñÁöÑÂÉèÁ¥†ÁâáÊÆµÔºàÁâáÊÆµÊòØÂØπÂÉèÁ¥†ÁöÑÊΩúÂú®Ë¥°ÁåÆÔºâ„ÄÇÂÉèÁ¥†ÁùÄËâ≤Âô®Á®ãÂ∫èÊâßË°åÊØèÁâáÊÆµÂ§ÑÁêÜÔºåÂåÖÊã¨ÂÜÖÊèíÊØèÁâáÊÆµÂèÇÊï∞ÔºåÁ∫πÁêÜÂíåÁùÄËâ≤„ÄÇÂÉèÁ¥†ÁùÄËâ≤Âô®‰ΩøÁî®ÊèíÂÄºÁöÑÊµÆÁÇπÂùêÊ†áÔºåÂ∞ÜÈááÊ†∑ÂíåÊª§Ê≥¢Êü•ÊâæÂπøÊ≥õÁî®‰∫éÁß∞‰∏∫Á∫πÁêÜÁöÑÂ§ßÂûã1DÔºå2DÊàñ3DÈòµÂàó„ÄÇÁùÄËâ≤Âô®‰ΩøÁî®Á∫πÁêÜËÆøÈóÆÂú∞ÂõæÔºåÂáΩÊï∞ÔºåË¥¥Ëä±ÔºåÂõæÂÉèÂíåÊï∞ÊçÆ„ÄÇÂÖâÊ†ÖÊìç‰ΩúÂ§ÑÁêÜÔºàÊàñËæìÂá∫ÂêàÂπ∂ÔºâÈò∂ÊÆµÊâßË°åZÁºìÂÜ≤Ê∑±Â∫¶ÊµãËØïÂíåÊ®°ÊùøÊµãËØïÔºåÂÆÉÂèØ‰ª•‰∏¢ÂºÉÈöêËóèÁöÑÂÉèÁ¥†ÁâáÊÆµÊàñÁî®ÁâáÊÆµÁöÑÊ∑±Â∫¶ÊõøÊç¢ÂÉèÁ¥†ÁöÑÊ∑±Â∫¶ÔºåÊâßË°åÂ∞ÜÁâáÊÆµÈ¢úËâ≤‰∏éÂÉèÁ¥†È¢úËâ≤ÁªÑÂêàÂú®‰∏ÄËµ∑ÁöÑÊ∑∑ÂêàÊìç‰ΩúÔºåÂπ∂Â∞ÜÊ∑∑ÂêàÁöÑÈ¢úËâ≤ÂÜôÂÖ•ÂÉèÁ¥†„ÄÇ The graphics API and graphics pipeline provide input, output, memory objects, and infrastructure for the shader programs that process each vertex, primitive, and pixel fragment.ÂõæÂΩ¢APIÂíåÂõæÂΩ¢ÁÆ°ÈÅì‰∏∫Â§ÑÁêÜÊØè‰∏™È°∂ÁÇπÔºåÂü∫ÂÖÉÂíåÂÉèÁ¥†ÁâáÊÆµÁöÑÁùÄËâ≤Âô®Á®ãÂ∫èÊèê‰æõËæìÂÖ•ÔºåËæìÂá∫ÔºåÂ≠òÂÇ®Âô®ÂØπË±°ÂíåÂü∫Á°ÄÁªìÊûÑ„ÄÇ texture: A 1D, 2D, or 3D array that supports sampled and filtered lookups with interpolated coordinates. ÊîØÊåÅÂ∏¶ÊèíÂÄºÂùêÊ†áÁöÑÈááÊ†∑ÂíåÊª§Ê≥¢Êü•ÊâæÁöÑ1DÔºå2DÊàñ3DÈòµÂàó„ÄÇ Graphics Shader Programs ÂõæÂΩ¢ÁùÄËâ≤Âô®Á®ãÂ∫èReal-time graphics applications use many diÔ¨Äerent shader programs to model how light interacts with diÔ¨Äerent materials and to render complex lighting and shadows. Shading languages are based on a dataÔ¨Çow or streaming programming model that corresponds with the logical graphics pipeline. Vertex shader programs map the position of triangle vertices onto the screen, altering their position, color, or orientation. Typically a vertex shader thread inputs a Ô¨Çoating-point (x, y, z, w) vertex position and computes a Ô¨Çoating-point (x, y, z) screen position. Geometry shader programs operate on geometric primitives (such as lines and triangles) defined by multiple vertices, changing them or generating additional primitives. Pixel fragment shaders each ‚Äúshade‚Äù one pixel, computing a Ô¨Çoating-point red, green, blue, alpha (RGBA) color contribution to the rendered image at its pixel sample (x, y) image position. Shaders (and GPUs) use Ô¨Çoating-point arithmetic for all pixel color calculations to eliminate visible artifacts while computing the extreme range of pixel contribution values encountered while rendering scenes with complex lighting, shadows, and high dynamic range. For all three types of graphics shaders, many program instances can be run in parallel, as independent parallel threads, because each works on independent data, produces independent results, and has no side effects. Independent vertices, primitives, and pixels further enable the same graphics program to run on diÔ¨Äerently sized GPUs that process different numbers of vertices, primitives, and pixels in parallel. Graphics programs thus scale transparently to GPUs with different amounts of parallelism and performance. shader: A program that operates on graphics data such as a vertex or a pixel fragment. ‰∏ÄÁßçÂØπÂõæÂΩ¢Êï∞ÊçÆÔºàÂ¶ÇÈ°∂ÁÇπÊàñÂÉèÁ¥†ÁâáÊÆµÔºâËøõË°åÊìç‰ΩúÁöÑÁ®ãÂ∫è„ÄÇ shading language: A graphics rendering language, usually having a dataÔ¨Çow or streaming programming model.‰∏ÄÁßçÂõæÂΩ¢Ê∏≤ÊüìËØ≠Ë®ÄÔºåÈÄöÂ∏∏ÂÖ∑ÊúâÊï∞ÊçÆÊµÅÊàñÊµÅÁºñÁ®ãÊ®°Âûã„ÄÇ Users program all three logical graphics threads with a common targeted highlevel language. HLSL (high-level shading language) and Cg (C for graphics) are commonly used. They have C-like syntax and a rich set of library functions for matrix operations, trigonometry, interpolation, and texture access and filtering, but are far from general computing languages: they currently lack general memory access, pointers, fle I/O, and recursion. HLSL and Cg assume that programs live within a logical graphics pipeline, and thus I/O is implicit. For example, a pixel fragment shader may expect the geometric normal and multiple texture coordinates to have been interpolated from vertex values by upstream fxed-function stages and can simply assign a value to the COLOR output parameter to pass it downstream to be blended with a pixel at an implied (x, y) position.Áî®Êà∑‰ΩøÁî®ÂÖ±ÂêåÁöÑÁõÆÊ†áÈ´òÁ∫ßËØ≠Ë®ÄÂØπÊâÄÊúâ‰∏â‰∏™ÈÄªËæëÂõæÂΩ¢Á∫øÁ®ãËøõË°åÁºñÁ®ã„ÄÇ ÈÄöÂ∏∏‰ΩøÁî®HLSLÔºàÈ´òÁ∫ßÁùÄËâ≤ËØ≠Ë®ÄÔºâÂíåCgÔºàÂõæÂΩ¢Áî®CÔºâ„ÄÇ ÂÆÉ‰ª¨ÂÖ∑ÊúâÁ±ª‰ººCËØ≠Ê≥ïÂíå‰∏∞ÂØåÁöÑÂ∫ìÂáΩÊï∞ÔºåÁî®‰∫éÁü©ÈòµËøêÁÆóÔºå‰∏âËßíÂáΩÊï∞ÔºåÊèíÂÄºÂíåÁ∫πÁêÜËÆøÈóÆÂíåËøáÊª§Ôºå‰ΩÜËøú‰∏çÊòØÈÄöÁî®ËÆ°ÁÆóËØ≠Ë®ÄÔºöÂÆÉ‰ª¨ÁõÆÂâçÁº∫Â∞ëÈÄöÁî®ÂÜÖÂ≠òËÆøÈóÆÔºåÊåáÈíàÔºåÊñá‰ª∂I / OÂíåÈÄíÂΩí„ÄÇ HLSLÂíåCgÂÅáËÆæÁ®ãÂ∫èÂ≠òÂú®‰∫éÈÄªËæëÂõæÂΩ¢ÁÆ°ÈÅì‰∏≠ÔºåÂõ†Ê≠§I / OÊòØÈöêÂê´ÁöÑ„ÄÇ ‰æãÂ¶ÇÔºåÂÉèÁ¥†ÁâáÊÆµÁùÄËâ≤Âô®ÂèØ‰ª•È¢ÑÊúüÂá†‰ΩïÊ≥ïÁ∫øÂíåÂ§ö‰∏™Á∫πÁêÜÂùêÊ†áÂ∑≤ÁªèÈÄöËøá‰∏äÊ∏∏Âõ∫ÂÆöÂäüËÉΩÈò∂ÊÆµ‰ªéÈ°∂ÁÇπÂÄºÊèíÂÄºÔºåÂπ∂‰∏îÂèØ‰ª•ÁÆÄÂçïÂú∞Â∞ÜÂÄºÂàÜÈÖçÁªôCOLORËæìÂá∫ÂèÇÊï∞‰ª•Â∞ÜÂÖ∂‰º†ÈÄíÂà∞‰∏ãÊ∏∏‰ª•‰∏éÂÉèÁ¥†Ê∑∑Âêà„ÄÇ Âú®ÈöêÂê´ÁöÑÔºàxÔºåyÔºâ‰ΩçÁΩÆ„ÄÇThe GPU hardware creates a new independent thread to execute a vertex, geometry, or pixel shader program for every vertex, every primitive, and every pixel fragment. In video games, the bulk of threads execute pixel shader programs, as there are typically 10 to 20 times or more pixel fragments than vertices, and complex lighting and shadows require even larger ratios of pixel to vertex shader threads. The graphics shader programming model drove the GPU architecture to effciently execute thousands of independent fne-grained threads on many parallel processor cores.GPUÁ°¨‰ª∂ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÁã¨Á´ãÁ∫øÁ®ãÔºå‰∏∫ÊØè‰∏™È°∂ÁÇπÔºåÊØè‰∏™Âü∫ÂÖÉÂíåÊØè‰∏™ÂÉèÁ¥†ÁâáÊÆµÊâßË°åÈ°∂ÁÇπÔºåÂá†‰ΩïÊàñÂÉèÁ¥†ÁùÄËâ≤Âô®Á®ãÂ∫è„ÄÇ Âú®ËßÜÈ¢ëÊ∏∏Êàè‰∏≠ÔºåÂ§ßÈÉ®ÂàÜÁ∫øÁ®ãÊâßË°åÂÉèÁ¥†ÁùÄËâ≤Âô®Á®ãÂ∫èÔºåÂõ†‰∏∫ÈÄöÂ∏∏Â≠òÂú®ÊØîÈ°∂ÁÇπÂ§ö10Âà∞20ÂÄçÊàñÊõ¥Â§öÁöÑÂÉèÁ¥†ÁâáÊÆµÔºåÂπ∂‰∏îÂ§çÊùÇÁöÑÂÖâÁÖßÂíåÈò¥ÂΩ±ÈúÄË¶ÅÁîöËá≥Êõ¥Â§ßÊØî‰æãÁöÑÂÉèÁ¥†Âà∞È°∂ÁÇπÁùÄËâ≤Âô®Á∫øÁ®ã„ÄÇ ÂõæÂΩ¢ÁùÄËâ≤Âô®ÁºñÁ®ãÊ®°ÂûãÊé®Âä®GPUÊû∂ÊûÑÂú®ËÆ∏Â§öÂπ∂Ë°åÂ§ÑÁêÜÂô®ÂÜÖÊ†∏‰∏äÊúâÊïàÂú∞ÊâßË°åÊï∞ÂçÉ‰∏™Áã¨Á´ãÁöÑÁªÜÁ≤íÂ∫¶Á∫øÁ®ã„ÄÇ Pixel Shader ExampleConsider the following Cg pixel shader program that implements the ‚Äúenvironment mapping‚Äù rendering technique. For each pixel thread, this shader is passed five parameters, including 2D Ô¨Çoating-point texture image coordinates needed to sample the surface color, and a 3D Ô¨Çoating-point vector giving the refection of the view direction off the surface. The other three ‚Äúuniform‚Äù parameters do not vary from one pixel instance (thread) to the next. The shader looks up color in two texture images: a 2D texture access for the surface color, and a 3D texture access into a cube map (six images corresponding to the faces of a cube) to obtain the external world color corresponding to the refection direction. Then the final four-component (red, green, blue, alpha) Ô¨Çoating-point color is computed using a weighted average called a ‚Äúlerp‚Äù or linear interpolation function.ËÄÉËôë‰ª•‰∏ãCgÂÉèÁ¥†ÁùÄËâ≤Âô®Á®ãÂ∫èÔºåËØ•Á®ãÂ∫èÂÆûÁé∞‚ÄúÁéØÂ¢ÉÊò†Â∞Ñ‚ÄùÊ∏≤ÊüìÊäÄÊúØ„ÄÇ ÂØπ‰∫éÊØè‰∏™ÂÉèÁ¥†Á∫øÁ®ãÔºåËØ•ÁùÄËâ≤Âô®‰º†ÈÄí‰∫î‰∏™ÂèÇÊï∞ÔºåÂåÖÊã¨ÈááÊ†∑Ë°®Èù¢È¢úËâ≤ÊâÄÈúÄÁöÑ2DÊµÆÁÇπÁ∫πÁêÜÂõæÂÉèÂùêÊ†áÔºå‰ª•Âèä3DÊµÆÁÇπÁü¢ÈáèÔºå‰ªéËÄå‰ΩøËßÜÂõæÊñπÂêëÂÅèÁ¶ªË°®Èù¢„ÄÇ ÂÖ∂‰ªñ‰∏â‰∏™‚ÄúÁªü‰∏Ä‚ÄùÂèÇÊï∞‰∏ç‰ºö‰ªé‰∏Ä‰∏™ÂÉèÁ¥†ÂÆû‰æãÔºàÁ∫øÁ®ãÔºâÂà∞‰∏ã‰∏Ä‰∏™ÂÉèÁ¥†ÂÆû‰æã„ÄÇ ÁùÄËâ≤Âô®Âú®‰∏§‰∏™Á∫πÁêÜÂõæÂÉè‰∏≠Êü•ÊâæÈ¢úËâ≤ÔºöË°®Èù¢È¢úËâ≤ÁöÑ2DÁ∫πÁêÜËÆøÈóÆÔºå‰ª•ÂèäÁ´ãÊñπ‰ΩìË¥¥Âõæ‰∏≠ÁöÑ3DÁ∫πÁêÜËÆøÈóÆÔºàÂØπÂ∫î‰∫éÁ´ãÊñπ‰ΩìÁöÑÈù¢ÁöÑÂÖ≠‰∏™ÂõæÂÉèÔºâÔºå‰ª•Ëé∑Âæó‰∏éÂèçÂ∞ÑÊñπÂêëÂØπÂ∫îÁöÑÂ§ñÈÉ®‰∏ñÁïåÈ¢úËâ≤„ÄÇ ÁÑ∂Âêé‰ΩøÁî®Áß∞‰∏∫‚Äúlerp‚ÄùÊàñÁ∫øÊÄßÊèíÂÄºÂáΩÊï∞ÁöÑÂä†ÊùÉÂπ≥ÂùáÊù•ËÆ°ÁÆóÊúÄÁªàÁöÑÂõõÂàÜÈáèÔºàÁ∫¢Ëâ≤ÔºåÁªøËâ≤ÔºåËìùËâ≤ÔºåŒ±ÔºâÊµÆÁÇπÈ¢úËâ≤„ÄÇ123456789101112131415void refection(float2 texCoord : TEXCOORD0,float3 refection_dir : TEXCOORD1,out float4 color : COLOR,uniform float shiny,uniform sampler2D surfaceMap,uniform samplerCUBE envMap)&#123;// Fetch the surface color from a texturefloat4 surfaceColor = tex2D(surfaceMap, texCoord);// Fetch reflected color by sampling a cube mapfloat4 reflectedColor = texCUBE(environmentMap, refection_dir);// Output is weighted average of the two colorscolor = lerp(surfaceColor, refectedColor, shiny);&#125; Although this shader program is only three lines long, it activates a lot of GPU hardware. For each texture fetch, the GPU texture subsystem makes multiple memory accesses to sample image colors in the vicinity of the sampling coordinates, and then interpolates the final result with Ô¨Çoating-point fltering arithmetic. The multithreaded GPU executes thousands of these lightweight Cg pixel shader threads in parallel, deeply interleaving them to hide texture fetch and memory latency.ËôΩÁÑ∂Ëøô‰∏™ÁùÄËâ≤Âô®Á®ãÂ∫èÂè™Êúâ‰∏âË°åÔºå‰ΩÜÂÆÉÊøÄÊ¥ª‰∫ÜÂæàÂ§öGPUÁ°¨‰ª∂„ÄÇ ÂØπ‰∫éÊØèÊ¨°Á∫πÁêÜÊèêÂèñÔºåGPUÁ∫πÁêÜÂ≠êÁ≥ªÁªüÂØπÈááÊ†∑ÂùêÊ†áÈôÑËøëÁöÑÈááÊ†∑ÂõæÂÉèÈ¢úËâ≤ËøõË°åÂ§öÊ¨°Â≠òÂÇ®Âô®ËÆøÈóÆÔºåÁÑ∂Âêé‰ΩøÁî®ÊµÆÁÇπËøêÁÆóÁÆóÊ≥ïÂØπÊúÄÁªàÁªìÊûúËøõË°åÊèíÂÄº„ÄÇ Â§öÁ∫øÁ®ãGPUÂπ∂Ë°åÊâßË°åÊï∞ÂçÉ‰∏™Ëøô‰∫õËΩªÈáèÁ∫ßCgÂÉèÁ¥†ÁùÄËâ≤Âô®Á∫øÁ®ãÔºåÊ∑±Â∫¶‰∫§Èîô‰ª•ÈöêËóèÁ∫πÁêÜËé∑ÂèñÂíåÂÜÖÂ≠òÂª∂Ëøü„ÄÇCg focuses the programmer‚Äôs view to a single vertex or primitive or pixel, which the GPU implements as a single thread; the shader program transparently scales to exploit thread parallelism on the available processors. Being applicationspecifc, Cg provides a rich set of useful data types, library functions, and language constructs to express diverse rendering techniques.CgÂ∞ÜÁ®ãÂ∫èÂëòÁöÑËßÜÂõæËÅöÁÑ¶Âà∞Âçï‰∏™È°∂ÁÇπÊàñÂü∫ÂÖÉÊàñÂÉèÁ¥†ÔºåGPUÂ∞ÜÂÖ∂ÂÆûÁé∞‰∏∫Âçï‰∏™Á∫øÁ®ã; ÁùÄËâ≤Âô®Á®ãÂ∫èÈÄèÊòéÂú∞Êâ©Â±ï‰ª•Âà©Áî®ÂèØÁî®Â§ÑÁêÜÂô®‰∏äÁöÑÁ∫øÁ®ãÂπ∂Ë°åÊÄß„ÄÇ ‰Ωú‰∏∫Â∫îÁî®Á®ãÂ∫èËßÑËåÉÔºåCgÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑÊúâÁî®Êï∞ÊçÆÁ±ªÂûãÔºåÂ∫ìÂáΩÊï∞ÂíåËØ≠Ë®ÄÁªìÊûÑÔºå‰ª•Ë°®ËææÂêÑÁßçÂëàÁé∞ÊäÄÊúØ„ÄÇFigure C.3.2 shows skin rendered by a fragment pixel shader. Real skin appears quite different from Ô¨Çesh-color paint because light bounces around a lot before re-emerging. In this complex shader, three separate skin layers, each with unique subsurface scattering behavior, are modeled to give the skin a visual depth and translucency. Scattering can be modeled by a blurring convolution in a fattened ‚Äútexture‚Äù space, with red being blurred more than green, and blue blurred less. The compiled Cg shader executes 1400 instructions to compute the color of one skin pixel.ÂõæC.3.2ÊòæÁ§∫‰∫ÜÁî±ÁâáÊÆµÂÉèÁ¥†ÁùÄËâ≤Âô®Ê∏≤ÊüìÁöÑÂ§ñËßÇ„ÄÇ ÁúüÊ≠£ÁöÑÁöÆËÇ§ÁúãËµ∑Êù•‰∏éÁ≤âÁ∫¢Ëâ≤Ê∂ÇÊñôÂÆåÂÖ®‰∏çÂêåÔºåÂõ†‰∏∫Âú®ÈáçÊñ∞Âá∫Áé∞‰πãÂâçÔºåÂÖâÁ∫ø‰ºöÂú®ÂæàÂ§öÂú∞ÊñπÂèçÂºπ„ÄÇ Âú®Ëøô‰∏™Â§çÊùÇÁöÑÁùÄËâ≤Âô®‰∏≠Ôºå‰∏â‰∏™Áã¨Á´ãÁöÑÁöÆËÇ§Â±ÇÔºàÊØè‰∏™ÈÉΩÂÖ∑ÊúâÁã¨ÁâπÁöÑÊ¨°Ë°®Èù¢Êï£Â∞ÑË°å‰∏∫ÔºâË¢´Âª∫Ê®°Ôºå‰ª•Ëµã‰∫àÁöÆËÇ§ËßÜËßâÊ∑±Â∫¶ÂíåÂçäÈÄèÊòéÂ∫¶„ÄÇ Êï£Â∞ÑÂèØ‰ª•ÈÄöËøáÂú®ËÇ•ËÉñÁöÑ‚ÄúÁ∫πÁêÜ‚ÄùÁ©∫Èó¥‰∏≠ÁöÑÊ®°Á≥äÂç∑ÁßØÊù•Âª∫Ê®°ÔºåÂÖ∂‰∏≠Á∫¢Ëâ≤ÊØîÁªøËâ≤Êõ¥Ê®°Á≥äÔºåËÄåËìùËâ≤Ê®°Á≥äÂæóÊõ¥Â∞ë„ÄÇ ÁºñËØëÁöÑCgÁùÄËâ≤Âô®ÊâßË°å1400Êù°Êåá‰ª§‰ª•ËÆ°ÁÆó‰∏Ä‰∏™ÁöÆËÇ§ÂÉèÁ¥†ÁöÑÈ¢úËâ≤„ÄÇ As GPUs have evolved superior Ô¨Çoating-point performance and very high streaming memory bandwidth for real-time graphics, they have attracted highly parallel applications beyond traditional graphics. At frst, access to this power was available only by couching an application as a graphics-rendering algorithm, but this GPGPU approach was ofen awkward and limiting. More recently, the CUDA programming model has provided a far easier way to exploit the scalable high-performance Ô¨Çoating-point and memory bandwidth of GPUs with the C programming language.Áî±‰∫éGPUÂ∑≤Áªè‰∏∫ÂÆûÊó∂ÂõæÂΩ¢Êèê‰æõ‰∫ÜÂá∫Ëâ≤ÁöÑÊµÆÁÇπÊÄßËÉΩÂíåÈùûÂ∏∏È´òÁöÑÊµÅÂ≠òÂÇ®Âô®Â∏¶ÂÆΩÔºåÂõ†Ê≠§ÂÆÉ‰ª¨Âê∏Âºï‰∫ÜË∂ÖË∂ä‰º†ÁªüÂõæÂΩ¢ÁöÑÈ´òÂ∫¶Âπ∂Ë°åÂ∫îÁî®„ÄÇ È¶ñÂÖàÔºåÂè™ÊúâÈÄöËøáÂ∞ÜÂ∫îÁî®Á®ãÂ∫è‰Ωú‰∏∫ÂõæÂΩ¢Ê∏≤ÊüìÁÆóÊ≥ïËøõË°åËÆøÈóÆÊâçËÉΩËé∑ÂæóËøôÁßçÂäüËÉΩÔºå‰ΩÜÊòØËøôÁßçGPGPUÊñπÊ≥ïÈùûÂ∏∏Á¨®Êãô‰∏îÊúâÈôê„ÄÇ ÊúÄËøëÔºåCUDAÁºñÁ®ãÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥ÁÆÄÂçïÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Âà©Áî®CÁºñÁ®ãËØ≠Ë®ÄÂà©Áî®GPUÁöÑÂèØÊâ©Â±ïÈ´òÊÄßËÉΩÊµÆÁÇπÂíåÂ≠òÂÇ®Âô®Â∏¶ÂÆΩ„ÄÇ Programming Parallel Computing ApplicationsCUDA, Brook, and CAL are programming interfaces for GPUs that are focused on data parallel computation rather than on graphics. CAL (Compute Abstraction Layer) is a low-level assembler language interface for AMD GPUs. Brook is a streaming language adapted for GPUs by Buck et al. [2004]. CUDA, developed by NVIDIA [2007], is an extension to the C and C++ languages for scalable parallel programming of manycore GPUs and multicore CPUs. The CUDA programming model is described below, adapted from an article by Nickolls et al.[2008].CUDAÔºåBrookÂíåCALÊòØGPUÁöÑÁºñÁ®ãÊé•Âè£Ôºå‰∏ìÊ≥®‰∫éÊï∞ÊçÆÂπ∂Ë°åËÆ°ÁÆóËÄåÈùûÂõæÂΩ¢„ÄÇ CALÔºàCompute Abstraction LayerÔºâÊòØAMD GPUÁöÑ‰ΩéÁ∫ßÊ±áÁºñËØ≠Ë®ÄÊé•Âè£„ÄÇ BrookÊòØ‰∏ÄÁßçÈÄÇÁî®‰∫éGPUÁ≠âGPUÁöÑÊµÅÂ™í‰ΩìËØ≠Ë®Ä„ÄÇ[2004Âπ¥]„ÄÇ Áî±NVIDIA [2007]ÂºÄÂèëÁöÑCUDAÊòØCÂíåC ++ËØ≠Ë®ÄÁöÑÊâ©Â±ïÔºåÁî®‰∫éÂ§öÊ†∏GPUÂíåÂ§öÊ†∏CPUÁöÑÂèØÊâ©Â±ïÂπ∂Ë°åÁºñÁ®ã„ÄÇ ‰∏ãÈù¢ÊèèËø∞‰∫ÜCUDAÁºñÁ®ãÊ®°ÂûãÔºåÊîπÁºñËá™NickollsÁ≠â‰∫∫[2008]ÁöÑÊñáÁ´†„ÄÇ With the new model the GPU excels in data parallel and throughput computing, executing high performance computing applications as well as graphics applications.Âá≠ÂÄüÊñ∞ÂûãÂè∑ÔºåGPUÊìÖÈïøÊï∞ÊçÆÂπ∂Ë°åÂíåÂêûÂêêÈáèËÆ°ÁÆóÔºåÊâßË°åÈ´òÊÄßËÉΩËÆ°ÁÆóÂ∫îÁî®Á®ãÂ∫è‰ª•ÂèäÂõæÂΩ¢Â∫îÁî®Á®ãÂ∫è„ÄÇ Data Parallel Problem Decomposition Êï∞ÊçÆÂπ∂Ë°åÈóÆÈ¢òÂàÜËß£To map large computing problems effectively to a highly parallel processing architecture, the programmer or compiler decomposes the problem into many small problems that can be solved in parallel. For example, the programmer partitions a large result data array into blocks and further partitions each block into elements, such that the result blocks can be computed independently in parallel, and the elements within each block are computed in parallel. Figure C.3.3 shows a decomposition of a result data array into a 3 x 2 grid of blocks, where each block is further decomposed into a 5 x 3 array of elements. Te two-level parallel decomposition maps naturally to the GPU architecture: parallel multiprocessors compute result blocks, and parallel threads compute result elements.‰∏∫‰∫ÜÂ∞ÜÂ§ßÂûãËÆ°ÁÆóÈóÆÈ¢òÊúâÊïàÂú∞Êò†Â∞ÑÂà∞È´òÂ∫¶Âπ∂Ë°åÁöÑÂ§ÑÁêÜÊû∂ÊûÑÔºåÁ®ãÂ∫èÂëòÊàñÁºñËØëÂô®Â∞ÜÈóÆÈ¢òÂàÜËß£‰∏∫ËÆ∏Â§öÂèØ‰ª•Âπ∂Ë°åËß£ÂÜ≥ÁöÑÂ∞èÈóÆÈ¢ò„ÄÇ ‰æãÂ¶ÇÔºåÁ®ãÂ∫èÂëòÂ∞ÜÂ§ßÁªìÊûúÊï∞ÊçÆÈòµÂàóÂàÜÊàêÂùóÂπ∂Ëøõ‰∏ÄÊ≠•Â∞ÜÊØè‰∏™ÂùóÂàÜÊàêÂÖÉÁ¥†Ôºå‰ΩøÂæóÁªìÊûúÂùóÂèØ‰ª•Áã¨Á´ãÂú∞Âπ∂Ë°åËÆ°ÁÆóÔºåÂπ∂‰∏îÊØè‰∏™ÂùóÂÜÖÁöÑÂÖÉÁ¥†ÊòØÂπ∂Ë°åËÆ°ÁÆóÁöÑ„ÄÇ ÂõæC.3.3ÊòæÁ§∫‰∫ÜÂ∞ÜÁªìÊûúÊï∞ÊçÆÈòµÂàóÂàÜËß£‰∏∫3√ó2ÁöÑÂùóÁΩëÊ†ºÔºåÂÖ∂‰∏≠ÊØè‰∏™ÂùóËøõ‰∏ÄÊ≠•ÂàÜËß£‰∏∫5√ó3ÁöÑÂÖÉÁ¥†ÈòµÂàó„ÄÇ ‰∏§Á∫ßÂπ∂Ë°åÂàÜËß£Ëá™ÁÑ∂Âú∞Êò†Â∞ÑÂà∞GPUÊû∂ÊûÑÔºöÂπ∂Ë°åÂ§öÂ§ÑÁêÜÂô®ËÆ°ÁÆóÁªìÊûúÂùóÔºåÂπ∂Ë°åÁ∫øÁ®ãËÆ°ÁÆóÁªìÊûúÂÖÉÁ¥†„ÄÇ The programmer writes a program that computes a sequence of result data grids, partitioning each result grid into coarse-grained result blocks that can be computed independently in parallel. The program computes each result block withan array of fine-grained parallel threads, partitioning the work among threads so that each computes one or more result elements.Á®ãÂ∫èÂëòÁºñÂÜô‰∏Ä‰∏™Á®ãÂ∫èÊù•ËÆ°ÁÆóÁªìÊûúÊï∞ÊçÆÁΩëÊ†ºÂ∫èÂàóÔºåÂ∞ÜÊØè‰∏™ÁªìÊûúÁΩëÊ†ºÂàíÂàÜ‰∏∫ÂèØ‰ª•Âπ∂Ë°åÁã¨Á´ãËÆ°ÁÆóÁöÑÁ≤óÁ≤íÂ∫¶ÁªìÊûúÂùó„ÄÇ Á®ãÂ∫è‰ΩøÁî®ÁªÜÁ≤íÂ∫¶Âπ∂Ë°åÁ∫øÁ®ãÊï∞ÁªÑËÆ°ÁÆóÊØè‰∏™ÁªìÊûúÂùóÔºåÂú®Á∫øÁ®ã‰πãÈó¥ÂØπÂ∑•‰ΩúËøõË°åÂàÜÂå∫Ôºå‰ª•‰æøÊØè‰∏™ÈÉΩËÆ°ÁÆó‰∏Ä‰∏™ÊàñÂ§ö‰∏™ÁªìÊûúÂÖÉÁ¥†„ÄÇ Scalable Parallel Programming with CUDA ‰ΩøÁî®CUDAËøõË°åÂèØÊâ©Â±ïÁöÑÂπ∂Ë°åÁºñÁ®ãThe CUDA scalable parallel programming model extends the C and C++ languages to exploit large degrees of parallelism for general applications on highly parallel multiprocessors, particularly GPUs. Early experience with CUDA shows that many sophisticated programs can be readily expressed with a few easily understood abstractions. Since NVIDIA released CUDA in 2007, developers have rapidly developed scalable parallel programs for a wide range of applications, including seismic data processing, computational chemistry, linear algebra, sparse matrix solvers, sorting, searching, physics models, and visual computing. These applications scale transparently to hundreds of processor cores and thousands of concurrent threads. NVIDIA GPUs with the Tesla unifed graphics and computing architecture (described in Sections C.4 and C.7) run CUDA C programs, and are widely available in laptops, PCs, workstations, and servers. The CUDA model is also applicable to other shared memory parallel processing architectures, including multicore CPUs.CUDAÂèØÊâ©Â±ïÂπ∂Ë°åÁºñÁ®ãÊ®°ÂûãÊâ©Â±ï‰∫ÜCÂíåC ++ËØ≠Ë®ÄÔºå‰ª•‰æøÂú®È´òÂ∫¶Âπ∂Ë°åÁöÑÂ§öÂ§ÑÁêÜÂô®ÔºàÂ∞§ÂÖ∂ÊòØGPUÔºâ‰∏ä‰∏∫‰∏ÄËà¨Â∫îÁî®Á®ãÂ∫èÂà©Áî®Â§ßÁ®ãÂ∫¶ÁöÑÂπ∂Ë°åÊÄß„ÄÇÊó©Êúü‰ΩøÁî®CUDAÁöÑÁªèÈ™åË°®ÊòéÔºåËÆ∏Â§öÂ§çÊùÇÁöÑÁ®ãÂ∫èÂèØ‰ª•ÈÄöËøá‰∏Ä‰∫õÊòì‰∫éÁêÜËß£ÁöÑÊäΩË±°Êù•Ë°®Ëææ„ÄÇËá™NVIDIA‰∫é2007Âπ¥ÂèëÂ∏ÉCUDA‰ª•Êù•ÔºåÂºÄÂèë‰∫∫ÂëòÂ∑≤ËøÖÈÄüÂºÄÂèëÂá∫ÂèØÊâ©Â±ïÁöÑÂπ∂Ë°åÁ®ãÂ∫èÔºåÈÄÇÁî®‰∫éÂêÑÁßçÂ∫îÁî®ÔºåÂåÖÊã¨Âú∞ÈúáÊï∞ÊçÆÂ§ÑÁêÜÔºåËÆ°ÁÆóÂåñÂ≠¶ÔºåÁ∫øÊÄß‰ª£Êï∞ÔºåÁ®ÄÁñèÁü©ÈòµÊ±ÇËß£Âô®ÔºåÊéíÂ∫èÔºåÊêúÁ¥¢ÔºåÁâ©ÁêÜÊ®°ÂûãÂíåËßÜËßâËÆ°ÁÆó„ÄÇËøô‰∫õÂ∫îÁî®Á®ãÂ∫èÈÄèÊòéÂú∞Êâ©Â±ïÂà∞Êï∞Áôæ‰∏™Â§ÑÁêÜÂô®Ê†∏ÂøÉÂíåÊï∞ÂçÉ‰∏™Âπ∂ÂèëÁ∫øÁ®ã„ÄÇÈááÁî®TeslaÁªü‰∏ÄÂõæÂΩ¢ÂíåËÆ°ÁÆóÊû∂ÊûÑÁöÑNVIDIA GPUÔºàÂú®C.4ÂíåC.7ËäÇ‰∏≠ÊèèËø∞ÔºâËøêË°åCUDA CÁ®ãÂ∫èÔºåÂπ∂ÂπøÊ≥õÁî®‰∫éÁ¨îËÆ∞Êú¨ÁîµËÑëÔºåPCÔºåÂ∑•‰ΩúÁ´ôÂíåÊúçÂä°Âô®„ÄÇ CUDAÊ®°Âûã‰πüÈÄÇÁî®‰∫éÂÖ∂‰ªñÂÖ±‰∫´ÂÜÖÂ≠òÂπ∂Ë°åÂ§ÑÁêÜ‰ΩìÁ≥ªÁªìÊûÑÔºåÂåÖÊã¨Â§öÊ†∏CPU„ÄÇCUDA provides three key abstractions-a hierarchy of thread groups, shared memories, and barrier synchronization‚Äîthat provide a clear parallel structure to conventional C code for one thread of the hierarchy. Multiple levels of threads, memory, and synchronization provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. The abstractions guide the programmer to partition the problem into coarse subproblems that can be solved independently in parallel, and then into fner pieces that can be solved in parallel. The programming model scales transparently to large numbers of processor cores: a compiled CUDA program executes on any number of processors, and only the runtime system needs to know the physical processor count.CUDAÊèê‰æõ‰∏â‰∏™ÂÖ≥ÈîÆÁöÑÊäΩË±° - Á∫øÁ®ãÁªÑÔºåÂÖ±‰∫´Â≠òÂÇ®Âô®ÂíåÂ±èÈöúÂêåÊ≠•ÁöÑÂ±ÇÊ¨°ÁªìÊûÑ - ‰∏∫Â±ÇÊ¨°ÁªìÊûÑÁöÑ‰∏Ä‰∏™Á∫øÁ®ãÊèê‰æõ‰∏é‰º†ÁªüC‰ª£Á†ÅÁöÑÊ∏ÖÊô∞Âπ∂Ë°åÁªìÊûÑ„ÄÇ Â§öÁ∫ßÁ∫øÁ®ãÔºåÂÜÖÂ≠òÂíåÂêåÊ≠•Êèê‰æõÁªÜÁ≤íÂ∫¶Êï∞ÊçÆÂπ∂Ë°åÂíåÁ∫øÁ®ãÂπ∂Ë°åÔºåÂµåÂ•óÂú®Á≤óÁ≤íÂ∫¶Êï∞ÊçÆÂπ∂Ë°åÂíå‰ªªÂä°Âπ∂Ë°å‰∏≠„ÄÇ ÊäΩË±°ÊåáÂØºÁ®ãÂ∫èÂëòÂ∞ÜÈóÆÈ¢òÂàíÂàÜ‰∏∫Á≤óÁï•ÁöÑÂ≠êÈóÆÈ¢òÔºåËøô‰∫õÂ≠êÈóÆÈ¢òÂèØ‰ª•Âπ∂Ë°åÁã¨Á´ãËß£ÂÜ≥ÔºåÁÑ∂ÂêéÂàÜÊàêÂèØ‰ª•Âπ∂Ë°åÊ±ÇËß£ÁöÑÊõ¥Â§öÈÉ®ÂàÜ„ÄÇ ÁºñÁ®ãÊ®°ÂûãÈÄèÊòéÂú∞Êâ©Â±ïÂà∞Â§ßÈáèÂ§ÑÁêÜÂô®ÂÜÖÊ†∏ÔºöÁºñËØëÁöÑCUDAÁ®ãÂ∫èÂú®‰ªªÊÑèÊï∞ÈáèÁöÑÂ§ÑÁêÜÂô®‰∏äÊâßË°åÔºåÂè™ÊúâËøêË°åÊó∂Á≥ªÁªüÈúÄË¶ÅÁü•ÈÅìÁâ©ÁêÜÂ§ÑÁêÜÂô®Êï∞Èáè„ÄÇ The CUDA Paradigm CUDAËåÉÂºèCUDA is a minimal extension of the C and C++ programming languages. The programmer writes a serial program that calls parallel kernels, which may be simple functions or full programs. A kernel executes in parallel across a set of parallel threads. The programmer organizes these threads into a hierarchy of thread blocks and grids of thread blocks. A thread block is a set of concurrent threads that can cooperate among themselves through barrier synchronization and through shared access to a memory space private to the block. A grid is a set of thread blocks that may each be executed independently and thus may execute in parallel.CUDAÊòØCÂíåC ++ÁºñÁ®ãËØ≠Ë®ÄÁöÑÊúÄÂ∞èÊâ©Â±ï„ÄÇ Á®ãÂ∫èÂëòÁºñÂÜô‰∏Ä‰∏™Ë∞ÉÁî®Âπ∂Ë°åÂÜÖÊ†∏ÁöÑ‰∏≤Ë°åÁ®ãÂ∫èÔºåÂèØ‰ª•ÊòØÁÆÄÂçïÁöÑÂáΩÊï∞ÊàñÂÆåÊï¥ÁöÑÁ®ãÂ∫è„ÄÇ ÂÜÖÊ†∏Ë∑®‰∏ÄÁªÑÂπ∂Ë°åÁ∫øÁ®ãÂπ∂Ë°åÊâßË°å„ÄÇ Á®ãÂ∫èÂëòÂ∞ÜËøô‰∫õÁ∫øÁ®ãÁªÑÁªáÊàêÁ∫øÁ®ãÂùóÂíåÁ∫øÁ®ãÂùóÁΩëÊ†ºÁöÑÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇ Á∫øÁ®ãÂùóÊòØ‰∏ÄÁªÑÂπ∂ÂèëÁ∫øÁ®ãÔºåÂÆÉ‰ª¨ÂèØ‰ª•ÈÄöËøáÂ±èÈöúÂêåÊ≠•ÂíåÈÄöËøáÂØπÂùó‰∏ìÁî®ÁöÑÂÜÖÂ≠òÁ©∫Èó¥ÁöÑÂÖ±‰∫´ËÆøÈóÆÊù•Áõ∏‰∫íÂçè‰Ωú„ÄÇ ÁΩëÊ†ºÊòØ‰∏ÄÁªÑÁ∫øÁ®ãÂùóÔºåÊØè‰∏™Á∫øÁ®ãÂùóÂèØ‰ª•Áã¨Á´ãÊâßË°åÔºåÂõ†Ê≠§ÂèØ‰ª•Âπ∂Ë°åÊâßË°å„ÄÇ kernel: A program or function for one thread, designed to be executed by many threads.‰∏Ä‰∏™Á®ãÂ∫èÊàñÂáΩÊï∞ÔºåÁî®‰∫é‰∏Ä‰∏™Á∫øÁ®ãÔºåÊó®Âú®Áî±ËÆ∏Â§öÁ∫øÁ®ãÊâßË°å„ÄÇ thread block: A set of concurrent threads that execute the same thread program and may cooperate to compute a result.‰∏ÄÁªÑÂπ∂ÂèëÁ∫øÁ®ãÔºåÂÆÉ‰ª¨ÊâßË°åÁõ∏ÂêåÁöÑÁ∫øÁ®ãÁ®ãÂ∫èÂπ∂ÂèØ‰ª•Âçè‰ΩúËÆ°ÁÆóÁªìÊûú„ÄÇ grid: A set of thread blocks that execute the same kernel program ‰∏ÄÁªÑÊâßË°åÁõ∏ÂêåÂÜÖÊ†∏Á®ãÂ∫èÁöÑÁ∫øÁ®ãÂùó When invoking a kernel, the programmer specifes the number of threads per block and the number of blocks comprising the grid. Each thread is given a unique thread ID number threadIdx within its thread block, numbered 0, 1, 2, ‚Ä¶, blockDim-1, and each thread block is given a unique block ID number blockIdx within its grid. CUDA supports thread blocks containing up to 512 threads. For convenience, thread blocks and grids may have 1, 2, or 3 dimensions, accessed via .x, .y, and .z index fields.Âú®Ë∞ÉÁî®ÂÜÖÊ†∏Êó∂ÔºåÁ®ãÂ∫èÂëòÊåáÂÆöÊØè‰∏™ÂùóÁöÑÁ∫øÁ®ãÊï∞ÂíåÊûÑÊàêÁΩëÊ†ºÁöÑÂùóÊï∞„ÄÇ ÊØè‰∏™Á∫øÁ®ãÂú®ÂÖ∂Á∫øÁ®ãÂùóÂÜÖË¢´Ëµã‰∫àÂîØ‰∏ÄÁöÑÁ∫øÁ®ãIDÂè∑threadIdxÔºåÁºñÂè∑‰∏∫0,1,2Ôºå‚Ä¶ÔºåblockDim-1ÔºåÂπ∂‰∏îÊØè‰∏™Á∫øÁ®ãÂùóÂú®ÂÖ∂ÁΩëÊ†ºÂÜÖË¢´Ëµã‰∫àÂîØ‰∏ÄÁöÑÂùóIDÂè∑blockIdx„ÄÇ CUDAÊîØÊåÅÂåÖÂê´Â§öËææ512‰∏™Á∫øÁ®ãÁöÑÁ∫øÁ®ãÂùó„ÄÇ ‰∏∫Êñπ‰æøËµ∑ËßÅÔºåÁ∫øÁ®ãÂùóÂíåÁΩëÊ†ºÂèØ‰ª•Êúâ1,2Êàñ3‰∏™Áª¥Â∫¶ÔºåÂèØÈÄöËøá.xÔºå.yÂíå.zÁ¥¢ÂºïÂ≠óÊÆµËÆøÈóÆ„ÄÇ As a very simple example of parallel programming, suppose that we are given two vectors x and y of n Ô¨Çoating-point numbers each and that we wish to compute the result of y = ax + y for some scalar value a. This is the so-called SAXPY kernel defined by the BLAS linear algebra library. Figure C.3.4 shows C code for performing this computation on both a serial processor and in parallel using CUDA.‰Ωú‰∏∫Âπ∂Ë°åÁºñÁ®ãÁöÑ‰∏Ä‰∏™ÈùûÂ∏∏ÁÆÄÂçïÁöÑ‰æãÂ≠êÔºåÂÅáËÆæÊàë‰ª¨ÁªôÂá∫‰∫Ü‰∏§‰∏™ÂêëÈáèxÂíåyÁöÑn‰∏™ÊµÆÁÇπÊï∞ÔºåÂπ∂‰∏îÊàë‰ª¨Â∏åÊúõËÆ°ÁÆóÊüê‰∫õÊ†áÈáèÂÄºaÁöÑy = ax + yÁöÑÁªìÊûú„ÄÇ ËøôÊòØÁî±BLASÁ∫øÊÄß‰ª£Êï∞Â∫ìÂÆö‰πâÁöÑÊâÄË∞ìÁöÑSAXPYÂÜÖÊ†∏„ÄÇ ÂõæC.3.4ÊòæÁ§∫‰∫Ü‰ΩøÁî®CUDAÂú®‰∏≤Ë°åÂ§ÑÁêÜÂô®ÂíåÂπ∂Ë°å‰∏äÊâßË°åÊ≠§ËÆ°ÁÆóÁöÑC‰ª£Á†Å„ÄÇ The __global__ declaration specifer indicates that the procedure is a kernelentry point. CUDA programs launch parallel kernels with the extended functioncall syntax:__global__Â£∞ÊòéËØ¥ÊòéÁ¨¶Ë°®ÊòéËØ•ËøáÁ®ãÊòØ‰∏Ä‰∏™ÂÜÖÊ†∏ÂÖ•Âè£ÁÇπ„ÄÇ CUDAÁ®ãÂ∫è‰ΩøÁî®Êâ©Â±ïÂäüËÉΩÂêØÂä®Âπ∂Ë°åÂÜÖÊ†∏Ë∞ÉÁî®ËØ≠Ê≥ïÔºö 1kernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(... parameter list ...); where dimGrid and dimBlock are three-element vectors of type dim3 that specify the dimensions of the grid in blocks and the dimensions of the blocks in threads, respectively. Unspecifed dimensions default to one.ÂÖ∂‰∏≠dimGridÂíådimBlockÊòØdim3Á±ªÂûãÁöÑ‰∏âÂÖÉÁ¥†ÂêëÈáèÔºåÂàÜÂà´ÊåáÂÆöÂùó‰∏≠ÁΩëÊ†ºÁöÑÂ∞∫ÂØ∏ÂíåÁ∫øÁ®ã‰∏≠ÂùóÁöÑÂ∞∫ÂØ∏„ÄÇ Êú™ÊåáÂÆöÁöÑÁª¥Â∫¶ÈªòËÆ§‰∏∫1„ÄÇ In Figure C.3.4, we launch a grid of n threads that assigns one thread to each element of the vectors and puts 256 threads in each block. Each individual thread computes an element index from its thread and block IDs and then performs the desired calculation on the corresponding vector elements. Comparing the serial and parallel versions of this code, we see that they are strikingly similar. This represents a fairly common pattern. Te serial code consists of a loop where each iteration is independent of all the others. Such loops can be mechanically transformed into parallel kernels: each loop iteration becomes an independent thread. By assigning a single thread to each output element, we avoid the need for any synchronization among threads when writing results to memory.Âú®ÂõæC.3.4‰∏≠ÔºåÊàë‰ª¨ÂêØÂä®‰∫Ü‰∏Ä‰∏™ÂåÖÂê´n‰∏™Á∫øÁ®ãÁöÑÁΩëÊ†ºÔºåÂÆÉ‰∏∫‰∏Ä‰∏™ÂêëÈáèÁöÑÊØè‰∏™ÂÖÉÁ¥†ÂàÜÈÖç‰∏Ä‰∏™Á∫øÁ®ãÔºåÂπ∂Âú®ÊØè‰∏™Âùó‰∏≠ÊîæÂÖ•256‰∏™Á∫øÁ®ã„ÄÇÊØè‰∏™ÂçïÁã¨ÁöÑÁ∫øÁ®ã‰ªéÂÖ∂Á∫øÁ®ãÂíåÂùóIDËÆ°ÁÆóÂÖÉÁ¥†Á¥¢ÂºïÔºåÁÑ∂ÂêéÂØπÁõ∏Â∫îÁöÑÂêëÈáèÂÖÉÁ¥†ÊâßË°åÊâÄÈúÄÁöÑËÆ°ÁÆó„ÄÇÊØîËæÉÊ≠§‰ª£Á†ÅÁöÑ‰∏≤Ë°åÂíåÂπ∂Ë°åÁâàÊú¨ÔºåÊàë‰ª¨ÂèëÁé∞ÂÆÉ‰ª¨ÈùûÂ∏∏Áõ∏‰ºº„ÄÇËøô‰ª£Ë°®‰∫Ü‰∏ÄÁßçÁõ∏ÂΩìÊôÆÈÅçÁöÑÊ®°Âºè„ÄÇ TeÂ∫èÂàó‰ª£Á†ÅÁî±‰∏Ä‰∏™Âæ™ÁéØÁªÑÊàêÔºåÂÖ∂‰∏≠ÊØè‰∏™Ëø≠‰ª£Áã¨Á´ã‰∫éÊâÄÊúâÂÖ∂‰ªñËø≠‰ª£„ÄÇËøôÊ†∑ÁöÑÂæ™ÁéØÂèØ‰ª•Êú∫Ê¢∞Âú∞ËΩ¨Êç¢‰∏∫Âπ∂Ë°åÂÜÖÊ†∏ÔºöÊØè‰∏™Âæ™ÁéØËø≠‰ª£Êàê‰∏∫‰∏Ä‰∏™Áã¨Á´ãÁöÑÁ∫øÁ®ã„ÄÇÈÄöËøá‰∏∫ÊØè‰∏™ËæìÂá∫ÂÖÉÁ¥†ÂàÜÈÖçÂçï‰∏™Á∫øÁ®ãÔºåÊàë‰ª¨ÈÅøÂÖç‰∫ÜÂú®Â∞ÜÁªìÊûúÂÜôÂÖ•ÂÜÖÂ≠òÊó∂Á∫øÁ®ã‰πãÈó¥ÁöÑ‰ªª‰ΩïÂêåÊ≠•„ÄÇ The text of a CUDA kernel is simply a C function for one sequential thread. Thus, it is generally straightforward to write and is typically simpler than writing parallel code for vector operations. Parallelism is determined clearly and explicitly by specifying the dimensions of a grid and its thread blocks when launching a kernel. CUDAÂÜÖÊ†∏ÁöÑÊñáÊú¨Âè™ÊòØ‰∏Ä‰∏™È°∫Â∫èÁ∫øÁ®ãÁöÑCÂáΩÊï∞„ÄÇÂõ†Ê≠§ÔºåÁºñÂÜôÈÄöÂ∏∏ÂæàÁÆÄÂçïÔºåÂπ∂‰∏îÈÄöÂ∏∏ÊØî‰∏∫ÂêëÈáèÊìç‰ΩúÁºñÂÜôÂπ∂Ë°å‰ª£Á†ÅÊõ¥ÁÆÄÂçï„ÄÇÈÄöËøáÂú®ÂêØÂä®ÂÜÖÊ†∏Êó∂ÊåáÂÆöÁΩëÊ†ºÂèäÂÖ∂Á∫øÁ®ãÂùóÁöÑÂ∞∫ÂØ∏ÔºåÂèØ‰ª•Ê∏ÖÊ•öÊòéÁ°ÆÂú∞Á°ÆÂÆöÂπ∂Ë°åÊÄß„ÄÇ Parallel execution and thread management is automatic. All thread creation, scheduling, and termination is handled for the programmer by the underlying system. Indeed, a Tesla architecture GPU performs all thread management directly in hardware. The threads of a block execute concurrently and may synchronize at a synchronization barrier by calling the __syncthreads() intrinsic. This guarantees that no thread in the block can proceed until all threads in the block have reached the barrier. Afer passing the barrier, these threads are also guaranteed to see all writes to memory performed by threads in the block before the barrier. Thus, threads in a block may communicate with each other by writing and reading per-block shared memory at a synchronization barrier.Âπ∂Ë°åÊâßË°åÂíåÁ∫øÁ®ãÁÆ°ÁêÜÊòØËá™Âä®ÁöÑ„ÄÇ ÊâÄÊúâÁ∫øÁ®ãÂàõÂª∫ÔºåË∞ÉÂ∫¶ÂíåÁªàÊ≠¢ÈÉΩÁî±Â∫ïÂ±ÇÁ≥ªÁªü‰∏∫Á®ãÂ∫èÂëòÂ§ÑÁêÜ„ÄÇ ÂÆûÈôÖ‰∏äÔºåTeslaÊû∂ÊûÑGPUÁõ¥Êé•Âú®Á°¨‰ª∂‰∏≠ÊâßË°åÊâÄÊúâÁ∫øÁ®ãÁÆ°ÁêÜ„ÄÇ ÂùóÁöÑÁ∫øÁ®ãÂπ∂ÂèëÊâßË°åÔºåÂπ∂‰∏îÂèØ‰ª•ÈÄöËøáË∞ÉÁî®__syncthreadsÔºàÔºâÂÜÖÂú®ÂáΩÊï∞Âú®ÂêåÊ≠•Â±èÈöú‰∏äÂêåÊ≠•„ÄÇ Ëøô‰øùËØÅ‰∫ÜÂùó‰∏≠ÁöÑÊâÄÊúâÁ∫øÁ®ãÈÉΩÂà∞ËææÂ±èÈöú‰πãÂâçÂùó‰∏≠ÁöÑ‰ªª‰ΩïÁ∫øÁ®ãÈÉΩ‰∏çËÉΩÁªßÁª≠„ÄÇ Âú®ÈÄöËøáÂ±èÈöúÂêéÔºåËøô‰∫õÁ∫øÁ®ã‰πüÂèØ‰ª•‰øùËØÅÂú®Â±èÈöú‰πãÂâçÁúãÂà∞Âùó‰∏≠Á∫øÁ®ãÊâßË°åÁöÑÊâÄÊúâÂÜÖÂ≠òÂÜôÂÖ•„ÄÇÂõ†Ê≠§ÔºåÂùó‰∏≠ÁöÑÁ∫øÁ®ãÂèØ‰ª•ÈÄöËøáÂú®ÂêåÊ≠•Â±èÈöúÂ§ÑÂÜôÂÖ•ÂíåËØªÂèñÊØèÂùóÂÖ±‰∫´Â≠òÂÇ®Âô®Êù•ÂΩºÊ≠§ÈÄö‰ø°„ÄÇ synchronization barrier: Threads wait at a synchronization barrier until all threads in the thread block arrive at the barrier. Á∫øÁ®ãÂú®ÂêåÊ≠•ÈöúÁ¢çÂ§ÑÁ≠âÂæÖÔºåÁõ¥Âà∞Á∫øÁ®ãÂùó‰∏≠ÁöÑÊâÄÊúâÁ∫øÁ®ãÂà∞ËææÂ±èÈöú„ÄÇ Since threads in a block may share memory and synchronize via barriers, they will reside together on the same physical processor or multiprocessor. The number of thread blocks can, however, greatly exceed the number of processors. The CUDA thread programming model virtualizes the processors and gives the programmer the Ô¨Çexibility to parallelize at whatever granularity is most convenient. Virtualization into threads and thread blocks allows intuitive problem decompositions, as the number of blocks can be dictated by the size of the data being processed rather than by the number of processors in the system. It also allows the same CUDA program to scale to widely varying numbers of processor cores.Áî±‰∫éÂùó‰∏≠ÁöÑÁ∫øÁ®ãÂèØ‰ª•ÂÖ±‰∫´ÂÜÖÂ≠òÂπ∂ÈÄöËøáÈöúÁ¢çËøõË°åÂêåÊ≠•ÔºåÂõ†Ê≠§ÂÆÉ‰ª¨Â∞Ü‰∏ÄËµ∑È©ªÁïôÂú®Âêå‰∏ÄÁâ©ÁêÜÂ§ÑÁêÜÂô®ÊàñÂ§öÂ§ÑÁêÜÂô®‰∏ä„ÄÇ ‰ΩÜÊòØÔºåÁ∫øÁ®ãÂùóÁöÑÊï∞ÈáèÂèØ‰ª•Â§ßÂ§ßË∂ÖËøáÂ§ÑÁêÜÂô®ÁöÑÊï∞Èáè„ÄÇ CUDAÁ∫øÁ®ãÁºñÁ®ãÊ®°ÂûãËôöÊãüÂåñÂ§ÑÁêÜÂô®ÔºåÂπ∂‰ΩøÁ®ãÂ∫èÂëòËÉΩÂ§üÁÅµÊ¥ªÂú∞‰ª•ÊúÄÊñπ‰æøÁöÑÁ≤íÂ∫¶ËøõË°åÂπ∂Ë°åÂåñ„ÄÇ ËôöÊãüÂåñÂà∞Á∫øÁ®ãÂíåÁ∫øÁ®ãÂùóÂÖÅËÆ∏Áõ¥ËßÇÁöÑÈóÆÈ¢òÂàÜËß£ÔºåÂõ†‰∏∫ÂùóÁöÑÊï∞ÈáèÂèØ‰ª•Áî±Ê≠£Âú®Â§ÑÁêÜÁöÑÊï∞ÊçÆÁöÑÂ§ßÂ∞èËÄå‰∏çÊòØÁî±Á≥ªÁªü‰∏≠ÁöÑÂ§ÑÁêÜÂô®ÁöÑÊï∞ÈáèÂÜ≥ÂÆö„ÄÇ ÂÆÉËøòÂÖÅËÆ∏Áõ∏ÂêåÁöÑCUDAÁ®ãÂ∫èÊâ©Â±ïÂà∞ÂêÑÁßçÊï∞ÈáèÁöÑÂ§ÑÁêÜÂô®ÂÜÖÊ†∏„ÄÇ To manage this processing element virtualization and provide scalability, CUDA requires that thread blocks be able to execute independently. It must be possible to execute blocks in any order, in parallel or in series. Different blocks have no means of direct communication, although they may coordinate their activities using atomic memory operations on the global memory visible to all threads‚Äîby atomically incrementing queue pointers, for example. This independence requirement allows thread blocks to be scheduled in any order across any number of cores, making the CUDA model scalable across an arbitrary number of cores as well as across a variety of parallel architectures. It also helps to avoid the possibility of deadlock. An application may execute multiple grids either independently or dependently. Independent grids may execute concurrently, given sufcient hardware resources. Dependent grids execute sequentially, with an implicit interkernel barrier between them, thus guaranteeing that all blocks of the first grid complete before any block of the second, dependent grid begins.‰∏∫‰∫ÜÁÆ°ÁêÜÊ≠§Â§ÑÁêÜÂÖÉÁ¥†ËôöÊãüÂåñÂπ∂Êèê‰æõÂèØ‰º∏Áº©ÊÄßÔºåCUDAË¶ÅÊ±ÇÁ∫øÁ®ãÂùóËÉΩÂ§üÁã¨Á´ãÊâßË°å„ÄÇÂøÖÈ°ªËÉΩÂ§ü‰ª•‰ªª‰ΩïÈ°∫Â∫èÔºåÂπ∂Ë°åÊàñ‰∏≤Ë°åÊâßË°åÂùó„ÄÇ‰∏çÂêåÁöÑÂùóÊ≤°ÊúâÁõ¥Êé•ÈÄö‰ø°ÁöÑÊâãÊÆµÔºåÂ∞ΩÁÆ°ÂÆÉ‰ª¨ÂèØ‰ª•‰ΩøÁî®ÂéüÂ≠êÂÜÖÂ≠òÊìç‰ΩúÂú®ÊâÄÊúâÁ∫øÁ®ãÂèØËßÅÁöÑÂÖ®Â±ÄÂÜÖÂ≠ò‰∏äÂçèË∞ÉÂÆÉ‰ª¨ÁöÑÊ¥ªÂä® - ‰æãÂ¶ÇÈÄöËøáÂéüÂ≠êÈÄíÂ¢ûÁöÑÈòüÂàóÊåáÈíà„ÄÇËøôÁßçÁã¨Á´ãÊÄßË¶ÅÊ±ÇÂÖÅËÆ∏Âú®‰ªªÊÑèÊï∞ÈáèÁöÑÂÜÖÊ†∏‰∏ä‰ª•‰ªª‰ΩïÈ°∫Â∫èË∞ÉÂ∫¶Á∫øÁ®ãÂùóÔºå‰ΩøCUDAÊ®°ÂûãÂèØ‰ª•Ë∑®‰ªªÊÑèÊï∞ÈáèÁöÑÂÜÖÊ†∏‰ª•ÂèäÂêÑÁßçÂπ∂Ë°å‰ΩìÁ≥ªÁªìÊûÑËøõË°åÊâ©Â±ï„ÄÇÂÆÉËøòÊúâÂä©‰∫éÈÅøÂÖçÊ≠ªÈîÅÁöÑÂèØËÉΩÊÄß„ÄÇÂ∫îÁî®Á®ãÂ∫èÂèØ‰ª•Áã¨Á´ãÂú∞Êàñ‰æùËµñÂú∞ÊâßË°åÂ§ö‰∏™ÁΩëÊ†º„ÄÇÂú®ÁªôÂÆöË∂≥Â§üÁöÑÁ°¨‰ª∂ËµÑÊ∫êÁöÑÊÉÖÂÜµ‰∏ãÔºåÁã¨Á´ãÁΩëÊ†ºÂèØ‰ª•ÂêåÊó∂ÊâßË°å„ÄÇ‰æùËµñÁΩëÊ†ºÈ°∫Â∫èÊâßË°åÔºåÂÆÉ‰ª¨‰πãÈó¥ÂÖ∑ÊúâÈöêÂºèÂÜÖÊ†∏Â±èÈöúÔºå‰ªéËÄå‰øùËØÅÁ¨¨‰∏ÄÁΩëÊ†ºÁöÑÊâÄÊúâÂùóÂú®Á¨¨‰∫å‰æùËµñÁΩëÊ†ºÁöÑ‰ªª‰ΩïÂùóÂºÄÂßã‰πãÂâçÂÆåÊàê„ÄÇ atomic memory operation: A memory read, modify, write operation sequence that completes without any intervening access.ÂÜÖÂ≠òËØªÂèñÔºå‰øÆÊîπÔºåÂÜôÂÖ•Êìç‰ΩúÂ∫èÂàóÔºåÊó†ÈúÄ‰ªª‰Ωï‰∏≠Èó¥ËÆøÈóÆÂç≥ÂèØÂÆåÊàê„ÄÇ Threads may access data from multiple memory spaces during their execution. Each thread has a private local memory. CUDA uses local memory for threadprivate variables that do not ft in the thread‚Äôs registers, as well as for stack frames and register spilling. Each thread block has a shared memory, visible to all threads of the block, which has the same lifetime as the block. Finally, all threads have access to the same global memory. Programs declare variables in shared and global memory with the shared and device type qualifers. On a Tesla architecture GPU, these memory spaces correspond to physically separate memories: per-block shared memory is a low-latency on-chip RAM, while global memory resides in the fast DRAM on the graphics board.Á∫øÁ®ãÂèØ‰ª•Âú®ÊâßË°åÊúüÈó¥‰ªéÂ§ö‰∏™Â≠òÂÇ®Á©∫Èó¥ËÆøÈóÆÊï∞ÊçÆ„ÄÇ ÊØè‰∏™Á∫øÁ®ãÈÉΩÊúâ‰∏Ä‰∏™ÁßÅÊúâÊú¨Âú∞ÂÜÖÂ≠ò„ÄÇ CUDAÂ∞ÜÊú¨Âú∞ÂÜÖÂ≠òÁî®‰∫éÁ∫øÁ®ãÁßÅÊúâÂèòÈáèÔºåËøô‰∫õÂèòÈáè‰∏çÂú®Á∫øÁ®ãÂØÑÂ≠òÂô®‰∏≠Ôºå‰πü‰∏çÁî®‰∫éÂ†ÜÊ†àÂ∏ßÂíåÂØÑÂ≠òÂô®Ê∫¢Âá∫„ÄÇ ÊØè‰∏™Á∫øÁ®ãÂùóÈÉΩÊúâ‰∏Ä‰∏™ÂÖ±‰∫´ÂÜÖÂ≠òÔºåÂØπÂùóÁöÑÊâÄÊúâÁ∫øÁ®ãÈÉΩÊòØÂèØËßÅÁöÑÔºåÂÆÉ‰∏éÂùóÁöÑÁîüÂëΩÂë®ÊúüÁõ∏Âêå„ÄÇ ÊúÄÂêéÔºåÊâÄÊúâÁ∫øÁ®ãÈÉΩÂèØ‰ª•ËÆøÈóÆÁõ∏ÂêåÁöÑÂÖ®Â±ÄÂÜÖÂ≠ò„ÄÇ Á®ãÂ∫è‰ΩøÁî®sharedÂíådeviceÁ±ªÂûãÁöÑÈôêÂÆöÁ¨¶Âú®ÂÖ±‰∫´ÂíåÂÖ®Â±ÄÂÜÖÂ≠ò‰∏≠Â£∞ÊòéÂèòÈáè„ÄÇ Âú®TeslaÊû∂ÊûÑGPU‰∏äÔºåËøô‰∫õÂ≠òÂÇ®Á©∫Èó¥ÂØπÂ∫î‰∫éÁâ©ÁêÜ‰∏äÁã¨Á´ãÁöÑÂ≠òÂÇ®Âô®ÔºöÊØèÂùóÂÖ±‰∫´Â≠òÂÇ®Âô®ÊòØ‰ΩéÂª∂ËøüÁâá‰∏äRAMÔºåËÄåÂÖ®Â±ÄÂ≠òÂÇ®Âô®È©ªÁïôÂú®ÂõæÂΩ¢Êùø‰∏äÁöÑÂø´ÈÄüDRAM‰∏≠„ÄÇ local memory: Per-thread local memory private to the thread. ÊØèÁ∫øÁ®ãÊú¨Âú∞ÂÜÖÂ≠òÂØπÁ∫øÁ®ãÊòØÁßÅÊúâÁöÑ„ÄÇshared memory: Per-block memory shared by all threads of the block. ÂùóÁöÑÊâÄÊúâÁ∫øÁ®ãÂÖ±‰∫´ÁöÑÊØèÂùóÂÜÖÂ≠ò„ÄÇglobal memory: Per-application memory shared by all threads. ÊâÄÊúâÁ∫øÁ®ãÂÖ±‰∫´ÁöÑÊØè‰∏™Â∫îÁî®Á®ãÂ∫èÂÜÖÂ≠ò„ÄÇ Shared memory is expected to be a low-latency memory near each processor, much like an L1 cache. It can therefore provide high-performance communication and data sharing among the threads of a thread block. Since it has the same lifetime as its corresponding thread block, kernel code will typically initialize data in shared variables, compute using shared variables, and copy shared memory results to global memory. Thread blocks of sequentially dependent grids communicate via global memory, using it to read input and write results.ÂÖ±‰∫´ÂÜÖÂ≠òÈ¢ÑËÆ°ÊòØÊØè‰∏™Â§ÑÁêÜÂô®ÈôÑËøëÁöÑ‰ΩéÂª∂ËøüÂÜÖÂ≠òÔºåÂæàÂÉèL1ÁºìÂ≠ò„ÄÇ Âõ†Ê≠§ÔºåÂÆÉÂèØ‰ª•Âú®Á∫øÁ®ãÂùóÁöÑÁ∫øÁ®ã‰πãÈó¥Êèê‰æõÈ´òÊÄßËÉΩÈÄö‰ø°ÂíåÊï∞ÊçÆÂÖ±‰∫´„ÄÇ Áî±‰∫éÂÆÉ‰∏éÁõ∏Â∫îÁöÑÁ∫øÁ®ãÂùóÂÖ∑ÊúâÁõ∏ÂêåÁöÑÁîüÂëΩÂë®ÊúüÔºåÂõ†Ê≠§ÂÜÖÊ†∏‰ª£Á†ÅÈÄöÂ∏∏‰ºöÂàùÂßãÂåñÂÖ±‰∫´ÂèòÈáè‰∏≠ÁöÑÊï∞ÊçÆÔºå‰ΩøÁî®ÂÖ±‰∫´ÂèòÈáèËøõË°åËÆ°ÁÆóÔºåÂπ∂Â∞ÜÂÖ±‰∫´ÂÜÖÂ≠òÁªìÊûúÂ§çÂà∂Âà∞ÂÖ®Â±ÄÂÜÖÂ≠ò‰∏≠„ÄÇ È°∫Â∫èÁõ∏ÂÖ≥ÁΩëÊ†ºÁöÑÁ∫øÁ®ãÂùóÈÄöËøáÂÖ®Â±ÄÂ≠òÂÇ®Âô®ËøõË°åÈÄö‰ø°Ôºå‰ΩøÁî®ÂÆÉÊù•ËØªÂèñËæìÂÖ•ÂíåÂÜôÂÖ•ÁªìÊûú„ÄÇFigure C.3.5 shows diagrams of the nested levels of threads, thread blocks, and grids of thread blocks. It further shows the corresponding levels of memory sharing: local, shared, and global memories for per-thread, per-thread-block, and per-application data sharing.ÂõæC.3.5ÊòæÁ§∫‰∫ÜÁ∫øÁ®ãÂùóÁöÑÁ∫øÁ®ãÔºåÁ∫øÁ®ãÂùóÂíåÁΩëÊ†ºÁöÑÂµåÂ•óÁ∫ßÂà´ÁöÑÂõæË°®„ÄÇ ÂÆÉËøõ‰∏ÄÊ≠•ÊòæÁ§∫‰∫ÜÁõ∏Â∫îÁöÑÂÜÖÂ≠òÂÖ±‰∫´Á∫ßÂà´ÔºöÊØè‰∏™Á∫øÁ®ãÔºåÊØè‰∏™Á∫øÁ®ãÂùóÂíåÊØè‰∏™Â∫îÁî®Á®ãÂ∫èÊï∞ÊçÆÂÖ±‰∫´ÁöÑÊú¨Âú∞ÔºåÂÖ±‰∫´ÂíåÂÖ®Â±ÄÂÜÖÂ≠ò„ÄÇ A program manages the global memory space visible to kernels through calls to the CUDA runtime, such as cudaMalloc() and cudaFree(). Kernels may execute on a physically separate device, as is the case when running kernels on the GPU. Consequently, the application must use cudaMemcpy() to copy data between the allocated space and the host system memory.Á®ãÂ∫èÈÄöËøáË∞ÉÁî®CUDAËøêË°åÊó∂Ôºà‰æãÂ¶ÇcudaMallocÔºàÔºâÂíåcudaFreeÔºàÔºâÔºâÊù•ÁÆ°ÁêÜÂÜÖÊ†∏ÂèØËßÅÁöÑÂÖ®Â±ÄÂÜÖÂ≠òÁ©∫Èó¥„ÄÇ ÂÜÖÊ†∏ÂèØ‰ª•Âú®Áâ©ÁêÜ‰∏äÁã¨Á´ãÁöÑËÆæÂ§á‰∏äÊâßË°åÔºåÂ∞±ÂÉèÂú®GPU‰∏äËøêË°åÂÜÖÊ†∏‰∏ÄÊ†∑„ÄÇ Âõ†Ê≠§ÔºåÂ∫îÁî®Á®ãÂ∫èÂøÖÈ°ª‰ΩøÁî®cudaMemcpyÔºàÔºâÂú®ÂàÜÈÖçÁöÑÁ©∫Èó¥Âíå‰∏ªÊú∫Á≥ªÁªüÂÜÖÂ≠ò‰πãÈó¥Â§çÂà∂Êï∞ÊçÆ„ÄÇ The CUDA programming model is similar in style to the familiar singleprogram multiple data (SPMD) model‚Äîit expresses parallelism explicitly, and each kernel executes on a fxed number of threads. However, CUDA is more Ô¨Çexible than most realizations of SPMD, because each kernel call dynamically creates a new grid with the right number of thread blocks and threads for that application step. The programmer can use a convenient degree of parallelism for each kernel, rather than having to design all phases of the computation to use the same numberof threads. Figure C.3.6 shows an example of an SPMD-like CUDA code sequence. It first instantiates kernelF on a 2D grid of 3 x 2 blocks where each 2D thread block consists of 5 x 3 threads. It then instantiates kernelG on a 1D grid of four 1D thread blocks with six threads each. Because kernelG depends on the results of kernelF, they are separated by an interkernel synchronization barrier.CUDAÁºñÁ®ãÊ®°ÂûãÁöÑÈ£éÊ†ºÁ±ª‰ºº‰∫éÁÜüÊÇâÁöÑÂçïÁ®ãÂ∫èÂ§öÊï∞ÊçÆÔºàSPMDÔºâÊ®°Âûã - ÂÆÉÊòéÁ°ÆÂú∞Ë°®ËææÂπ∂Ë°åÊÄßÔºåÂπ∂‰∏îÊØè‰∏™ÂÜÖÊ†∏Âú®Âõ∫ÂÆöÊï∞ÈáèÁöÑÁ∫øÁ®ã‰∏äÊâßË°å„ÄÇ ‰ΩÜÊòØÔºåCUDAÊØîSPMDÁöÑÂ§ßÂ§öÊï∞ÂÆûÁé∞Êõ¥ÁÅµÊ¥ªÔºåÂõ†‰∏∫ÊØè‰∏™ÂÜÖÊ†∏Ë∞ÉÁî®Âä®ÊÄÅÂú∞‰∏∫ËØ•Â∫îÁî®Á®ãÂ∫èÊ≠•È™§ÂàõÂª∫ÂÖ∑ÊúâÊ≠£Á°ÆÊï∞ÈáèÁöÑÁ∫øÁ®ãÂùóÂíåÁ∫øÁ®ãÁöÑÊñ∞ÁΩëÊ†º„ÄÇ Á®ãÂ∫èÂëòÂèØ‰ª•‰∏∫ÊØè‰∏™ÂÜÖÊ†∏‰ΩøÁî®Êñπ‰æøÁöÑÂπ∂Ë°åÂ∫¶ÔºåËÄå‰∏çÂøÖËÆæËÆ°ËÆ°ÁÆóÁöÑÊâÄÊúâÈò∂ÊÆµ‰ª•‰ΩøÁî®Áõ∏ÂêåÊï∞ÈáèÁöÑÁ∫øÁ®ã„ÄÇ ÂõæC.3.6ÊòæÁ§∫‰∫ÜÁ±ª‰ººSPMDÁöÑCUDA‰ª£Á†ÅÂ∫èÂàóÁöÑÁ§∫‰æã„ÄÇ ÂÆÉÈ¶ñÂÖàÂú®3 x 2ÂùóÁöÑ2DÁΩëÊ†º‰∏äÂÆû‰æãÂåñkernelFÔºåÂÖ∂‰∏≠ÊØè‰∏™2DÁ∫øÁ®ãÂùóÁî±5 x 3‰∏™Á∫øÁ®ãÁªÑÊàê„ÄÇ ÁÑ∂ÂêéÔºåÂÆÉÂú®Âõõ‰∏™1DÁ∫øÁ®ãÂùóÁöÑ1DÁΩëÊ†º‰∏äÂÆû‰æãÂåñkernelGÔºåÊØè‰∏™Á∫øÁ®ãÂùóÊúâ6‰∏™Á∫øÁ®ã„ÄÇ Âõ†‰∏∫kernelG‰æùËµñ‰∫ékernelFÁöÑÁªìÊûúÔºåÊâÄ‰ª•ÂÆÉ‰ª¨Ë¢´ÂÜÖÊ†∏ÂêåÊ≠•ÈöúÁ¢çÈöîÂºÄ„ÄÇ single-program multiple data (SPMD): A style of parallel programming model in which all threads execute the same program. SPMD threads typically coordinate with barrier synchronization.‰∏ÄÁßçÂπ∂Ë°åÁºñÁ®ãÊ®°ÂûãÔºåÂÖ∂‰∏≠ÊâÄÊúâÁ∫øÁ®ãÈÉΩÊâßË°åÁõ∏ÂêåÁöÑÁ®ãÂ∫è„ÄÇ SPMDÁ∫øÁ®ãÈÄöÂ∏∏‰∏éÂ±èÈöúÂêåÊ≠•ÂçèË∞É„ÄÇ The concurrent threads of a thread block express fne-grained data parallelism and thread parallelism. The independent thread blocks of a grid express coarse-grained data parallelism. Independent grids express coarse-grained task parallelism. A kernel is simply C code for one thread of the hierarchy.Á∫øÁ®ãÂùóÁöÑÂπ∂ÂèëÁ∫øÁ®ãË°®Á§∫ÁªÜÁ≤íÂ∫¶Êï∞ÊçÆÂπ∂Ë°åÊÄßÂíåÁ∫øÁ®ãÂπ∂Ë°åÊÄß„ÄÇ ÁΩëÊ†ºÁöÑÁã¨Á´ãÁ∫øÁ®ãÂùóË°®Á§∫Á≤óÁ≤íÂ∫¶Êï∞ÊçÆÂπ∂Ë°åÊÄß„ÄÇ Áã¨Á´ãÁΩëÊ†ºË°®Á§∫Á≤óÁ≤íÂ∫¶ÁöÑ‰ªªÂä°Âπ∂Ë°åÊÄß„ÄÇ ÂÜÖÊ†∏Âè™ÊòØÂ±ÇÊ¨°ÁªìÊûÑ‰∏≠‰∏Ä‰∏™Á∫øÁ®ãÁöÑC‰ª£Á†Å RestrictionsFor efficiency, and to simplify its implementation, the CUDA programming model has some restrictions. Threads and thread blocks may only be created by invoking a parallel kernel, not from within a parallel kernel. Together with the required independence of thread blocks, this makes it possible to execute CUDA programs with a simple scheduler that introduces minimal runtime overhead. In fact, the Tesla GPU architecture implements hardware management and scheduling of threads and thread blocks.‰∏∫‰∫ÜÊèêÈ´òÊïàÁéáÂπ∂ÁÆÄÂåñÂÖ∂ÂÆûÁé∞ÔºåCUDAÁºñÁ®ãÊ®°ÂûãÊúâ‰∏Ä‰∫õÈôêÂà∂„ÄÇÁ∫øÁ®ãÂíåÁ∫øÁ®ãÂùóÂè™ËÉΩÈÄöËøáË∞ÉÁî®Âπ∂Ë°åÂÜÖÊ†∏Êù•ÂàõÂª∫ÔºåËÄå‰∏çËÉΩÈÄöËøáÂπ∂Ë°åÂÜÖÊ†∏Êù•ÂàõÂª∫„ÄÇ‰∏éÁ∫øÁ®ãÂùóÊâÄÈúÄÁöÑÁã¨Á´ãÊÄß‰∏ÄËµ∑ÔºåËøô‰ΩøÂæó‰ΩøÁî®ÁÆÄÂçïÁöÑË∞ÉÂ∫¶Á®ãÂ∫èÊâßË°åCUDAÁ®ãÂ∫èÊàê‰∏∫ÂèØËÉΩÔºåËØ•Ë∞ÉÂ∫¶Á®ãÂ∫èÂºïÂÖ•‰∫ÜÊúÄÂ∞èÁöÑËøêË°åÊó∂ÂºÄÈîÄ„ÄÇÂÆûÈôÖ‰∏äÔºåTesla GPUÊû∂ÊûÑÂÆûÁé∞‰∫ÜÁ∫øÁ®ãÂíåÁ∫øÁ®ãÂùóÁöÑÁ°¨‰ª∂ÁÆ°ÁêÜÂíåË∞ÉÂ∫¶„ÄÇ Task parallelism can be expressed at the thread block level but is difficult to express within a thread block because thread synchronization barriers operate on all the threads of the block. To enable CUDA programs to run on any number of processors, dependencies among thread blocks within the same kernel grid are not allowed‚Äîblocks must execute independently. Since CUDA requires that thread blocks be independent and allows blocks to be executed in any order, combining results generated by multiple blocks must in general be done by launching a second kernel on a new grid of thread blocks (although thread blocks may coordinate their activities using atomic memory operations on the global memory visible to all threads‚Äîby atomically incrementing queue pointers, for example).‰ªªÂä°Âπ∂Ë°åÊÄßÂèØ‰ª•Âú®Á∫øÁ®ãÂùóÁ∫ßÂà´Ë°®Á§∫Ôºå‰ΩÜÈöæ‰ª•Âú®Á∫øÁ®ãÂùóÂÜÖË°®ËææÔºåÂõ†‰∏∫Á∫øÁ®ãÂêåÊ≠•ÈöúÁ¢çÂú®ÂùóÁöÑÊâÄÊúâÁ∫øÁ®ã‰∏äËøêË°å„ÄÇË¶Å‰ΩøCUDAÁ®ãÂ∫èËÉΩÂ§üÂú®‰ªªÊÑèÊï∞ÈáèÁöÑÂ§ÑÁêÜÂô®‰∏äËøêË°åÔºå‰∏çÂÖÅËÆ∏Âêå‰∏ÄÂÜÖÊ†∏ÁΩëÊ†º‰∏≠ÁöÑÁ∫øÁ®ãÂùó‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª - ÂùóÂøÖÈ°ªÁã¨Á´ãÊâßË°å„ÄÇÁî±‰∫éCUDAË¶ÅÊ±ÇÁ∫øÁ®ãÂùóÊòØÁã¨Á´ãÁöÑÂπ∂‰∏îÂÖÅËÆ∏‰ª•‰ªª‰ΩïÈ°∫Â∫èÊâßË°åÂùóÔºåÂõ†Ê≠§ÈÄöÂ∏∏ÂøÖÈ°ªÈÄöËøáÂú®Êñ∞ÁöÑÁ∫øÁ®ãÂùóÁΩëÊ†º‰∏äÂêØÂä®Á¨¨‰∫å‰∏™ÂÜÖÊ†∏Êù•ÁªÑÂêàÁî±Â§ö‰∏™ÂùóÁîüÊàêÁöÑÁªìÊûúÔºàÂ∞ΩÁÆ°Á∫øÁ®ãÂùóÂèØ‰ª•‰ΩøÁî®ÂÆÉÊù•ÂçèË∞ÉÂÆÉ‰ª¨ÁöÑÊ¥ªÂä®ÔºâÊâÄÊúâÁ∫øÁ®ãÈÉΩÂèØËßÅÁöÑÂÖ®Â±ÄÂÜÖÂ≠ò‰∏äÁöÑÂéüÂ≠êÂÜÖÂ≠òÊìç‰Ωú - ‰æãÂ¶ÇÔºåÈÄöËøáÂéüÂ≠êÈÄíÂ¢ûÈòüÂàóÊåáÈíàÔºâ„ÄÇ Recursive function calls are not currently allowed in CUDA kernels. Recursion is unattractive in a massively parallel kernel, because providing stack space for the tens of thousands of threads that may be active would require substantial amounts of memory. Serial algorithms that are normally expressed using recursion, such asquicksort, are typically best implemented using nested data parallelism rather than explicit recursion.CUDAÂÜÖÊ†∏ÂΩìÂâç‰∏çÂÖÅËÆ∏ÈÄíÂΩíÂáΩÊï∞Ë∞ÉÁî®„ÄÇ ÈÄíÂΩíÂú®Â§ßËßÑÊ®°Âπ∂Ë°åÂÜÖÊ†∏‰∏≠Ê≤°ÊúâÂê∏ÂºïÂäõÔºåÂõ†‰∏∫‰∏∫ÂèØËÉΩÊ¥ªË∑ÉÁöÑÊï∞‰∏á‰∏™Á∫øÁ®ãÊèê‰æõÂ†ÜÊ†àÁ©∫Èó¥Â∞ÜÈúÄË¶ÅÂ§ßÈáèÁöÑÂÜÖÂ≠ò„ÄÇ ÈÄöÂ∏∏‰ΩøÁî®ÈÄíÂΩíË°®Á§∫ÁöÑ‰∏≤Ë°åÁÆóÊ≥ïÔºà‰æãÂ¶ÇÂø´ÈÄüÊéíÂ∫èÔºâÈÄöÂ∏∏ÊúÄÂ•Ω‰ΩøÁî®ÂµåÂ•óÊï∞ÊçÆÂπ∂Ë°åËÄå‰∏çÊòØÊòæÂºèÈÄíÂΩíÊù•ÂÆûÁé∞„ÄÇ To support a heterogeneous system architecture combining a CPU and a GPU, each with its own memory system, CUDA programs must copy data and results between host memory and device memory. The overhead of CPU‚ÄìGPU interaction and data transfers is minimized by using DMA block transfer engines and fast interconnects. Compute-intensive problems large enough to need a GPU performance boost amortize the overhead better than small problems.‰∏∫‰∫ÜÊîØÊåÅÁªÑÂêàCPUÂíåGPUÁöÑÂºÇÊûÑÁ≥ªÁªüÊû∂ÊûÑÔºåÊØè‰∏™Êû∂ÊûÑÈÉΩÊúâËá™Â∑±ÁöÑÂÜÖÂ≠òÁ≥ªÁªüÔºåCUDAÁ®ãÂ∫èÂøÖÈ°ªÂú®‰∏ªÊú∫ÂÜÖÂ≠òÂíåËÆæÂ§áÂÜÖÂ≠ò‰πãÈó¥Â§çÂà∂Êï∞ÊçÆÂíåÁªìÊûú„ÄÇ ÈÄöËøá‰ΩøÁî®DMAÂùó‰º†ËæìÂºïÊìéÂíåÂø´ÈÄü‰∫íËøûÔºåÂèØ‰ª•ÊúÄÂ§ßÈôêÂ∫¶Âú∞ÂáèÂ∞ëCPU-GPU‰∫§‰∫íÂíåÊï∞ÊçÆ‰º†ËæìÁöÑÂºÄÈîÄ„ÄÇ Â§ßÂà∞Ë∂≥‰ª•ÈúÄË¶ÅGPUÊÄßËÉΩÊèêÂçáÁöÑËÆ°ÁÆóÂØÜÈõÜÂûãÈóÆÈ¢òÂèØ‰ª•ÊØîÂ∞èÈóÆÈ¢òÊõ¥Â•ΩÂú∞ÂàÜÊëäÂºÄÈîÄ„ÄÇ Implications for Architecture ÂØπÂª∫Á≠ëÁöÑÂêØÁ§∫The parallel programming models for graphics and computing have driven GPU architecture to be different than CPU architecture. The key aspects of GPU programs driving GPU processor architecture are:Áî®‰∫éÂõæÂΩ¢ÂíåËÆ°ÁÆóÁöÑÂπ∂Ë°åÁºñÁ®ãÊ®°ÂûãÈ©±Âä®GPUÊû∂ÊûÑ‰∏éCPUÊû∂ÊûÑ‰∏çÂêå„ÄÇ GPUÁ®ãÂ∫èÈ©±Âä®GPUÂ§ÑÁêÜÂô®Êû∂ÊûÑÁöÑÂÖ≥ÈîÆÊñπÈù¢ÊòØÔºö Extensive use of fne-grained data parallelism: Shader programs describe how to process a single pixel or vertex, and CUDA programs describe how to compute an individual result.ÂπøÊ≥õ‰ΩøÁî®ÁªÜÁ≤íÂ∫¶Êï∞ÊçÆÂπ∂Ë°åÔºöÁùÄËâ≤Âô®Á®ãÂ∫èÊèèËø∞Â¶Ç‰ΩïÂ§ÑÁêÜÂçï‰∏™ÂÉèÁ¥†ÊàñÈ°∂ÁÇπÔºåËÄåCUDAÁ®ãÂ∫èÊèèËø∞Â¶Ç‰ΩïËÆ°ÁÆóÂçï‰∏™ÁªìÊûú„ÄÇ Highly threaded programming model: A shader thread program processes a single pixel or vertex, and a CUDA thread program may generate a single result. A GPU must create and execute millions of such thread programs per frame, at 60 frames per second. È´òÁ∫øÁ®ãÁºñÁ®ãÊ®°ÂûãÔºöÁùÄËâ≤Âô®Á∫øÁ®ãÁ®ãÂ∫èÂ§ÑÁêÜÂçï‰∏™ÂÉèÁ¥†ÊàñÈ°∂ÁÇπÔºåCUDAÁ∫øÁ®ãÁ®ãÂ∫èÂèØ‰ª•ÁîüÊàêÂçï‰∏™ÁªìÊûú„ÄÇ GPUÂøÖÈ°ªÊØèÂ∏ßÂàõÂª∫Âπ∂ÊâßË°åÊï∞Áôæ‰∏á‰∏™ËøôÊ†∑ÁöÑÁ∫øÁ®ãÁ®ãÂ∫èÔºåÊØèÁßí60Â∏ß„ÄÇ Scalability: A program must automatically increase its performance when provided with additional processors, without recompiling. Á®ãÂ∫èÂøÖÈ°ªÂú®Êèê‰æõÈ¢ùÂ§ñÁöÑÂ§ÑÁêÜÂô®Êó∂Ëá™Âä®ÊèêÈ´òÂÖ∂ÊÄßËÉΩÔºåËÄåÊó†ÈúÄÈáçÊñ∞ÁºñËØë„ÄÇ Intensive Ô¨Çoating-point (or integer) computation.Âº∫ÂåñÊµÆÁÇπÔºàÊàñÊï¥Êï∞ÔºâËÆ°ÁÆó„ÄÇ Support of high throughput computations.ÊîØÊåÅÈ´òÂêûÂêêÈáèËÆ°ÁÆó„ÄÇ]]></content>
      <categories>
        <category>GPUÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÁøªËØë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2_GPU_System_Architectures]]></title>
    <url>%2F2019%2F01%2F23%2FGPU_System_Architectures%2F</url>
    <content type="text"><![CDATA[In this section, we survey GPU system architectures in common use today. We discuss system configurations, GPU functions and services, standard programming interfaces, and a basic GPU internal architecture.Âú®Êú¨ËäÇ‰∏≠ÔºåÊàë‰ª¨Â∞ÜË∞ÉÊü•ÁõÆÂâçÂ∏∏Áî®ÁöÑGPUÁ≥ªÁªüÊû∂ÊûÑ„ÄÇ Êàë‰ª¨Â∞ÜËÆ®ËÆ∫Á≥ªÁªüÈÖçÁΩÆÔºåGPUÂäüËÉΩÂíåÊúçÂä°ÔºåÊ†áÂáÜÁºñÁ®ãÊé•Âè£‰ª•ÂèäÂü∫Êú¨ÁöÑGPUÂÜÖÈÉ®Êû∂ÊûÑ„ÄÇ Heterogeneous CPU‚ÄìGPU System Architecture ÂºÇÊûÑCPU-GPUÁ≥ªÁªüÊû∂ÊûÑA heterogeneous computer system architecture using a GPU and a CPU can be described at a high level by two primary characteristics: first, how many functional subsystems and/or chips are used and what are their interconnection technologies and topology; and second, what memory subsystems are available to these functional subsystems. See Chapter 6 for background on the PC I/O systems and chip sets.‰ΩøÁî®GPUÂíåCPUÁöÑÂºÇÊûÑËÆ°ÁÆóÊú∫Á≥ªÁªüÊû∂ÊûÑÂèØ‰ª•ÈÄöËøá‰∏§‰∏™‰∏ªË¶ÅÁâπÂæÅÂú®È´òÁ∫ßÂà´ÊèèËø∞ÔºöÁ¨¨‰∏ÄÔºå‰ΩøÁî®Â§öÂ∞ëÂäüËÉΩÂ≠êÁ≥ªÁªüÂíå/ÊàñËäØÁâá‰ª•ÂèäÂÆÉ‰ª¨ÁöÑ‰∫íËøûÊäÄÊúØÂíåÊãìÊâëÊòØ‰ªÄ‰πà; Á¨¨‰∫åÔºåËøô‰∫õÂäüËÉΩÂ≠êÁ≥ªÁªüÂèØ‰ª•‰ΩøÁî®Âì™‰∫õÂÜÖÂ≠òÂ≠êÁ≥ªÁªü„ÄÇ ÊúâÂÖ≥PC I / OÁ≥ªÁªüÂíåËäØÁâáÁªÑÁöÑËÉåÊôØ‰ø°ÊÅØÔºåËØ∑ÂèÇËßÅÁ¨¨6Á´†„ÄÇ The Historical PC(circa 1990) ÂéÜÂè≤PCÔºàÂ§ßÁ∫¶1990Âπ¥ÔºâFigure C.2.1 shows a high-level block diagram of a legacy PC, circa 1990. The north bridge (see Chapter 6) contains high-bandwidth interfaces, connecting the CPU, memory, and PCI bus. The south bridge contains legacy interfaces and devices: ISA bus (audio, LAN), interrupt controller; DMA controller; time/counter. In this system, the display was driven by a simple framebuffer subsystem known as a VGA (video graphics array) which was attached to the PCI bus. Graphics subsystems with built-in processing elements (GPUs) did not exist in the PC landscape of 1990.ÂõæC.2.1ÊòæÁ§∫‰∫ÜÂ§ßÁ∫¶ÊòØ1990Âπ¥„ÄÅÁöÑ‰º†ÁªüPCÁöÑÈ´òÁ∫ßÊ°ÜÂõæ„ÄÇÂåóÊ°•ÔºàËßÅÁ¨¨6Á´†ÔºâÂåÖÂê´È´òÂ∏¶ÂÆΩÊé•Âè£ÔºåËøûÊé•CPUÔºåÂÜÖÂ≠òÂíåPCIÊÄªÁ∫ø„ÄÇ ÂçóÊ°•ÂåÖÂê´‰º†ÁªüÊé•Âè£ÂíåËÆæÂ§áÔºöISAÊÄªÁ∫øÔºàÈü≥È¢ëÔºåLANÔºâÔºå‰∏≠Êñ≠ÊéßÂà∂Âô®; DMAÊéßÂà∂Âô®;ÂÆöÊó∂/ËÆ°Êï∞Âô®„ÄÇ Âú®ËØ•Á≥ªÁªü‰∏≠ÔºåÊòæÁ§∫Âô®Áî±Áß∞‰∏∫VGAÔºàËßÜÈ¢ëÂõæÂΩ¢ÈòµÂàóÔºâÁöÑÁÆÄÂçïÂ∏ßÁºìÂÜ≤Â≠êÁ≥ªÁªüÈ©±Âä®ÔºåËØ•Â≠êÁ≥ªÁªüËøûÊé•Âà∞PCIÊÄªÁ∫ø„ÄÇ 1990Âπ¥ÁöÑPCÁéØÂ¢É‰∏≠‰∏çÂ≠òÂú®ÂÖ∑ÊúâÂÜÖÁΩÆÂ§ÑÁêÜÂÖÉ‰ª∂ÔºàGPUÔºâÁöÑÂõæÂΩ¢Â≠êÁ≥ªÁªü„ÄÇ Figure C.2.2 illustrates two configurations in common use today. These are characterized by a separate GPU (discrete GPU) and CPU with respective memory subsystems. In Figure C.2.2a, with an Intel CPU, we see the GPU attached via a 16-lane PCI-Express 2.0 link to provide a peak 16 GB/s transfer rate, (peak of 8 GB/s in each direction). Similarly, in Figure C.2.2b, with an AMD CPU, the GPU is attached to the chipset, also via PCI-Express with the same available bandwidth. In both cases, the GPUs and CPUs may access each other‚Äôs memory, albeit with less available bandwidth than their access to the more directly attached memories. In the case of the AMD system, the north bridge or memory controller is integrated into the same die as the CPU.ÂõæC.2.2ËØ¥Êòé‰∫ÜÁõÆÂâçÂ∏∏Áî®ÁöÑ‰∏§ÁßçÈÖçÁΩÆ„ÄÇ ÂÆÉ‰ª¨ÁöÑÁâπÂæÅÂú®‰∫éÂçïÁã¨ÁöÑGPUÔºàÁ¶ªÊï£GPUÔºâÂíåÂÖ∑ÊúâÁõ∏Â∫îÂ≠òÂÇ®Âô®Â≠êÁ≥ªÁªüÁöÑCPU„ÄÇ Âú®ÂõæC.2.2a‰∏≠Ôºå‰ΩøÁî®Intel CPUÔºåÊàë‰ª¨ÁúãÂà∞GPUÈÄöËøá16ÈÄöÈÅìPCI-Express 2.0ÈìæË∑ØËøûÊé•Ôºå‰ª•Êèê‰æõ16 GB / sÁöÑÂ≥∞ÂÄº‰º†ËæìÈÄüÁéáÔºàÊØè‰∏™ÊñπÂêëÁöÑÂ≥∞ÂÄº‰∏∫8 GB / sÔºâ„ÄÇÂêåÊ†∑ÔºåÂú®ÂõæC.2.2b‰∏≠Ôºå‰ΩøÁî®AMD CPUÔºåGPU‰πüÈÄöËøáÂÖ∑ÊúâÁõ∏ÂêåÂèØÁî®Â∏¶ÂÆΩÁöÑPCI-ExpressËøûÊé•Âà∞ËäØÁâáÁªÑ„ÄÇ Âú®Ëøô‰∏§ÁßçÊÉÖÂÜµ‰∏ãÔºåGPUÂíåCPUÂèØ‰ª•ËÆøÈóÆÂΩºÊ≠§ÁöÑÂÜÖÂ≠òÔºåÂ∞ΩÁÆ°ÂèØÁî®Â∏¶ÂÆΩÂ∞ë‰∫éËÆøÈóÆÊõ¥Áõ¥Êé•ËøûÊé•ÁöÑÂÜÖÂ≠ò„ÄÇ Âú®AMDÁ≥ªÁªüÁöÑÊÉÖÂÜµ‰∏ãÔºåÂåóÊ°•ÊàñÂ≠òÂÇ®Âô®ÊéßÂà∂Âô®ÈõÜÊàêÂà∞‰∏éCPUÁõ∏ÂêåÁöÑÁÆ°ËäØ‰∏≠„ÄÇ A low-cost variation on these systems, a unifed memory architecture (UMA) system, uses only CPU system memory, omitting GPU memory from the system. These systems have relatively low performance GPUs, since their achieved performance is limited by the available system memory bandwidth and increased latency of memory access, whereas dedicated GPU memory provides high bandwidth and low latency.Ëøô‰∫õÁ≥ªÁªüÁöÑ‰ΩéÊàêÊú¨ÂèòÂåñÔºåÁªü‰∏ÄÂÜÖÂ≠òÊû∂ÊûÑÔºàUMAÔºâÁ≥ªÁªüÔºå‰ªÖ‰ΩøÁî®CPUÁ≥ªÁªüÂÜÖÂ≠òÔºå‰ªéÁ≥ªÁªü‰∏≠ÁúÅÁï•GPUÂÜÖÂ≠ò„ÄÇ Ëøô‰∫õÁ≥ªÁªüÂÖ∑ÊúâÁõ∏ÂØπ‰ΩéÊÄßËÉΩÁöÑGPUÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÂÆûÁé∞ÁöÑÊÄßËÉΩÂèóÂèØÁî®Á≥ªÁªüÂ≠òÂÇ®Âô®Â∏¶ÂÆΩÂíåÂ≠òÂÇ®Âô®ËÆøÈóÆÂª∂ËøüÁöÑÈôêÂà∂ÔºåËÄå‰∏ìÁî®GPUÂ≠òÂÇ®Âô®Êèê‰æõÈ´òÂ∏¶ÂÆΩÂíå‰ΩéÂª∂Ëøü„ÄÇA high performance system variation uses multiple attached GPUs, typically two to four working in parallel, with their displays daisy-chained. An example is the NVIDIA SLI (scalable link interconnect) multi-GPU system, designed for high performance gaming and workstations.È´òÊÄßËÉΩÁ≥ªÁªüÂèò‰Ωì‰ΩøÁî®Â§ö‰∏™ËøûÊé•ÁöÑGPUÔºåÈÄöÂ∏∏‰∏§Âà∞Âõõ‰∏™Âπ∂Ë°åÂ∑•‰ΩúÔºåÂÖ∂ÊòæÁ§∫Âô®ÈááÁî®ËèäËä±ÈìæÂºèËøûÊé•„ÄÇ‰∏Ä‰∏™‰æãÂ≠êÊòØNVIDIA SLIÔºàÂèØÊâ©Â±ïÈìæË∑Ø‰∫íËøûÔºâÂ§öGPUÁ≥ªÁªüÔºå‰∏ì‰∏∫È´òÊÄßËÉΩÊ∏∏ÊàèÂíåÂ∑•‰ΩúÁ´ôËÄåËÆæËÆ°„ÄÇThe next system category integrates the GPU with the north bridge (Intel) or chipset (AMD) with and without dedicated graphics memory.‰∏ã‰∏Ä‰∏™Á≥ªÁªüÁ±ªÂà´Â∞ÜGPU‰∏éÂåóÊ°•ÔºàIntelÔºâÊàñËäØÁâáÁªÑÔºàAMDÔºâÈõÜÊàêÔºåÊúâÊàñÊ≤°Êúâ‰∏ìÁî®ÁöÑÂõæÂΩ¢ÂÜÖÂ≠òChapter 5 explains how caches maintain coherence in a shared address space. With CPUs and GPUs, there are multiple address spaces. GPUs can access their own physical local memory and the CPU system‚Äôs physical memory using virtual addresses that are translated by an MMU on the GPU. The operating system kernel manages the GPU‚Äôs page tables. A system physical page can be accessed using either coherent or noncoherent PCI-Express transactions, determined by an attribute in the GPU‚Äôs page table. The CPU can access GPU‚Äôs local memory through an address range (also called aperture) in the PCI-Express address space.Á¨¨5Á´†Ëß£Èáä‰∫ÜÁºìÂ≠òÂ¶Ç‰ΩïÂú®ÂÖ±‰∫´Âú∞ÂùÄÁ©∫Èó¥‰∏≠‰øùÊåÅ‰∏ÄËá¥ÊÄß„ÄÇÂØπ‰∫éCPUÂíåGPUÔºåÊúâÂ§ö‰∏™Âú∞ÂùÄÁ©∫Èó¥„ÄÇ GPUÂèØ‰ª•‰ΩøÁî®Áî±GPU‰∏äÁöÑMMUËΩ¨Êç¢ÁöÑËôöÊãüÂú∞ÂùÄËÆøÈóÆËá™Â∑±ÁöÑÁâ©ÁêÜÊú¨Âú∞ÂÜÖÂ≠òÂíåCPUÁ≥ªÁªüÁöÑÁâ©ÁêÜÂÜÖÂ≠ò„ÄÇÊìç‰ΩúÁ≥ªÁªüÂÜÖÊ†∏ÁÆ°ÁêÜGPUÁöÑÈ°µË°®„ÄÇÂèØ‰ª•‰ΩøÁî®Áî±GPUÈ°µÈù¢Ë°®‰∏≠ÁöÑÂ±ûÊÄßÁ°ÆÂÆöÁöÑÁõ∏Âπ≤ÊàñÈùûÁõ∏Âπ≤PCI-Express‰∫ãÂä°Êù•ËÆøÈóÆÁ≥ªÁªüÁâ©ÁêÜÈ°µÈù¢„ÄÇ CPUÂèØ‰ª•ÈÄöËøáPCI-ExpressÂú∞ÂùÄÁ©∫Èó¥‰∏≠ÁöÑÂú∞ÂùÄËåÉÂõ¥Ôºà‰πüÁß∞‰∏∫Â≠îÂæÑÔºâËÆøÈóÆGPUÁöÑÊú¨Âú∞Â≠òÂÇ®Âô®„ÄÇ Game Consoles Ê∏∏Êàè‰∏ªÊú∫Console systems such as the Sony PlayStation 3 and the Microsof Xbox 360 resemble the PC system architectures previously described. Console systems are designed to be shipped with identical performance and functionality over a lifespan that can last five years or more. During this time, a system may be reimplemented many times to exploit more advanced silicon manufacturing processes and thereby to provide constant capability at ever lower costs. Console systems do not need to have their subsystems expanded and upgraded the way PC systems do, so themajor internal system buses tend to be customized rather than standardized.ËØ∏Â¶ÇSony PlayStation 3ÂíåMicrosof Xbox 360‰πãÁ±ªÁöÑÊéßÂà∂Âè∞Á≥ªÁªüÁ±ª‰ºº‰∫éÂÖàÂâçÊèèËø∞ÁöÑPCÁ≥ªÁªüÊû∂ÊûÑ„ÄÇ ÊéßÂà∂Âè∞Á≥ªÁªüËÆæËÆ°‰∏∫Âú®Áõ∏ÂêåÁöÑ‰ΩøÁî®ÂØøÂëΩÊúüÂÜÖÂÖ∑ÊúâÁõ∏ÂêåÁöÑÊÄßËÉΩÂíåÂäüËÉΩÔºåÂèØ‰ª•‰ΩøÁî®‰∫îÂπ¥ÊàñÊõ¥ÈïøÊó∂Èó¥„ÄÇ Âú®Ê≠§ÊúüÈó¥ÔºåÁ≥ªÁªüÂèØ‰ª•Â§öÊ¨°ÈáçÊñ∞ÂÆûÁé∞‰ª•Âà©Áî®Êõ¥ÂÖàËøõÁöÑÁ°ÖÂà∂ÈÄ†Â∑•Ëâ∫Ôºå‰ªéËÄå‰ª•Êõ¥‰ΩéÁöÑÊàêÊú¨Êèê‰æõÊÅíÂÆöÁöÑËÉΩÂäõ„ÄÇ ÊéßÂà∂Âè∞Á≥ªÁªü‰∏çÈúÄË¶ÅÂÉèPCÁ≥ªÁªüÈÇ£Ê†∑Êâ©Â±ïÂíåÂçáÁ∫ßÂ≠êÁ≥ªÁªüÔºåÊâÄ‰ª•‰∏ªË¶ÅÁöÑÂÜÖÈÉ®Á≥ªÁªüÊÄªÁ∫øÂæÄÂæÄÊòØÂÆöÂà∂ÁöÑËÄå‰∏çÊòØÊ†áÂáÜÂåñÁöÑ„ÄÇ GPU Interfaces and DriversIn a PC today, GPUs are attached to a CPU via PCI-Express. Earlier generations used AGP. Graphics applications call OpenGL [Segal and Akeley, 2006] or Direct3D [Microsof DirectX Specifcation] API functions that use the GPU as a coprocessor. The APIs send commands, programs, and data to the GPU via a graphics device driver optimized for the particular GPU.Âú®‰ªäÂ§©ÁöÑPC‰∏≠ÔºåGPUÈÄöËøáPCI-ExpressËøûÊé•Âà∞CPU„ÄÇ Êó©ÊúüÁöÑÂá†‰ª£‰∫∫‰ΩøÁî®AGP„ÄÇ ÂõæÂΩ¢Â∫îÁî®Á®ãÂ∫èË∞ÉÁî®OpenGL [SegalÂíåAkeleyÔºå2006]ÊàñDirect3D [Microsof DirectXËßÑËåÉ] APIÂáΩÊï∞ÔºåËøô‰∫õÂáΩÊï∞‰ΩøÁî®GPU‰Ωú‰∏∫ÂçèÂ§ÑÁêÜÂô®„ÄÇ APIÈÄöËøáÈíàÂØπÁâπÂÆöGPU‰ºòÂåñÁöÑÂõæÂΩ¢ËÆæÂ§áÈ©±Âä®Á®ãÂ∫èÂ∞ÜÂëΩ‰ª§ÔºåÁ®ãÂ∫èÂíåÊï∞ÊçÆÂèëÈÄÅÂà∞GPU„ÄÇ APG :An extended version of the original PCI I/O bus, which provided up to eight times the bandwidth of the original PCI bus to a single card slot. Its primary purpose was to connect graphics subsystems into PC systems.APG: ÂéüÂßãPCI I / OÊÄªÁ∫øÁöÑÊâ©Â±ïÁâàÊú¨ÔºåÂÖ∂Êèê‰æõÁöÑÂ∏¶ÂÆΩÊòØÂéüÂßãPCIÊÄªÁ∫øÁöÑÂÖ´ÂÄçÔºåËææÂà∞Âçï‰∏™Âç°ÊèíÊßΩ„ÄÇ ÂÖ∂‰∏ªË¶ÅÁõÆÁöÑÊòØÂ∞ÜÂõæÂΩ¢Â≠êÁ≥ªÁªüËøûÊé•Âà∞PCÁ≥ªÁªü„ÄÇ Graphics Logical PipelineThe graphics logical pipeline is described in Section C.3. Figure C.2.3 illustrates the major processing stages, and highlights the important programmable stages (vertex, geometry, and pixel shader stages).ÂõæÂΩ¢ÈÄªËæëÊµÅÊ∞¥Á∫øÂú®C.3ËäÇ‰∏≠ÊèèËø∞„ÄÇ ÂõæC.2.3ËØ¥Êòé‰∫Ü‰∏ªË¶ÅÁöÑÂ§ÑÁêÜÈò∂ÊÆµÔºåÂπ∂ÈáçÁÇπ‰ªãÁªç‰∫ÜÈáçË¶ÅÁöÑÂèØÁºñÁ®ãÈò∂ÊÆµÔºàÈ°∂ÁÇπÔºåÂá†‰ΩïÂíåÂÉèÁ¥†ÁùÄËâ≤Âô®Èò∂ÊÆµÔºâ„ÄÇ Mapping Graphics Pipeline to Unified GPU ProcessorsFigure C.2.4 shows how the logical pipeline comprising separate independent programmable stages is mapped onto a physical distributed array of processors. Basic Unifed GPU ArchitectureUnifed GPU architectures are based on a parallel array of many programmable processors. They unify vertex, geometry, and pixel shader processing and parallel computing on the same processors, unlike earlier GPUs which had separate processors dedicated to each processing type. The programmable processor array is tightly integrated with fixed function processors for texture filtering, rasterization, raster operations, anti-aliasing, compression, decompression, display, video decoding, and high-defnition video processing. Although the fixed-function processors signifcantly outperform more general programmable processors in terms of absolute performance constrained by an area, cost, or power budget, we will focus on the programmable processors here.Áªü‰∏ÄÁöÑGPUÊû∂ÊûÑÂü∫‰∫éËÆ∏Â§öÂèØÁºñÁ®ãÂ§ÑÁêÜÂô®ÁöÑÂπ∂Ë°åÈòµÂàó„ÄÇ ÂÆÉ‰ª¨Âú®Áõ∏ÂêåÁöÑÂ§ÑÁêÜÂô®‰∏äÁªü‰∏ÄÈ°∂ÁÇπÔºåÂá†‰ΩïÂíåÂÉèÁ¥†ÁùÄËâ≤Âô®Â§ÑÁêÜ‰ª•ÂèäÂπ∂Ë°åËÆ°ÁÆóÔºåËøô‰∏éÊó©ÊúüÁöÑGPU‰∏çÂêåÔºåÂêéËÄÖÂÖ∑Êúâ‰∏ìÁî®‰∫éÊØèÁßçÂ§ÑÁêÜÁ±ªÂûãÁöÑÁã¨Á´ãÂ§ÑÁêÜÂô®„ÄÇ ÂèØÁºñÁ®ãÂ§ÑÁêÜÂô®ÈòµÂàó‰∏éÂõ∫ÂÆöÂäüËÉΩÂ§ÑÁêÜÂô®Á¥ßÂØÜÈõÜÊàêÔºåÁî®‰∫éÁ∫πÁêÜËøáÊª§ÔºåÂÖâÊ†ÖÂåñÔºåÂÖâÊ†ÖÊìç‰ΩúÔºåÊäóÈîØÈΩøÔºåÂéãÁº©ÔºåËß£ÂéãÁº©ÔºåÊòæÁ§∫ÔºåËßÜÈ¢ëËß£Á†ÅÂíåÈ´òÊ∏ÖÊô∞Â∫¶ËßÜÈ¢ëÂ§ÑÁêÜ„ÄÇ ËôΩÁÑ∂Âõ∫ÂÆöÂäüËÉΩÂ§ÑÁêÜÂô®Âú®Èù¢ÁßØÔºåÊàêÊú¨ÊàñÂäüÁéáÈ¢ÑÁÆóÈôêÂà∂ÁöÑÁªùÂØπÊÄßËÉΩÊñπÈù¢ÊòéÊòæ‰ºò‰∫éÊõ¥Â§öÈÄöÁî®ÂèØÁºñÁ®ãÂ§ÑÁêÜÂô®Ôºå‰ΩÜÊàë‰ª¨Â∞Ü‰∏ìÊ≥®‰∫éÊ≠§Â§ÑÁöÑÂèØÁºñÁ®ãÂ§ÑÁêÜÂô®„ÄÇ Compared with multicore CPUs, manycore GPUs have a different architectural design point, one focused on executing many parallel threads effciently on many processor cores. By using many simpler cores and optimizing for data-parallel behavior among groups of threads, more of the per-chip transistor budget is devoted to computation, and less to on-chip caches and overhead.‰∏éÂ§öÊ†∏CPUÁõ∏ÊØîÔºåÂ§öÊ†∏GPUÂÖ∑Êúâ‰∏çÂêåÁöÑÊû∂ÊûÑËÆæËÆ°ÁÇπÔºå‰∏Ä‰∏™‰æßÈáç‰∫éÂú®ËÆ∏Â§öÂ§ÑÁêÜÂô®ÂÜÖÊ†∏‰∏äÊúâÊïàÂú∞ÊâßË°åËÆ∏Â§öÂπ∂Ë°åÁ∫øÁ®ã„ÄÇ ÈÄöËøá‰ΩøÁî®ËÆ∏Â§öÊõ¥ÁÆÄÂçïÁöÑÂÜÖÊ†∏Âπ∂‰ºòÂåñÁ∫øÁ®ãÁªÑ‰πãÈó¥ÁöÑÊï∞ÊçÆÂπ∂Ë°åË°å‰∏∫ÔºåÊõ¥Â§öÁöÑÊØèËäØÁâáÊô∂‰ΩìÁÆ°È¢ÑÁÆóÁî®‰∫éËÆ°ÁÆóÔºåËÄåÊõ¥Â∞ëÁî®‰∫éÁâá‰∏äÈ´òÈÄüÁºìÂ≠òÂíåÂºÄÈîÄ„ÄÇ Processor ArrayA unifed GPU processor array contains many processor cores, typically organized into multithreaded multiprocessors. Figure C.2.5 shows a GPU with an array of 112 streaming processor (SP) cores, organized as 14 multithreaded streaming multiprocessors (SMs). Each SP core is highly multithreaded, managing 96 concurrent threads and their state in hardware. The processors connect with four 64-bit-wide DRAM partitions via an interconnection network. Each SM has eight SP cores, two special function units (SFUs), instruction and constant caches, a multithreaded instruction unit, and a shared memory. This is the basic Tesla architecture implemented by the NVIDIA GeForce 8800. It has a unifed architecture in which the traditional graphics programs for vertex, geometry, and pixel shading run on the unifed SMs and their SP cores, and computing programs run on the same processors.Áªü‰∏ÄGPUÂ§ÑÁêÜÂô®ÈòµÂàóÂåÖÂê´ËÆ∏Â§öÂ§ÑÁêÜÂô®ÂÜÖÊ†∏ÔºåÈÄöÂ∏∏ÁªÑÁªáÊàêÂ§öÁ∫øÁ®ãÂ§öÂ§ÑÁêÜÂô®„ÄÇ ÂõæC.2.5ÊòæÁ§∫‰∫Ü‰∏Ä‰∏™ÂÖ∑Êúâ112‰∏™ÊµÅÂ§ÑÁêÜÂô®ÔºàSPÔºâÂÜÖÊ†∏ÈòµÂàóÁöÑGPUÔºåÁªÑÁªá‰∏∫14‰∏™Â§öÁ∫øÁ®ãÊµÅÂ§öÂ§ÑÁêÜÂô®ÔºàSMÔºâ„ÄÇ ÊØè‰∏™SPÊ†∏ÂøÉÈÉΩÊòØÈ´òÂ∫¶Â§öÁ∫øÁ®ãÁöÑÔºåÁÆ°ÁêÜ96‰∏™Âπ∂ÂèëÁ∫øÁ®ãÂèäÂÖ∂Á°¨‰ª∂Áä∂ÊÄÅ„ÄÇ Â§ÑÁêÜÂô®ÈÄöËøá‰∫íËøûÁΩëÁªúËøûÊé•Âõõ‰∏™64‰ΩçÂÆΩÁöÑDRAMÂàÜÂå∫„ÄÇ ÊØè‰∏™SMÊúâ8‰∏™SPÂÜÖÊ†∏Ôºå2‰∏™ÁâπÊÆäÂäüËÉΩÂçïÂÖÉÔºàSFUÔºâÔºåÊåá‰ª§ÂíåÂ∏∏ÈáèÈ´òÈÄüÁºìÂ≠òÔºåÂ§öÁ∫øÁ®ãÊåá‰ª§ÂçïÂÖÉÂíåÂÖ±‰∫´Â≠òÂÇ®Âô®„ÄÇ ËøôÊòØÁî±NVIDIA GeForce 8800ÂÆûÁé∞ÁöÑÂü∫Êú¨TeslaÊû∂ÊûÑ„ÄÇÂÆÉÈááÁî®Áªü‰∏ÄÊû∂ÊûÑÔºåÂÖ∂‰∏≠È°∂ÁÇπÔºåÂá†‰ΩïÂíåÂÉèÁ¥†ÁùÄËâ≤ÁöÑ‰º†ÁªüÂõæÂΩ¢Á®ãÂ∫èÂú®Áªü‰∏ÄÁöÑSMÂèäÂÖ∂SPÂÜÖÊ†∏‰∏äËøêË°åÔºåËÆ°ÁÆóÁ®ãÂ∫èÂú®Áõ∏ÂêåÁöÑÂ§ÑÁêÜÂô®‰∏äËøêË°å„ÄÇThe processor array architecture is scalable to smaller and larger GPU confgurations by scaling the number of multiprocessors and the number of memory partitions. Figure C.2.5 shows seven clusters of two SMs sharing a texture unit and a texture L1 cache. The texture unit delivers filtered results to the SM given a set of coordinates into a texture map. Because filter regions of support often overlap for successive texture requests, a small streaming L1 texture cache is effective to reduce the number of requests to the memory system. The processor array connects with raster operation processors (ROPs), L2 texture caches, external DRAM memories, and system memory via a GPU-wide interconnection network. The number of processors and number of memories can scale to design balanced GPU systems for different performance and market segments.ÈÄöËøáÊâ©Â±ïÂ§öÂ§ÑÁêÜÂô®ÁöÑÊï∞ÈáèÂíåÂÜÖÂ≠òÂàÜÂå∫ÁöÑÊï∞ÈáèÔºåÂ§ÑÁêÜÂô®ÈòµÂàóÊû∂ÊûÑÂèØÊâ©Â±ïÂà∞Ë∂äÊù•Ë∂äÂ∞èÁöÑGPUÈÖçÁΩÆ„ÄÇ ÂõæC.2.5ÊòæÁ§∫‰∫ÜÂÖ±‰∫´Á∫πÁêÜÂçïÂÖÉÂíåÁ∫πÁêÜL1ÁºìÂ≠òÁöÑ‰∏§‰∏™SMÁöÑ‰∏É‰∏™Á∞á„ÄÇ Á∫πÁêÜÂçïÂÖÉÂ∞ÜËøáÊª§ÁªìÊûú‰º†ÈÄíÁªôSMÔºåÁªôÂÆö‰∏ÄÁªÑÂùêÊ†áÂà∞Á∫πÁêÜË¥¥Âõæ‰∏≠„ÄÇ Âõ†‰∏∫ÊîØÊåÅÁöÑËøáÊª§Âô®Âå∫ÂüüÁªèÂ∏∏‰∏éËøûÁª≠ÁöÑÁ∫πÁêÜËØ∑Ê±ÇÈáçÂè†ÔºåÊâÄ‰ª•Â∞èÁöÑÊµÅÂºèL1Á∫πÁêÜÈ´òÈÄüÁºìÂ≠òÂØπ‰∫éÂáèÂ∞ëÂØπÂ≠òÂÇ®Âô®Á≥ªÁªüÁöÑËØ∑Ê±ÇÁöÑÊï∞ÈáèÊòØÊúâÊïàÁöÑ„ÄÇ Â§ÑÁêÜÂô®ÈòµÂàóÈÄöËøáGPUËåÉÂõ¥ÁöÑ‰∫íËøûÁΩëÁªú‰∏éÂÖâÊ†ÖÊìç‰ΩúÂ§ÑÁêÜÂô®ÔºàROPÔºâÔºåL2Á∫πÁêÜÈ´òÈÄüÁºìÂ≠òÔºåÂ§ñÈÉ®DRAMÂ≠òÂÇ®Âô®ÂíåÁ≥ªÁªüÂ≠òÂÇ®Âô®ËøûÊé•„ÄÇ Â§ÑÁêÜÂô®ÁöÑÊï∞ÈáèÂíåÂ≠òÂÇ®Âô®ÁöÑÊï∞ÈáèÂèØ‰ª•Êâ©Â±ïÔºå‰ª•ËÆæËÆ°Áî®‰∫é‰∏çÂêåÊÄßËÉΩÂíåÁªÜÂàÜÂ∏ÇÂú∫ÁöÑÂπ≥Ë°°GPUÁ≥ªÁªü„ÄÇ]]></content>
      <categories>
        <category>GPUÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÁøªËØë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1_ÂõæÂΩ¢ÂíåËÆ°ÁÆóGPU]]></title>
    <url>%2F2019%2F01%2F11%2FGraphics_and_Computing_GPUs%2F</url>
    <content type="text"><![CDATA[This appendix focuses on the GPU‚Äîthe ubiquitous graphics processing unit in every PC, laptop, desktop computer, and workstation. In its most basic form, the GPU generates 2D and 3D graphics, images, and video that enable windowbased operating systems, graphical user interfaces, video games, visual imaging applications, and video. The modern GPU that we describe here is a highly parallel, highly multithreaded multiprocessor optimized for visual computing. To provide real-time visual interaction with computed objects via graphics, images, and video, the GPU has a unifed graphics and computing architecture that serves as both a programmable graphics processor and a scalable parallel computing platform. PCs and game consoles combine a GPU with a CPU to form heterogeneous systems. Êú¨ÈôÑÂΩïÈáçÁÇπ‰ªãÁªçGPU‚Äî‚ÄîÊØèÂè∞PCÔºåÁ¨îËÆ∞Êú¨ÁîµËÑëÔºåÂè∞ÂºèÊú∫ÂíåÂ∑•‰ΩúÁ´ô‰∏≠Êó†Â§Ñ‰∏çÂú®ÁöÑÂõæÂΩ¢Â§ÑÁêÜÂçïÂÖÉ„ÄÇ GPUÊúÄÂü∫Êú¨ÁöÑÂäüËÉΩÊòØÁîüÊàê2DÂíå3DÂõæÂΩ¢ÔºåÂõæÂÉèÂíåËßÜÈ¢ëÔºå‰ª•ÂèäÊîØÊåÅÂü∫‰∫éÁ™óÂè£ÁöÑÊìç‰ΩúÁ≥ªÁªüÔºåÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºåËßÜÈ¢ëÊ∏∏ÊàèÔºåËßÜËßâÊàêÂÉèÂ∫îÁî®Á®ãÂ∫èÂíåËßÜÈ¢ë„ÄÇ Êàë‰ª¨Âú®ËøôÈáåÊèèËø∞ÁöÑÁé∞‰ª£GPUÊòØ‰∏Ä‰∏™È´òÂ∫¶Âπ∂Ë°åÔºåÈ´òÂ∫¶Â§öÁ∫øÁ®ãÁöÑÂ§öÂ§ÑÁêÜÂô®ÔºåÈíàÂØπËßÜËßâËÆ°ÁÆóËøõË°å‰∫Ü‰ºòÂåñ„ÄÇ ‰∏∫‰∫ÜÈÄöËøáÂõæÂΩ¢ÔºåÂõæÂÉèÂíåËßÜÈ¢ëÊèê‰æõ‰∏éËÆ°ÁÆóÂØπË±°ÁöÑÂÆûÊó∂ÂèØËßÜ‰∫§‰∫íÔºåGPUÂÖ∑ÊúâÁªü‰∏ÄÁöÑÂõæÂΩ¢ÂíåËÆ°ÁÆóÊû∂ÊûÑÔºåÂèØÁî®‰ΩúÂèØÁºñÁ®ãÂõæÂΩ¢Â§ÑÁêÜÂô®ÂíåÂèØÊâ©Â±ïÁöÑÂπ∂Ë°åËÆ°ÁÆóÂπ≥Âè∞„ÄÇ PCÂíåÊ∏∏Êàè‰∏ªÊú∫Ôºàgame consoleÔºâÂ∞ÜGPU‰∏éCPUÁõ∏ÁªìÂêàÔºåÂΩ¢ÊàêÂºÇÊûÑÁ≥ªÁªü„ÄÇ graphics processing unit (gpu) Ôºö a processor optimized for 2d and 3d graphics, video, visual computing, and display ‰∏ÄÁßçÈíàÂØπ2DÂíå3DÂõæÂΩ¢ÔºåËßÜÈ¢ëÔºåËßÜËßâËÆ°ÁÆóÂíåÊòæÁ§∫ËøõË°å‰∫Ü‰ºòÂåñÁöÑÂ§ÑÁêÜÂô® visual computingËßÜËßâËÆ°ÁÆó Ôºöa mix of graphics processing and computing that lets you visually interact with computed objects via graphics, images, and video ‰∏ÄÁßçÂõæÂΩ¢Â§ÑÁêÜÂíåËÆ°ÁÆóÁöÑÊ∑∑ÂêàÔºå‰Ωø‰Ω†ÂèØ‰ª•ÈÄöËøáÂõæÂΩ¢„ÄÅÂõæÂÉèÂíåËßÜÈ¢ë‰∏éËÆ°ÁÆóÂØπË±°ËøõË°åÂèØËßÜÂåñ‰∫§‰∫í„ÄÇ heterogeneous system: A system combining different processor types. A PC is a heterogeneous CPU‚ÄìGPU system. ‰∏ÄÁßçÁªÑÂêà‰∫Ü‰∏çÂêåÂ§ÑÁêÜÂô®Á±ªÂûãÁöÑÁ≥ªÁªü„ÄÇ PCÂ∞±ÊòØ‰∏ÄÁßçÂºÇÊûÑCPU-GPUÁ≥ªÁªü„ÄÇ A Brief History of GPU Evolution GPUÂèëÂ±ïÁÆÄÂè≤fifteen years ago, there was no such thing as a gpu. graphics on a pc were performed by a video graphics array (vga) controller. a vga controller was simply a memory controller and display generator connected to some DRAM. in the 1990s, semiconductor technology advanced sufciently that more functions could be added to the vga controller. by 1997, vga controllers were beginning to incorporate some three-dimensional (3d) acceleration functions, including hardware for triangle setup and rasterization (dicing triangles into individual pixels) and texture mapping and shading (applying ‚Äúdecals‚Äù or patterns to pixels and blending colors).ÂçÅ‰∫îÂπ¥ÂâçÔºåËøòÊ≤°ÊúâÂÉèGPUËøôÊ†∑ÁöÑ‰∏úË•ø„ÄÇ PC‰∏äÁöÑÂõæÂΩ¢Áî±ËßÜÈ¢ëÂõæÂΩ¢ÈòµÂàóÔºàvideo graphics array ,VGAÔºâÊéßÂà∂Âô®Â±ïÁé∞„ÄÇ VGAÊéßÂà∂Âô®Âè™ÊòØ‰∏Ä‰∏™ËøûÊé•Âà∞Êüê‰∫õDRAMÁöÑÂ≠òÂÇ®Âô®ÊéßÂà∂Âô®ÂíåÊòæÁ§∫ÂèëÁîüÂô®„ÄÇ Âú®20‰∏ñÁ∫™90Âπ¥‰ª£ÔºåÂçäÂØº‰ΩìÊäÄÊúØÂÖÖÂàÜÂèëÂ±ïÔºåÂèØ‰ª•Âú®VGAÊéßÂà∂Âô®‰∏≠Ê∑ªÂä†Êõ¥Â§öÂäüËÉΩ„ÄÇ Âà∞1997Âπ¥ÔºåVGAÊéßÂà∂Âô®ÂºÄÂßãÈááÁî®‰∏Ä‰∫õ‰∏âÁª¥Ôºà3dÔºâÂä†ÈÄüÂäüËÉΩÔºåÂåÖÊã¨Áî®‰∫é‰∏âËßíÂΩ¢ËÆæÁΩÆÔºàTriangle SetupÔºâÂíåÂÖâÊ†ÖÂåñ(Rasterization)ÁöÑÁ°¨‰ª∂ÔºàÂ∞Ü‰∏âËßíÂΩ¢ÂàáÂâ≤ÊàêÂçï‰∏™ÂÉèÁ¥†ÔºâÂíåÁ∫πÁêÜÊò†Â∞ÑÂíåÁùÄËâ≤ÔºàÂ∞Ü‚ÄúË¥¥Ëä±‚ÄùÊàñÂõæÊ°àÂ∫îÁî®‰∫éÂÉèÁ¥†ÂíåÊ∑∑ÂêàÈ¢úËâ≤Ôºâ„ÄÇ In 2000, the single chip graphics processor incorporated almost every detail of the traditional high-end workstation graphics pipeline and, therefore, deserved a new name beyond vga controller. the term gpu was coined to denote that the graphics device had become a processor.2000Âπ¥ÔºåÂçïËäØÁâáÂõæÂΩ¢Â§ÑÁêÜÂô®Âá†‰πéÊï¥Âêà‰∫Ü‰º†ÁªüÈ´òÁ´ØÂ∑•‰ΩúÁ´ôÂõæÂΩ¢ÁÆ°ÈÅìÁöÑÊØè‰∏™ÁªÜËäÇÔºåÂõ†Ê≠§ÔºåÂÆÉÂ∫îÂΩìÊúâ‰∏Ä‰∏™Èô§VGAÂ§ñÁöÑÂÖ®Êñ∞ÁöÑÂêçÁß∞„ÄÇ ÊúØËØ≠GPUË¢´Áî®Êù•Ë°®Á§∫Ëøô‰∏™ÂõæÂΩ¢ËÆæÂ§áÔºàgraphics deviceÔºâÂ∑≤Êàê‰∏∫‰∏ÄÁßçÂ§ÑÁêÜÂô®„ÄÇ Over time, GPUs became more programmable, as programmable processors replaced fixed function dedicated logic while maintaining the basic 3D graphics pipeline organization. In addition, computations became more precise over time, progressing from indexed arithmetic, to integer and fixed point, to single precision Ô¨Çoating-point, and recently to double precision Ô¨Çoating-point. GPUs have become massively parallel programmable processors with hundreds of cores and thousands of threads. ÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ÁßªÔºåGPUÂèòÂæóÊõ¥Âä†ÂèØÁºñÁ®ãÔºåÂõ†‰∏∫ÂèØÁºñÁ®ãÂ§ÑÁêÜÂô®Âèñ‰ª£‰∫ÜÂõ∫ÂÆöÂäüËÉΩ‰∏ìÁî®ÈÄªËæëÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂü∫Êú¨ÁöÑ3DÂõæÂΩ¢ÁÆ°ÈÅìÁªÑÁªá„ÄÇÊ≠§Â§ñÔºåËÆ°ÁÆóÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ÁßªÂèòÂæóÊõ¥Âä†Á≤æÁ°ÆÔºå‰ªéÁ¥¢ÂºïÁÆóÊúØ(indexed arithmetic)ÔºåÊï¥Êï∞ÂíåÂõ∫ÂÆöÁÇπÔºåÂà∞ÂçïÁ≤æÂ∫¶ÊµÆÁÇπÔºåÊúÄËøëÂèàÂà∞ÂèåÁ≤æÂ∫¶ÊµÆÁÇπ„ÄÇ GPUÂ∑≤Êàê‰∏∫ÂÖ∑ÊúâÊï∞Áôæ‰∏™ÂÜÖÊ†∏ÂíåÊï∞ÂçÉ‰∏™Á∫øÁ®ãÁöÑÂ§ßËßÑÊ®°Âπ∂Ë°åÂèØÁºñÁ®ãÂ§ÑÁêÜÂô®„ÄÇ Recently, processor instructions and memory hardware were added to support general purpose programming languages, and a programming environment was created to allow GPUs to be programmed using familiar languages, including C and C++. This innovation makes a GPU a fully general-purpose, programmable, manycore processor, albeit still with some special benefits and limitations.ÊúÄËøëÔºåÊ∑ªÂä†‰∫ÜÂ§ÑÁêÜÂô®Êåá‰ª§ÂíåÂ≠òÂÇ®Âô®Á°¨‰ª∂‰ª•ÊîØÊåÅÈÄöÁî®ÁºñÁ®ãËØ≠Ë®ÄÔºåÂπ∂‰∏îÂàõÂª∫‰∫ÜÁºñÁ®ãÁéØÂ¢É‰ª•ÂÖÅËÆ∏‰ΩøÁî®ÁÜüÊÇâÁöÑËØ≠Ë®ÄÔºàÂåÖÊã¨CÂíåC ++ÔºâÂØπGPUËøõË°åÁºñÁ®ã„ÄÇ ËøôÈ°πÂàõÊñ∞‰ΩøGPUÊàê‰∏∫‰∏Ä‰∏™ÂÆåÂÖ®ÈÄöÁî®ÁöÑÂèØÁºñÁ®ãÂ§öÊ†∏Â§ÑÁêÜÂô®ÔºåÂ∞ΩÁÆ°‰ªçÊúâ‰∏Ä‰∫õÁâπÊÆäÁöÑÂ•ΩÂ§ÑÂíåÂ±ÄÈôê„ÄÇ GPU Graphics Trends GPUÂõæÂΩ¢ÁöÑÂèëÂ±ïË∂ãÂäøGPUs and their associated drivers implement the OpenGL and DirectX models of graphics processing. OpenGL is an open standard for 3D graphics programming available for most computers. DirectX is a series of Microsof multimedia programming interfaces, including Direct3D for 3D graphics. Since these application programming interfaces (APIs) have well-defned behavior, it is possible to build effective hardware acceleration of the graphics processing functions defned by the APIs. This is one of the reasons (in addition to increasing device density) why new GPUs are being developed every 12 to 18 months that double the performance of the previous generation on existing applications.GPUÂèäÂÖ∂Áõ∏ÂÖ≥È©±Âä®Á®ãÂ∫èÂÆûÁé∞‰∫ÜÂõæÂΩ¢Â§ÑÁêÜÁöÑOpenGLÂíåDirectXÊ®°Âûã„ÄÇ OpenGLÊòØÂ§ßÂ§öÊï∞ËÆ°ÁÆóÊú∫ÂèØÁî®ÁöÑ3DÂõæÂΩ¢ÁºñÁ®ãÁöÑÂºÄÊîæÊ†áÂáÜ„ÄÇ DirectXÊòØ‰∏ÄÁ≥ªÂàóMicrosofÂ§öÂ™í‰ΩìÁºñÁ®ãÊé•Âè£ÔºåÂåÖÊã¨Áî®‰∫é3DÂõæÂΩ¢ÁöÑDirect3D„ÄÇ Áî±‰∫éËøô‰∫õÂ∫îÁî®Á®ãÂ∫èÁºñÁ®ãÊé•Âè£ÔºàAPIÔºâÂÖ∑ÊúâËâØÂ•ΩÁöÑË°å‰∏∫ÔºåÂõ†Ê≠§ÂèØ‰ª•ÊûÑÂª∫Áî±APIÂÆö‰πâÁöÑÂõæÂΩ¢Â§ÑÁêÜÂäüËÉΩÁöÑÊúâÊïàÁ°¨‰ª∂Âä†ÈÄü„ÄÇ ËøôÊòØ‰∏∫‰ªÄ‰πàÔºàÈô§‰∫ÜÂ¢ûÂä†ËÆæÂ§áÂØÜÂ∫¶‰πãÂ§ñÔºâÊØè12Âà∞18‰∏™ÊúàÂºÄÂèëÊñ∞GPU‰ª•‰ΩøÁé∞ÊúâÂ∫îÁî®ÁöÑÂâç‰∏Ä‰ª£ÊÄßËÉΩÁøªÂÄçÁöÑÂéüÂõ†‰πã‰∏Ä„ÄÇ application programming interface (API) : A set of function and data structure definitions providing aninterface to a library of functions. ‰∏ÄÁªÑÂáΩÊï∞ÂíåÊï∞ÊçÆÁªìÊûÑÂÆö‰πâÔºåÊèê‰æõ‰∫ÜÂáΩÊï∞Â∫ìÁöÑÊé•Âè£„ÄÇ Frequent doubling of GPU performance enables new applications that were not previously possible. The intersection of graphics processing and parallel computing invites a new paradigm for graphics, known as visual computing. It replaces large sections of the traditional sequential hardware graphics pipeline model with programmable elements for geometry, vertex, and pixel programs.Visual computing in a modern GPU combines graphics processing and parallel computing in novel ways that permit new graphics algorithms to be implemented, and opens the door to entirely new parallel processing applications on pervasive high-performance GPUs.GPUÊÄßËÉΩÁöÑÈ¢ëÁπÅÂä†ÂÄçÂèØÂÆûÁé∞‰ª•ÂâçÊó†Ê≥ïÂÆûÁé∞ÁöÑÊñ∞Â∫îÁî®Á®ãÂ∫è„ÄÇ ÂõæÂΩ¢Â§ÑÁêÜÂíåÂπ∂Ë°åËÆ°ÁÆóÁöÑ‰∫§ÈõÜÂºïÂèë‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂõæÂΩ¢ËåÉ‰æãÔºåÁß∞‰∏∫ËßÜËßâËÆ°ÁÆó„ÄÇ ÂÆÉÂ∞Ü‰º†ÁªüÈ°∫Â∫èÁ°¨‰ª∂ÂõæÂΩ¢ÁÆ°ÈÅìÊ®°ÂûãÁöÑÂ§ßÈÉ®ÂàÜÊõøÊç¢‰∏∫Âá†‰ΩïÔºåÈ°∂ÁÇπÂíåÂÉèÁ¥†Á®ãÂ∫èÁöÑÂèØÁºñÁ®ãÂÖÉÁ¥†„ÄÇÁé∞‰ª£GPU‰∏≠ÁöÑÂèØËßÜËÆ°ÁÆó‰ª•Êñ∞È¢ñÁöÑÊñπÂºèÂ∞ÜÂõæÂΩ¢Â§ÑÁêÜÂíåÂπ∂Ë°åËÆ°ÁÆóÁõ∏ÁªìÂêàÔºåÂÖÅËÆ∏ÂÆûÁé∞Êñ∞ÁöÑÂõæÂΩ¢ÁÆóÊ≥ïÔºåÂπ∂ÊâìÂºÄ Âú®ÊôÆÂèäÁöÑÈ´òÊÄßËÉΩGPU‰∏äÂÆûÁé∞ÂÖ®Êñ∞Âπ∂Ë°åÂ§ÑÁêÜÂ∫îÁî®ÁöÑÂ§ßÈó®„ÄÇ Heterogeneous System ÂºÇÊûÑÁ≥ªÁªüAlthough the GPU is arguably the most parallel and most powerful processor in a typical PC, it is certainly not the only processor. The CPU, now multicore and soon to be manycore, is a complementary, primarily serial processor companion to the massively parallel manycore GPU. Together, these two types of processors comprise a heterogeneous multiprocessor system.ËôΩÁÑ∂GPUÂèØ‰ª•ËØ¥ÊòØÂÖ∏ÂûãPC‰∏≠ÊúÄÂπ∂Ë°åÔºåÊúÄÂº∫Â§ßÁöÑÂ§ÑÁêÜÂô®Ôºå‰ΩÜÂÆÉËÇØÂÆö‰∏çÊòØÂîØ‰∏ÄÁöÑÂ§ÑÁêÜÂô®„ÄÇ CPUÔºåÁé∞Âú®ÊòØÂ§öÊ†∏(multicore)ÁöÑÔºåÂæàÂø´Â∞±‰ºöÊàê‰∏∫‰ºóÊ†∏ÔºàmanycoreÔºâÔºåÊòØÂ§ßËßÑÊ®°Âπ∂Ë°åÂ§öÊ†∏GPU‰∫íË°•ÁöÑÔºåÈáçË¶ÅÁöÑ‰∏≤Ë°åÂ§ÑÁêÜÂô®‰ºô‰º¥ÔºàÂ•ΩÊãóÂè£Ôºâ„ÄÇ Ëøô‰∏§ÁßçÁ±ªÂûãÁöÑÂ§ÑÁêÜÂô®‰∏ÄËµ∑ÊûÑÊàê‰∫ÜÂºÇÊûÑÂ§öÂ§ÑÁêÜÂô®Á≥ªÁªü„ÄÇ The best performance for many applications comes from using both the CPU and the GPU. Tis appendix will help you understand how and when to best split the work between these two increasingly parallel processors.ËÆ∏Â§öÂ∫îÁî®Á®ãÂ∫èÁöÑÊúÄ‰Ω≥ÊÄßËÉΩÊù•Ëá™‰∫é‰ΩøÁî®CPUÂíåGPU„ÄÇ ÈôÑÂΩïÂ∞ÜÂ∏ÆÂä©ÊÇ®‰∫ÜËß£Â¶Ç‰Ωï‰ª•Âèä‰ΩïÊó∂ÊúÄÂ•ΩÂú∞ÂàÜÂâ≤Ëøô‰∏§‰∏™Êó•ÁõäÂπ∂Ë°åÁöÑÂ§ÑÁêÜÂô®‰πãÈó¥ÁöÑÂ∑•‰Ωú„ÄÇ GPU Evolves into Scalable Parallel Processor GPUÂèëÂ±ïÊàêÂèØÊâ©Â±ïÁöÑÂπ∂Ë°åÂ§ÑÁêÜÂô®GPUs have evolved functionally from hardwired, limited capability VGA controllers to programmable parallel processors. This evolution has proceeded by changing the logical (API-based) graphics pipeline to incorporate programmable elements and also by making the underlying hardware pipeline stages less specialized and more programmable. Eventually, it made sense to merge disparate programmable pipeline elements into one unifed array of many programmable processors.GPUÂ∑≤Áªè‰ªéÁ°¨ËøûÁ∫øÔºåÊúâÈôêÂäüËÉΩÁöÑVGAÊéßÂà∂Âô®ÂèëÂ±ïÂà∞ÂèØÁºñÁ®ãÂπ∂Ë°åÂ§ÑÁêÜÂô®„ÄÇ ÈÄöËøáÊîπÂèòÈÄªËæëÔºàÂü∫‰∫éAPIÔºâÁöÑÂõæÂΩ¢ÊµÅÊ∞¥Á∫ø‰ª•ÁªìÂêàÂèØÁºñÁ®ãÂÖÉ‰ª∂‰ª•ÂèäÈÄöËøá‰ΩøÂ∫ïÂ±ÇÁ°¨‰ª∂ÊµÅÊ∞¥Á∫øÈò∂ÊÆµ‰∏çÈÇ£‰πà‰∏ì‰∏öÂåñÂíåÊõ¥ÂèØÁºñÁ®ãÊù•ËøõË°åËøôÁßçÊºîÂèò„ÄÇ ÊúÄÁªàÔºåÂ∞Ü‰∏çÂêåÁöÑÂèØÁºñÁ®ãÊµÅÊ∞¥Á∫øÂÖÉ‰ª∂ÂêàÂπ∂Âà∞‰∏Ä‰∏™Áî±Â§ö‰∏™ÂèØÁºñÁ®ãÂ§ÑÁêÜÂô®ÁªÑÊàêÁöÑÁªü‰∏ÄÈòµÂàó‰∏≠ÊòØÊúâÊÑè‰πâÁöÑ„ÄÇIn the GeForce 8-series generation of GPUs, the geometry, vertex, and pixel processing all run on the same type of processor. This unification allows for dramatic scalability. More programmable processor cores increase the total system throughput. Unifying the processors also delivers very eÔ¨Äective load balancing, since any processing function can use the whole processor array. At the other end of the spectrum, a processor array can now be built with very few processors, since all of the functions can be run on the same processors.Âú®GeForce 8Á≥ªÂàóGPU‰∏≠ÔºåÂá†‰ΩïÔºåÈ°∂ÁÇπÂíåÂÉèÁ¥†Â§ÑÁêÜÈÉΩÂú®Âêå‰∏ÄÁ±ªÂûãÁöÑÂ§ÑÁêÜÂô®‰∏äËøêË°å„ÄÇ ËøôÁßçÁªü‰∏ÄÂÖÅËÆ∏ÊòæÁùÄÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ Êõ¥Â§öÂèØÁºñÁ®ãÂ§ÑÁêÜÂô®ÂÜÖÊ†∏ÂèØÊèêÈ´òÁ≥ªÁªüÊÄªÂêûÂêêÈáè„ÄÇ Áªü‰∏ÄÂ§ÑÁêÜÂô®ËøòÂèØ‰ª•Êèê‰æõÈùûÂ∏∏ÊúâÊïàÁöÑË¥üËΩΩÂπ≥Ë°°ÔºåÂõ†‰∏∫‰ªª‰ΩïÂ§ÑÁêÜÂäüËÉΩÈÉΩÂèØ‰ª•‰ΩøÁî®Êï¥‰∏™Â§ÑÁêÜÂô®ÈòµÂàó„ÄÇ Âè¶‰∏ÄÊñπÈù¢ÔºåÂ§ÑÁêÜÂô®ÈòµÂàóÁé∞Âú®ÂèØ‰ª•Áî®ÂæàÂ∞ëÁöÑÂ§ÑÁêÜÂô®ÊûÑÂª∫ÔºåÂõ†‰∏∫ÊâÄÊúâÂäüËÉΩÈÉΩÂèØ‰ª•Âú®Áõ∏ÂêåÁöÑÂ§ÑÁêÜÂô®‰∏äËøêË°å„ÄÇ Why CUDA and GPU Computing? ‰∏∫‰ªÄ‰πàÈÄâÊã©CUDAÂíåGPUËÆ°ÁÆóÔºüThis uniform and scalable array of processors invites a new model of programming for the GPU. The large amount of Ô¨Çoating-point processing power in the GPU processor array is very attractive for solving nongraphics problems. Given the large degree of parallelism and the range of scalability of the processor array for graphics applications, the programming model for more general computing must express the massive parallelism directly, but allow for scalable execution.ËøôÁßçÁªü‰∏Ä‰∏îÂèØÊâ©Â±ïÁöÑÂ§ÑÁêÜÂô®ÈòµÂàó‰∏∫GPUÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁºñÁ®ãÊ®°Âûã„ÄÇ GPUÂ§ÑÁêÜÂô®ÈòµÂàó‰∏≠ÁöÑÂ§ßÈáèÊµÆÁÇπÂ§ÑÁêÜËÉΩÂäõÂØπ‰∫éËß£ÂÜ≥ÈùûÂõæÂΩ¢ÈóÆÈ¢òÈùûÂ∏∏ÊúâÂê∏ÂºïÂäõ„ÄÇ Èâ¥‰∫éÁî®‰∫éÂõæÂΩ¢Â∫îÁî®Á®ãÂ∫èÁöÑÂ§ÑÁêÜÂô®ÈòµÂàóÁöÑÈ´òÂ∫¶Âπ∂Ë°åÊÄßÂíåÂèØÊâ©Â±ïÊÄßËåÉÂõ¥ÔºåÁî®‰∫éÊõ¥‰∏ÄËà¨ËÆ°ÁÆóÁöÑÁºñÁ®ãÊ®°ÂûãÂøÖÈ°ªÁõ¥Êé•Ë°®ËææÂ§ßËßÑÊ®°Âπ∂Ë°åÊÄßÔºå‰ΩÜÂÖÅËÆ∏ÂèØ‰º∏Áº©ÊâßË°å„ÄÇ GPU computing is the term coined for using the GPU for computing via a parallel programming language and API, without using the traditional graphics API and graphics pipeline model. This is in contrast to the earlier General Purpose computation on GPU (GPGPU) approach, which involves programming the GPU using a graphics API and graphics pipeline to perform nongraphics tasks.GPUËÆ°ÁÆóÊòØÈÄöËøáÂπ∂Ë°åÁºñÁ®ãËØ≠Ë®ÄÂíåAPI‰ΩøÁî®GPUËøõË°åËÆ°ÁÆóËÄåÂàõÈÄ†ÁöÑÊúØËØ≠ÔºåËÄå‰∏ç‰ΩøÁî®‰º†ÁªüÁöÑÂõæÂΩ¢APIÂíåÂõæÂΩ¢ÁÆ°ÈÅìÊ®°Âûã„ÄÇ Ëøô‰∏éÊó©ÊúüÁöÑGPU‰∏äÈÄöÁî®ËÆ°ÁÆóÔºàGPGPUÔºâÊñπÊ≥ïÂΩ¢ÊàêÂØπÊØîÔºåÂêéËÄÖÊ∂âÂèä‰ΩøÁî®ÂõæÂΩ¢APIÂíåÂõæÂΩ¢ÁÆ°ÈÅìÂØπGPUËøõË°åÁºñÁ®ã‰ª•ÊâßË°åÈùûÂõæÂΩ¢‰ªªÂä°„ÄÇ GPU computing : Using a GPU for computing via a parallel programming language and API. ‰ΩøÁî®GPUÈÄöËøáÂπ∂Ë°åÁºñÁ®ãËØ≠Ë®ÄÂíåAPIËøõË°åËÆ°ÁÆó„ÄÇGPGPU : Using a GPU for general-purpose computation via a traditional graphics API and graphics pipeline.ÈÄöËøá‰º†ÁªüÁöÑÂõæÂΩ¢APIÂíåÂõæÂΩ¢ÁÆ°ÈÅìÂ∞ÜGPUÁî®‰∫éÈÄöÁî®ËÆ°ÁÆó„ÄÇ Compute Unifed Device Architecture (CUDA) is a scalable parallel programming model and sofware platform for the GPU and other parallel processors that allows the programmer to bypass the graphics API and graphics interfaces of the GPU and simply program in C or C++. The CUDA programming model has an SPMD (single-program multiple data) software style, in which a programmer writes a program for one thread that is instanced and executed by many threads in parallel on the multiple processors of the GPU. In fact, CUDA also provides a facility for programming multiple CPU cores as well, so CUDA is an environment for writing parallel programs for the entire heterogeneous computer system.ËÆ°ÁÆóÁªü‰∏ÄËÆæÂ§áÊû∂ÊûÑÔºàCUDAÔºâÊòØGPUÂíåÂÖ∂‰ªñÂπ∂Ë°åÂ§ÑÁêÜÂô®ÁöÑÂèØÊâ©Â±ïÂπ∂Ë°åÁºñÁ®ãÊ®°ÂûãÂíåËΩØ‰ª∂Âπ≥Âè∞ÔºåÂÖÅËÆ∏Á®ãÂ∫èÂëòÁªïËøáGPUÁöÑÂõæÂΩ¢APIÂíåÂõæÂΩ¢Êé•Âè£ÔºåÂè™ÈúÄÁî®CÊàñC ++ÁºñÁ®ã„ÄÇ CUDAÁºñÁ®ãÊ®°ÂûãÂÖ∑ÊúâSPMDÔºàÂçïÁ®ãÂ∫èÂ§öÊï∞ÊçÆÔºâÁöÑËΩØ‰ª∂È£éÊ†ºÔºåÂÖ∂‰∏≠Á®ãÂ∫èÂëò‰∏∫‰∏Ä‰∏™Á∫øÁ®ãÁºñÂÜôÁ®ãÂ∫èÔºåËØ•Á®ãÂ∫èÁî±Â§ö‰∏™Á∫øÁ®ãÂπ∂Ë°åÊâßË°åÂπ∂Áî±GPUÁöÑÂ§ö‰∏™Â§ÑÁêÜÂô®ÊâßË°å„ÄÇ ÂÆûÈôÖ‰∏äÔºåCUDA‰πüÊèê‰æõ‰∫ÜÁºñÁ®ãÂ§ö‰∏™CPUÂÜÖÊ†∏ÁöÑÂ∑•ÂÖ∑ÔºåÂõ†Ê≠§CUDAÊòØ‰∏Ä‰∏™‰∏∫Êï¥‰∏™ÂºÇÊûÑËÆ°ÁÆóÊú∫Á≥ªÁªüÁºñÂÜôÂπ∂Ë°åÁ®ãÂ∫èÁöÑÁéØÂ¢É„ÄÇ CUDA: A scalable parallel programming model and language based on C/C++. It is a parallel programming platform for GPUs and multicore CPUs ‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÂπ∂Ë°åÁºñÁ®ãÊ®°ÂûãÂíåÂü∫‰∫éC / C ++ÁöÑËØ≠Ë®Ä„ÄÇ ÂÆÉÊòØGPUÂíåÂ§öÊ†∏CPUÁöÑÂπ∂Ë°åÁºñÁ®ãÂπ≥Âè∞ GPU Unifes Graphics and Computing GPUÁªü‰∏Ä‰∫ÜÂõæÂΩ¢ÂíåËÆ°ÁÆóWith the addition of CUDA and GPU computing to the capabilities of the GPU, it is now possible to use the GPU as both a graphics processor and a computing processor at the same time, and to combine these uses in visual computing applications. The underlying processor architecture of the GPU is exposed in two ways: first, as implementing the programmable graphics APIs, and second, as a massively parallel processor array programmable in C/C++ with CUDA.ÈÄöËøáÂ∞ÜCUDAÂíåGPUËÆ°ÁÆóÊ∑ªÂä†Âà∞GPUÁöÑÂäüËÉΩÔºåÁé∞Âú®ÂèØ‰ª•ÂêåÊó∂Â∞ÜGPUÁî®‰ΩúÂõæÂΩ¢Â§ÑÁêÜÂô®ÂíåËÆ°ÁÆóÂ§ÑÁêÜÂô®ÔºåÂπ∂Â∞ÜËøô‰∫õÁî®ÈÄîÁªìÂêàÂú®ËßÜËßâËÆ°ÁÆóÂ∫îÁî®‰∏≠„ÄÇ GPUÁöÑÂ∫ïÂ±ÇÂ§ÑÁêÜÂô®Êû∂ÊûÑ‰ª•‰∏§ÁßçÊñπÂºèÊö¥Èú≤ÔºöÁ¨¨‰∏ÄÔºå‰Ωú‰∏∫ÂÆûÁé∞ÂèØÁºñÁ®ãÂõæÂΩ¢APIÔºåÁ¨¨‰∫åÔºå‰Ωú‰∏∫‰ΩøÁî®CUDAÂú®C / C ++‰∏≠ÁºñÁ®ãÁöÑÂ§ßËßÑÊ®°Âπ∂Ë°åÂ§ÑÁêÜÂô®ÈòµÂàó„ÄÇAlthough the underlying processors of the GPU are unified, it is not necessary that all of the SPMD thread programs are the same. The GPU can run graphics shader programs for the graphics aspect of the GPU, processing geometry, vertices, and pixels, and also run thread programs in CUDA.ËôΩÁÑ∂GPUÁöÑÂ∫ïÂ±ÇÂ§ÑÁêÜÂô®ÊòØÁªü‰∏ÄÁöÑÔºå‰ΩÜÂπ∂‰∏çÊòØÊâÄÊúâÁöÑSPMDÁ∫øÁ®ãÁ®ãÂ∫èÈÉΩÊòØÁõ∏ÂêåÁöÑ„ÄÇ GPUÂèØ‰ª•‰∏∫GPUÁöÑÂõæÂΩ¢ÊñπÈù¢ËøêË°åÂõæÂΩ¢ÁùÄËâ≤Âô®Á®ãÂ∫èÔºåÂ§ÑÁêÜÂá†‰ΩïÔºåÈ°∂ÁÇπÂíåÂÉèÁ¥†ÔºåËøòÂèØ‰ª•Âú®CUDA‰∏≠ËøêË°åÁ∫øÁ®ãÁ®ãÂ∫è„ÄÇThe GPU is truly a versatile multiprocessor architecture, supporting a variety of processing tasks. GPUs are excellent at graphics and visual computing as they were specifcally designed for these applications. GPUs are also excellent at many generalpurpose throughput applications that are ‚Äúfirst cousins‚Äù of graphics, in that they perform a lot of parallel work, as well as having a lot of regular problem structure. In general, they are a good match to data-parallel problems (see Chapter 6), particularly large problems, but less so for less regular, smaller problems.GPUÊòØÁúüÊ≠£ÁöÑÂ§öÂäüËÉΩÂ§öÂ§ÑÁêÜÂô®Êû∂ÊûÑÔºåÊîØÊåÅÂêÑÁßçÂ§ÑÁêÜ‰ªªÂä°„ÄÇ GPUÂú®ÂõæÂΩ¢ÂíåËßÜËßâËÆ°ÁÆóÊñπÈù¢ÈùûÂ∏∏Âá∫Ëâ≤ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÊòØ‰∏ìÈó®‰∏∫Ëøô‰∫õÂ∫îÁî®Á®ãÂ∫èËÆæËÆ°ÁöÑ„ÄÇ GPUÂú®ËÆ∏Â§öÈÄöÁî®ÂêûÂêêÈáèÂ∫îÁî®Á®ãÂ∫è‰∏≠‰πüÈùûÂ∏∏Âá∫Ëâ≤ÔºåËøô‰∫õÂ∫îÁî®Á®ãÂ∫èÊòØÂõæÂΩ¢ÁöÑ‚ÄúÁ¨¨‰∏Ä‰ª£Ë°®Áé∞ÂΩ¢Âºè‚ÄùÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÊâßË°åÂ§ßÈáèÂπ∂Ë°åÂ∑•‰ΩúÔºåÂπ∂‰∏îÂÖ∑ÊúâËÆ∏Â§öÂ∏∏ËßÑÈóÆÈ¢òÁªìÊûÑ„ÄÇ ‰∏ÄËà¨Êù•ËØ¥ÔºåÂÆÉ‰ª¨‰∏éÊï∞ÊçÆÂπ∂Ë°åÈóÆÈ¢òÔºàËßÅÁ¨¨6Á´†ÔºâÈùûÂ∏∏ÂåπÈÖçÔºåÁâπÂà´ÊòØÂ§ßÈóÆÈ¢òÔºå‰ΩÜÂØπ‰∫é‰∏çÈÇ£‰πàËßÑÂæãÔºåËæÉÂ∞èÁöÑÈóÆÈ¢òÂàô‰∏çÈÇ£‰πàÈáçË¶Å„ÄÇ GPU Visual Computing Applications GPUËßÜËßâËÆ°ÁÆóÂ∫îÁî®Á®ãÂ∫èVisual computing includes the traditional types of graphics applications plus many new applications. The original purview of a GPU was ‚Äúanything with pixels,‚Äù but it now includes many problems without pixels but with regular computation and/or data structure. GPUs are effective at 2D and 3D graphics, since that is the purpose for which they are designed. Failure to deliver this application performance would be fatal. 2D and 3D graphics use the GPU in its ‚Äúgraphics mode‚Äù, accessing the processing power of the GPU through the graphics APIs, OpenGL‚Ñ¢, and DirectX‚Ñ¢. Games are built on the 3D graphics processing capability.ËßÜËßâËÆ°ÁÆóÂåÖÊã¨‰º†ÁªüÁ±ªÂûãÁöÑÂõæÂΩ¢Â∫îÁî®Á®ãÂ∫è‰ª•ÂèäËÆ∏Â§öÊñ∞Â∫îÁî®Á®ãÂ∫è„ÄÇ GPUÁöÑÂéüÂßãËåÉÂõ¥ÊòØ‚Äú‰ªª‰ΩïÂ∏¶ÊúâÂÉèÁ¥†ÁöÑ‰∏úË•ø‚ÄùÔºå‰ΩÜÂÆÉÁé∞Âú®ÂåÖÊã¨ËÆ∏Â§öÊ≤°ÊúâÂÉèÁ¥†‰ΩÜÂÖ∑ÊúâÂ∏∏ËßÑËÆ°ÁÆóÂíå/ÊàñÊï∞ÊçÆÁªìÊûÑÁöÑÈóÆÈ¢ò„ÄÇ GPUÂú®2DÂíå3DÂõæÂΩ¢‰∏äÊòØÊúâÊïàÁöÑÔºåÂõ†‰∏∫ËøôÊòØÂÆÉ‰ª¨ÁöÑËÆæËÆ°ÁõÆÁöÑ„ÄÇ Êú™ËÉΩÊèê‰æõÊ≠§Â∫îÁî®ÊÄßËÉΩ(application performance)Â∞ÜÊòØËá¥ÂëΩÁöÑ„ÄÇ 2DÂíå3DÂõæÂΩ¢Âú®ÂÖ∂‚ÄúÂõæÂΩ¢Ê®°Âºè‚Äù‰∏≠‰ΩøÁî®GPUÔºåÈÄöËøáÂõæÂΩ¢APIÔºåOpenGL‚Ñ¢ÂíåDirectX‚Ñ¢Êù•Ëé∑ÂèñGPUÁöÑÂ§ÑÁêÜËÉΩÂäõ„ÄÇ Ê∏∏ÊàèÊòØÂü∫‰∫é3DÂõæÂΩ¢Â§ÑÁêÜÂäüËÉΩÁöÑ„ÄÇ Application performance, ËøôÊòØ‰ªÄ‰πàÔºüÊÑüËßâÂÉèÊòØÂú®Êåá‚ÄúÂäüËÉΩ‚ÄùÔºåÊØîÂ¶ÇÊ≤°ÊúâÊèê‰æõËøô‰∏™ÂäüËÉΩÂ∞ÜÊòØËá¥ÂëΩÁöÑ Beyond 2D and 3D graphics, image processing and video are important applications for GPUs. These can be implemented using the graphics APIs or as computational programs, using CUDA to program the GPU in computing mode. Using CUDA, image processing is simply another data-parallel array program. To the extent that the data access is regular and there is good locality, the program will be efficient. In practice, image processing is a very good application for GPUs. Video processing, especially encode and decode (compression and decompression according to some standard algorithms), is quite efficient.Èô§‰∫Ü2DÂíå3DÂõæÂΩ¢ÔºåÂõæÂÉèÂ§ÑÁêÜÂíåËßÜÈ¢ëÊòØGPUÁöÑÈáçË¶ÅÂ∫îÁî®„ÄÇ Ëøô‰∫õÂèØ‰ª•‰ΩøÁî®ÂõæÂΩ¢APIÊàñËÆ°ÁÆóÁ®ãÂ∫èÊù•ÂÆûÁé∞Ôºå‰ΩøÁî®CUDAÂú®ËÆ°ÁÆóÊ®°Âºè‰∏ãÂØπGPUËøõË°åÁºñÁ®ã„ÄÇ ‰ΩøÁî®CUDAÔºåÂõæÂÉèÂ§ÑÁêÜÂè™ÊòØÂè¶‰∏ÄÁßçÊï∞ÊçÆÂπ∂Ë°åÈòµÂàóÁ®ãÂ∫è„ÄÇ Â¶ÇÊûúÊï∞ÊçÆËÆøÈóÆÊòØËßÑÂàôÁöÑÂπ∂‰∏îÂÖ∑ÊúâËâØÂ•ΩÁöÑ‰ΩçÁΩÆÔºåÂàôËØ•Á®ãÂ∫èÂ∞ÜÊòØÊúâÊïàÁöÑ„ÄÇ ÂÆûÈôÖ‰∏äÔºåÂõæÂÉèÂ§ÑÁêÜÊòØGPUÁöÑ‰∏Ä‰∏™ÈùûÂ∏∏Â•ΩÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇ ËßÜÈ¢ëÂ§ÑÁêÜÔºåÂ∞§ÂÖ∂ÊòØÁºñÁ†ÅÂíåËß£Á†ÅÔºàÊ†πÊçÆ‰∏Ä‰∫õÊ†áÂáÜÁÆóÊ≥ïËøõË°åÂéãÁº©ÂíåËß£ÂéãÁº©ÔºâÈùûÂ∏∏ÊúâÊïà„ÄÇ The greatest opportunity for visual computing applications on GPUs is to ‚Äúbreak the graphics pipeline.‚Äù Early GPUs implemented only specific graphics APIs, albeit at very high performance. This was wonderful if the API supported the operations that you wanted to do. If not, the GPU could not accelerate your task, because early GPU functionality was immutable. Now, with the advent of GPU computing and CUDA, these GPUs can be programmed to implement a different virtual pipeline by simply writing a CUDA program to describe the computation and data Ô¨Çow that is desired. So, all applications are now possible, which will stimulate new visual computing approaches.GPU‰∏äÂèØËßÜÂåñËÆ°ÁÆóÂ∫îÁî®Á®ãÂ∫èÁöÑÊúÄÂ§ßÁöÑÊú∫ÈÅáÊòØ‚ÄúÊâìÁ†¥ÂõæÂΩ¢ÁÆ°ÈÅì„ÄÇ‚Äù Êó©ÊúüÁöÑGPUÂè™ÂÆûÁé∞‰∫ÜÁâπÂÆöÁöÑÂõæÂΩ¢APIÔºåÂ∞ΩÁÆ°ÊÄßËÉΩÈùûÂ∏∏È´ò„ÄÇ Â¶ÇÊûúAPIÊîØÊåÅ‰Ω†ÊÉ≥Ë¶ÅÊâßË°åÁöÑÊìç‰ΩúÔºåÈÇ£Â∞±Â§™Ê£í‰∫Ü„ÄÇ Â¶ÇÊûúÊ≤°ÊúâÔºåGPUÂ∞±Êó†Ê≥ïÂä†ÈÄüÊÇ®ÁöÑ‰ªªÂä°ÔºåÂõ†‰∏∫Êó©ÊúüÁöÑGPUÂäüËÉΩÊòØ‰∏çÂèØÂèòÁöÑ„ÄÇ Áé∞Âú®ÔºåÈöèÁùÄGPUËÆ°ÁÆóÂíåCUDAÁöÑÂá∫Áé∞ÔºåËøô‰∫õGPUÂèØ‰ª•ÈÄöËøáÁºñÂÜôCUDAÁ®ãÂ∫èÊù•ÁºñÁ®ãÂÆûÁé∞‰∏çÂêåÁöÑËôöÊãüÁÆ°ÈÅìÔºå‰ª•ÊèèËø∞ÊâÄÈúÄÁöÑËÆ°ÁÆóÂíåÊï∞ÊçÆÊµÅ„ÄÇ Âõ†Ê≠§ÔºåÁé∞Âú®ÊâÄÊúâÂ∫îÁî®Á®ãÂ∫èÈÉΩÂèØ‰ª•ÂÆûÁé∞ÔºåËøôÂ∞ÜÂà∫ÊøÄÊñ∞ÁöÑËßÜËßâËÆ°ÁÆóÊñπÊ≥ï„ÄÇ]]></content>
      <categories>
        <category>GPUÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÁøªËØë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TelloÁâπÊ¥õÊó†‰∫∫Êú∫ÁºñÁ®ãÂ∞èÁªì]]></title>
    <url>%2F2019%2F01%2F05%2FTello%2F</url>
    <content type="text"><![CDATA[Á†îÁ©∂‰∫Ü‰∏Ä‰∏ãTelloÊó†‰∫∫Êú∫ÁöÑÁºñÁ®ãÔºåÁªìÊûúÂèëÁé∞ËøôÁé©ÂÖ∑ÁöÑÁºñÁ®ãËøòÁúüÁöÑÊòØÁÆÄÂçï„ÄÇÂÆòÊñπÁªô‰∫Ü‰∏Ä‰∏™ÊïôÁ®ãÊòØ‰ΩøÁî®scratchÂπ≥Âè∞ËøõË°åÁºñÁ®ãÔºåÁÑ∂ËÄåÂ±ÄÈôêÊÄßÊúâÁÇπÂ§ßÔºåÁßØÊú®Â∞±ÈÇ£Âá†‰∏™ËÄåSDK‰∏≠ÁöÑAPIË¶ÅÊõ¥Â§ö‰∏Ä‰∫õ„ÄÇÂØπ‰∫éÊúâÁÇπÁºñÁ®ãÂü∫Á°ÄÁöÑ‰∫∫Êù•ËØ¥ÔºåÊèê‰æõ‰∫ÜAPIÂ∞±ÊÑèÂë≥ÁùÄ‰∏çÂøÖÂ±ÄÈôê‰∫é‰ªª‰ΩïÂπ≥Âè∞‰∫Ü„ÄÇ Êû∂ÊûÑÂª∫Á´ãÁâπÊ¥õ Tello Âíå PC„ÄÅMac ÊàñÁßªÂä®ËÆæÂ§á‰πãÈó¥ÁöÑ Wi-Fi ÈÄö‰ø° ÂèëÈÄÅÂëΩ‰ª§ÂíåÊé•Êî∂ÂìçÂ∫îTello IPÔºö192.168.10.1 UDP PORTÔºö8889 &lt;&lt; - - &gt;&gt; PC / Mac / Mobile Â§áÊ≥®1ÔºöÂú®PCÔºåMacÊàñÁßªÂä®ËÆæÂ§á‰∏äËÆæÁΩÆUDPÂÆ¢Êà∑Á´ØÔºåÂêëÁâπÊ¥õ Tello UDP Á´ØÂè£ 8889 ÂèëÈÄÅÂëΩ‰ª§ÂíåÊé•Êî∂ÂìçÂ∫î„ÄÇ Â§áÊ≥®2ÔºöÂú®ÂèëÈÄÅÊâÄÊúâÂÖ∂‰ªñÂëΩ‰ª§‰πãÂâçÔºåÂêëÁâπÊ¥õ Tello UDP Á´ØÂè£ 8889 ÂèëÈÄÅ‚Äúcommand‚Äù ÂëΩ‰ª§‰ª•ÂêØÂä®ÁâπÊ¥õ Tello ÁöÑ SDK Ê®°Âºè„ÄÇ Êé•Êî∂ÁâπÊ¥õ Tello Áä∂ÊÄÅTello IPÔºö192.168.10.1 - &gt;&gt; PC / Mac / Mobile UDP ServerÔºö0.0.0.0 UDP PORTÔºö8890 Â§áÊ≥®3ÔºöÂú® PCÔºåMac ÊàñÁßªÂä®ËÆæÂ§á‰∏äÂª∫Á´ã UDP ÊúçÂä°Âô®ÔºåÈÄöËøá UDP Á´ØÂè£ 8890 ‰ªé IP 0.0.0.0Êî∂Âê¨Ê∂àÊÅØ„ÄÇÂ¶ÇÊûúÊú™ËøõË°åÂ§áÊ≥®1Âíå2ÁöÑÊìç‰ΩúÔºåËØ∑ÂÖàÂÆåÊàê„ÄÇ Êé•Êî∂ÁâπÊ¥õ Tello ËßÜÈ¢ëÊµÅTello IPÔºö192.168.10.1 - &gt;&gt; PC / Mac / Mobile UDP ServerÔºö0.0.0.0 UDP PORTÔºö11111 Â§áÊ≥®4ÔºöÂú® PCÔºåMac ÊàñÁßªÂä®ËÆæÂ§á‰∏äËÆæÁΩÆ UDP ÊúçÂä°Âô®ÔºåÈÄöËøáÊúçÂä°Âô® UDP Á´ØÂè£ 11111 ‰ªéIP 0.0.0.0 Êî∂Âê¨Ê∂àÊÅØ„ÄÇ Â§áÊ≥®5ÔºöÂÖàËøõË°åÂ§áÊ≥®1Âíå2ÁöÑÊìç‰Ωú„ÄÇÁÑ∂ÂêéÂêëÁâπÊ¥õ Tello UDP Á´ØÂè£ 8889 ÂèëÈÄÅ ‚Äústreamon‚Äù ÂëΩ‰ª§ÔºåÂºÄÂßãÊé•ÂèóÁâπÊ¥õ Tello ËßÜÈ¢ëÊµÅ„ÄÇ ËÆæÁΩÆÂëΩ‰ª§ÊéßÂà∂ÂëΩ‰ª§ËØªÂèñÂëΩ‰ª§TelloÁä∂ÊÄÅ ÂëΩ‰ª§ ÊèèËø∞ ÂèØËÉΩÁöÑÂìçÂ∫î speed xx ËÆæÁΩÆÂΩìÂâçÈÄüÂ∫¶‰∏∫xx(1-100) okerror rc a b c d ËÆæÁΩÆÈÅ•ÊéßÂô®ÁöÑ4‰∏™ÈÄöÈÅìÊùÜÈáèÔºåa:Ê®™ÊªöÔºà-100~100Ôºâb:‰øØ‰ª∞Ôºà-100~100Ôºâc:Ê≤πÈó®Ôºà~100-100Ôºâd:ÊóãËΩ¨Ôºà-100~100Ôºâ okerror wifi ssid pass ËÆæÁΩÆ WiFi SSID ÂØÜÁ†Å okerror ÂëΩ‰ª§ ÊèèËø∞ ÂèØËÉΩÁöÑÂìçÂ∫î command ËøõÂÖ•SDKÂëΩ‰ª§Ê®°Âºè okerror takeoff Ëá™Âä®Ëµ∑È£û okerror land Ëá™Âä®ÈôçËêΩ okerror streamon ÊâìÂºÄËßÜÈ¢ëÊµÅ okerror streamoff ÂÖ≥Èó≠ËßÜÈ¢ëÊµÅ okerror emergency ÂÅúÊ≠¢ÁîµÊú∫ËΩ¨Âä® okerror up x Âêë‰∏äÈ£û x ÂéòÁ±≥x = 20-500 okerror down x Âêë‰∏ãÈ£û x ÂéòÁ±≥x = 20-500 okerror left x ÂêëÂ∑¶È£û x ÂéòÁ±≥x = 20-500 okerror right x ÂêëÂè≥È£ûx ÂéòÁ±≥x = 20-500 okerror forward xx ÂêëÂâçÈ£û x ÂéòÁ±≥x = 20-500 okerror back xx ÂêëÂêéÈ£û x ÂéòÁ±≥x = 20-500 okerror cw xx È°∫Êó∂ÈíàÊóãËΩ¨x¬∞x = 1-3600 okerror ccw ÈÄÜÊó∂ÈíàÊóãËΩ¨ x¬∞x = 1-3600 okerror flip x Êúù x ÊñπÂêëÁøªÊªöl = (left)r =(right)f = (forwardb = (back) okerror go x y z speed ‰ª•ËÆæÁΩÆÈÄüÂ∫¶Ôºàcm/sÔºâÈ£ûÂæÄÂùêÊ†áÔºàx,y,zÔºâx: 20-500y: 20-500z: 20-500speed: 10-100 okerror curve x1 y1 z1 x2 y2 z2 speed ‰ª•ËÆæÁΩÆÈÄüÂ∫¶Ôºà cm/s ÔºâÈ£ûÂºßÁ∫øÔºåÁªèËøáÔºàx1,y1,z1ÔºâÂà∞Ôºàx2,y2,z2ÔºâÂ¶ÇÊûúÂºßÁ∫øÂçäÂæÑ‰∏çÂú® 0.5-10 Á±≥ËåÉÂõ¥ÂÜÖÔºåÂàôËøîÂõûÁõ∏Â∫îÊèêÈÜí„ÄÇx1, x2: -500 - 500y1, y2: -500 - 500z1, z2: -500 - 500speed: 10-60x„ÄÅy„ÄÅz ‰∏çËÉΩÂêåÊó∂Âú®-20 ~ 20 ‰πãÈó¥ okerror speed? Ëé∑ÂèñÂΩìÂâçÈÄüÂ∫¶ xx battery? Ëé∑ÂèñÁîµÈáè‰ø°ÊÅØ xx(0~100) ÂëΩ‰ª§ ÊèèËø∞ ÂèØËÉΩÁöÑÂìçÂ∫î speed? Ëé∑ÂèñÂΩìÂâçËÆæÁΩÆÈÄüÂ∫¶Ôºàcm/sÔºâ xx = (10-100) battery? Ëé∑ÂèñÂΩìÂâçÁîµÊ±†Ââ©‰ΩôÁîµÈáèÁöÑÁôæÂàÜÊØîÂÄº xx = (0-100) time? Ëé∑ÂèñÁîµÊú∫ËøêËΩ¨Êó∂Èó¥Êó∂Èó¥ÔºàsÔºâ x height? Ëé∑ÂèñÁõ∏ÂØπÈ´òÂ∫¶ (cm) x: 10-3000 temp? Ëé∑Âèñ‰∏ªÊùøÊúÄÈ´òÂíåÊúÄ‰ΩéÊ∏©Â∫¶(‚ÑÉ) x: 0-90 attitude? Ëé∑Âèñ IMU ‰∏âËΩ¥ÂßøÊÄÅÊï∞ÊçÆ pitch roll yawpitch=Ôºà-89¬∞- 89¬∞Ôºâroll=Ôºà-179¬∞- 179¬∞Ôºâyaw=Ôºà-179¬∞- 179¬∞Ôºâ baro? Ëé∑ÂèñÊ∞îÂéãËÆ°È´òÂ∫¶(m) x acceleration? Ëé∑Âèñ IMU ‰∏âËΩ¥Âä†ÈÄüÂ∫¶Êï∞ÊçÆ(0.001g) x y z tof? Ëé∑Âèñ ToF ÁöÑÈ´òÂ∫¶ÂÄº(cm) x: 10-400 &amp; 6553ËøîÂõû 6553 ÊÑèÂë≥ÁùÄÊµãÈáèÂÄºË∂ÖËøáToF ÈáèÁ®ã„ÄÇ wifi? Ëé∑Âæó Wi-Fi ‰ø°Âô™ÊØî snr Êï∞ÊçÆÁ±ªÂûãÔºöÂ≠óÁ¨¶‰∏≤ ‰æãÂ¶Ç‚Äúpitch:%d;roll:%d;yaw:%d;vgx:%d;vgy%d;vgz:%d;templ:%d;temph:%d;tof:%d;h:%d;bat:%d;baro:%f;\r \n‚ÄùËØ¥ÊòéÔºö pitchÔºö‰øØ‰ª∞ËßíÂ∫¶ÔºåÂ∫¶Êï∞ rollÔºöÊ®™ÊªöËßíÂ∫¶ÔºåÂ∫¶Êï∞ yawÔºöÂÅèËà™ÂÅèËà™ÔºåÂ∫¶Êï∞ vgxÔºöx ËΩ¥ÈÄüÂ∫¶Ôºå vgyÔºöy ËΩ¥ÈÄüÂ∫¶Ôºå vgzÔºöz ËΩ¥ÈÄüÂ∫¶Ôºå templÔºö‰∏ªÊùøÊúÄ‰ΩéÊ∏©Â∫¶ÔºåÊëÑÊ∞èÂ∫¶ temphÔºö‰∏ªÊùøÊúÄÈ´òÊ∏©Â∫¶ÔºåÊëÑÊ∞èÂ∫¶ tofÔºöToF Ë∑ùÁ¶ªÔºåÂéòÁ±≥ hÔºöÁõ∏ÂØπËµ∑È£ûÁÇπÈ´òÂ∫¶ÔºåÂéòÁ±≥ batÔºöÂΩìÂâçÁîµÈáèÁôæÂàÜÊØîÔºåÔºÖ baroÔºöÊ∞îÂéãËÆ°ÊµãÈáèÈ´òÂ∫¶ÔºåÁ±≥ time: ÁîµÊú∫ËøêËΩ¨Êó∂Èó¥ÔºåÁßí agx: x ËΩ¥Âä†ÈÄüÂ∫¶ agy: y ËΩ¥Âä†ÈÄüÂ∫¶ agz: z ËΩ¥Âä†ÈÄüÂ∫¶,]]></content>
      <categories>
        <category>IoT</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Next‰∏ªÈ¢ò‰∏™ÊÄßÂåñ]]></title>
    <url>%2F2019%2F01%2F02%2FNext%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%2F</url>
    <content type="text"><![CDATA[ËøôÁØáÂçöÂÆ¢ÊòØÂØπËøáÂéªÂá†Â§©Êù•ÊäòËÖæNext‰∏ªÈ¢òÁªèÈ™åÁöÑ‰∏ÄÊ¨°Â§ßÊÄªÁªì„ÄÇÊú¨Êù•Áî®ÁöÑÊòØ yilia‰∏ªÈ¢òÔºåÂèØÊòØ‰∏çÁü•ÈÅìÊÄé‰πàÁùÄËÑëÊäΩ‰∫ÜÊÉ≥Êç¢‰∏Ä‰∏™‰∏ªÈ¢òÁé©Áé©ÔºåËøô‰∏ãÂ•Ω‰∫ÜÔºå‰∏ÄÊäòËÖæÊäòËÖæ‰∫ÜÂ•ΩÂá†Â§©¬∑¬∑¬∑¬∑ÂçöÂÆ¢ÂòõÔºåÂΩìÁÑ∂ÊòØÂÜÖÂÆπÊúÄÈáçË¶Å¬∑¬∑¬∑‰∏çËøáÈ´òÈ¢úÂÄºÊõ¥ËÉΩËÆ©ÊàëÊúâÂä®ÂäõÂìàÂìàÂìàüòÇÊ≤°ÂäûÊ≥ïÊàëÂ∞±ÂñúÊ¨¢Ëøô‰∫õËä±ÈáåËÉ°Âì®ÁöÑ‰∏úË•ø ‰∏ªÈ¢òÁæéÂåñ‰ª•‰∏ãÊòØÊàëÁî®Âà∞ÁöÑNext‰∏ªÈ¢òÁöÑ‰∏™ÊÄßÂåñÂèÇËÄÉÈìæÊé•„ÄÇËøô‰∏§‰∏™Â∑≤ÁªèÂæàÂÖ®Èù¢Âï¶ÔºåÊõ¥Â§öÁöÑÁæéÂåñÊñπÊ°àÂ∞±ÁúãËøô‰∏§‰∏™Âêß„ÄÇ ÊâìÈÄ†‰∏™ÊÄßË∂ÖËµûÂçöÂÆ¢Hexo+NexT+GitHubPagesÁöÑË∂ÖÊ∑±Â∫¶‰ºòÂåñ hexoÁöÑnext‰∏ªÈ¢ò‰∏™ÊÄßÂåñÊïôÁ®ãÔºöÊâìÈÄ†ÁÇ´ÈÖ∑ÁΩëÁ´ô Â¢ûÂä†ÁúãÊùøÂ®òÁü•ÈÅìÁúãÊùøÂ®òÊòØ‰ªÄ‰πàÊÑèÊÄùÁöÑ‰º∞ËÆ°Â∑Æ‰∏çÂ§öÈÉΩÂæàÂÆÖÂêß„ÄÇÂΩìÁÑ∂ÁúãÊùøÂ®ò‰∏ç‰∏ÄÂÆöÈùûÂæóÊòØ‰∫∫ÔºåÊàëÁöÑÂ∞±ÊòØ‰∏ÄÁõ¥Â∞èÈªëÁå´ÂìàÂìàÂìàËØ¶ÊÉÖËØ∑Áúã‰∏ãÈù¢ÁöÑÈìæÊé•Áî®Live2DËÆ©ÁúãÊùøÂñµÂÖ•‰Ωè‰Ω†ÁöÑHexoÂçöÂÆ¢Âêß(^o^)/~ Â¢ûÂä†Èº†Ê†áÁÇπÂáªÁâπÊïàÊµÆÁé∞ÂΩ©Ëâ≤Ê°ÉÂøÉ‰ª•‰∏ãÊòØÊâÄÈúÄjs‰ª£Á†ÅÔºöÊú™ÂéãÁº©123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051! function(e, t, a) &#123; function n() &#123; c( ".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"), o(), r() &#125; function r() &#123; for (var e = 0; e &lt; d.length; e++) d[e].alpha &lt;= 0 ? (t.body.removeChild(d[e].el), d.splice(e, 1)) : (d[e].y--, d[e].scale += .004, d[e].alpha -= .013, d[e].el.style.cssText = "left:" + d[e].x + "px;top:" + d[e].y + "px;opacity:" + d[e].alpha + ";transform:scale(" + d[e].scale + "," + d[e].scale + ") rotate(45deg);background:" + d[e].color + ";z-index:99999"); requestAnimationFrame(r) &#125; function o() &#123; var t = "function" == typeof e.onclick &amp;&amp; e.onclick; e.onclick = function(e) &#123; t &amp;&amp; t(), i(e) &#125; &#125; function i(e) &#123; var a = t.createElement("div"); a.className = "heart", d.push(&#123; el: a, x: e.clientX - 5, y: e.clientY - 5, scale: 1, alpha: 1, color: s() &#125;), t.body.appendChild(a) &#125; function c(e) &#123; var a = t.createElement("style"); a.type = "text/css"; try &#123; a.appendChild(t.createTextNode(e)) &#125; catch (t) &#123; a.styleSheet.cssText = e &#125; t.getElementsByTagName("head")[0].appendChild(a) &#125; function s() &#123; return "rgb(" + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + ")" &#125; var d = []; e.requestAnimationFrame = function() &#123; return e.requestAnimationFrame || e.webkitRequestAnimationFrame || e.mozRequestAnimationFrame || e.oRequestAnimationFrame || e.msRequestAnimationFrame || function(e) &#123; setTimeout(e, 1e3 / 60) &#125; &#125;(), n()&#125;(window, document); ÂéãÁº©ÂêéÔºö1!function(e,t,a)&#123;function n()&#123;c(".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText="left:"+d[e].x+"px;top:"+d[e].y+"px;opacity:"+d[e].alpha+";transform:scale("+d[e].scale+","+d[e].scale+") rotate(45deg);background:"+d[e].color+";z-index:99999");requestAnimationFrame(r)&#125;function o()&#123;var t="function"==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement("div");a.className="heart",d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement("style");a.type="text/css";try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName("head")[0].appendChild(a)&#125;function s()&#123;return"rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); Êñ∞Âª∫love.jsÊñá‰ª∂Âπ∂‰∏îÂ∞Ü‰∏äÈù¢ÁöÑ‰ª£Á†ÅÂ§çÂà∂ËøõÂéª„ÄÇÁÑ∂ÂêéÂ∞Ülove.jsÊñá‰ª∂ÊîæÂà∞Ë∑ØÂæÑ /themes/next/source/js/src ‰∏ãÔºåÁÑ∂ÂêéÊâìÂºÄ\themes\next\layout\_layout.swigÊñá‰ª∂,Âú®Êú´Â∞æÊ∑ªÂä†‰ª•‰∏ã‰ª£Á†ÅÔºö12&lt;!-- Èº†Ê†áÊ°ÉÂøÉÂä®Áîª --&gt;&lt;script type="text/javascript" src="/js/src/love.js"&gt;&lt;/script&gt; ÊµÆÁé∞EmojiÂíåÂÖ∂‰ªñÊñáÂ≠óÂéü‰ΩúËÄÖÂçöÂÆ¢ ‰ª•‰∏ãÊòØÊàëÁ®çÂæÆÊîπ‰∫Ü‰∏ãËá™Â∑±Áî®ÁöÑÁâàÊú¨üò≥ÔºåÂ¶ÇÊûú‰Ω†ÊàêÂäüÂä†ÂÖ•‰∫Ü‰∏äÈù¢ÁöÑÊ°ÉÂøÉÁâπÊïàÔºåÈÇ£‰πàËøô‰∏™Â∞±ÊòØÊç¢‰∏ã‰ª£Á†ÅËÄåÂ∑≤„ÄÇ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071onload = function() &#123; var click_cnt = 0; var $html = document.getElementsByTagName("html")[0]; var $body = document.getElementsByTagName("body")[0]; $html.onclick = function(e) &#123; var $elem = document.createElement("b"); $elem.style.color = "#E94F06"; $elem.style.zIndex = 9999; $elem.style.position = "absolute"; $elem.style.select = "none"; var x = e.pageX; var y = e.pageY; $elem.style.left = (x - 10) + "px"; $elem.style.top = (y - 20) + "px"; clearInterval(anim); switch (++click_cnt) &#123; case 10: $elem.innerText = "ü§©"; break; case 20: $elem.innerText = "üòé"; break; case 30: $elem.innerText = "üòè"; break; case 40: $elem.innerText = "üòê"; break; case 50: $elem.innerText = "üòë"; break; case 60: $elem.innerText = "üòí"; break; case 70: $elem.innerText = "‡´Æ( ·µíÃåÁöø·µíÃå )·Éê"; break; case 80: $elem.innerText = "(‚ïØ¬∞Âè£¬∞)‚ïØ(‚î¥‚Äî‚î¥"; break; case 90: $elem.innerText = "Âà´ÁÇπÂï¶"; break; case 100: case 101: case 102: case 103: case 104: case 105: $elem.innerText = "‰ºëÊÅØ‰ºöÂÑø"; break; default: $elem.innerText = "üéà"; break; &#125; $elem.style.fontSize = Math.random() * 10 + 8 + "px"; var increase = 0; var anim; setTimeout(function() &#123; anim = setInterval(function() &#123; if (++increase == 150) &#123; clearInterval(anim); $body.removeChild($elem); &#125; $elem.style.top = y - 20 - increase + "px"; $elem.style.opacity = (150 - increase) / 120; &#125;, 8); &#125;, 70); $body.appendChild($elem); &#125;;&#125;; ÂéãÁº©Âêé 1onload=function()&#123;var click_cnt=0;var $html=document.getElementsByTagName("html")[0];var $body=document.getElementsByTagName("body")[0];$html.onclick=function(e)&#123;var $elem=document.createElement("b");$elem.style.color="#E94F06";$elem.style.zIndex=9999;$elem.style.position="absolute";$elem.style.select="none";var x=e.pageX;var y=e.pageY;$elem.style.left=(x-10)+"px";$elem.style.top=(y-20)+"px";clearInterval(anim);switch(++click_cnt)&#123;case 10:$elem.innerText="ü§©";break;case 20:$elem.innerText="üòé";break;case 30:$elem.innerText="üòè";break;case 40:$elem.innerText="üòê";break;case 50:$elem.innerText="üòë";break;case 60:$elem.innerText="üòí";break;case 70:$elem.innerText="‡´Æ( ·µíÃåÁöø·µíÃå )·Éê";break;case 80:$elem.innerText="(‚ïØ¬∞Âè£¬∞)‚ïØ(‚î¥‚Äî‚î¥";break;case 90:$elem.innerText="Âà´ÁÇπÂï¶";break;case 100:case 101:case 102:case 103:case 104:case 105:$elem.innerText="‰ºëÊÅØ‰ºöÂÑø";break;default:$elem.innerText="üéà";break&#125;$elem.style.fontSize=Math.random()*10+8+"px";var increase=0;var anim;setTimeout(function()&#123;anim=setInterval(function()&#123;if(++increase==150)&#123;clearInterval(anim);$body.removeChild($elem)&#125;$elem.style.top=y-20-increase+"px";$elem.style.opacity=(150-increase)/120&#125;,8)&#125;,70);$body.appendChild($elem)&#125;&#125;; Âä®ÊÄÅÊ†áÁ≠æÊ†èHexo NexT‰∏ªÈ¢ò‰∏≠Ê∑ªÂä†ÁΩëÈ°µÊ†áÈ¢òÂ¥©Ê∫ÉÊ¨∫È™óÊêûÊÄ™ÁâπÊïà ÁÆóÊòØ‰∏Ä‰∏™Â∞èÁªÜËäÇÁöÑÁæéÂåñÂêßÔºåÂÆÉÂèØ‰ª•Âú®‰Ω†ÂàáÊç¢È°µÈù¢ÁöÑÊó∂ÂÄôÊõ¥ÊîπÊ†áÁ≠æÊ†èÁöÑÊñáÂ≠ó 12345678910111213141516var OriginTitle = document.title;var titleTime;document.addEventListener('visibilitychange', function () &#123; if (document.hidden) &#123; // $('[rel="icon"]').attr('href', "/images/TEP.ico"); //Â¶ÇÊûúÈúÄË¶ÅÂõæÊ†á‰∏ÄËµ∑ÂèòÔºåÈÇ£‰πàÂ∞±Â∞ÜËøôË°åÂèñÊ∂àÊ≥®ÈáäÂπ∂ÈÄâÊã©Ê≠£Á°ÆÁöÑÂõæÁâáË∑ØÂæÑ document.title = ' üòâ‰∏çÁúã‰∫ÜÂ∞±ÂÖ≥Êéâ ~';//Ë¶ÅÊòæÁ§∫ÁöÑ‰ø°ÊÅØ clearTimeout(titleTime); &#125; else &#123; //$('[rel="icon"]').attr('href', "/favicon.ico"); Âêå‰∏ä document.title = 'üòçÊ¨¢ËøéÂõûÊù•~' + OriginTitle; titleTime = setTimeout(function () &#123; document.title = OriginTitle; &#125;, 2000); &#125;&#125;); Êñ∞Âª∫crash_cheat.jsÊñá‰ª∂Âπ∂‰∏îÂ∞Ü‰∏äÈù¢ÁöÑ‰ª£Á†ÅÂ§çÂà∂ËøõÂéª„ÄÇÁÑ∂ÂêéÂ∞Ülove.jsÊñá‰ª∂ÊîæÂà∞Ë∑ØÂæÑ /themes/next/source/js/src ‰∏ãÔºåÁÑ∂ÂêéÊâìÂºÄ\themes\next\layout\_layout.swigÊñá‰ª∂,Âú®Êú´Â∞æÊ∑ªÂä†‰ª•‰∏ã‰ª£Á†ÅÔºö12&lt;!--Â¥©Ê∫ÉÊ¨∫È™ó--&gt;&lt;script type="text/javascript" src="/js/src/crash_cheat.js"&gt;&lt;/script&gt; ÊéíÁâàÁõ∏ÂÖ≥tabsÁöÑ‰ΩøÁî®tabsÁöÑËØ¶ÁªÜÁî®Ê≥ï ÊàëËÆ§‰∏∫tabÂèØ‰ª•Ëµ∑Âà∞ËäÇÁúÅÈ°µÈù¢Á©∫Èó¥ÁöÑ‰ΩúÁî®ÔºåÊää‰∏Ä‰∫õÊñáÂ≠óÂàÜÈ°µÊîæÁΩÆÊõ¥Âä†ÁæéËßÇtabÊ†áÈ¢òÊµãËØï 1tabÊ†áÈ¢òÊµãËØï 2tabÊ†áÈ¢òÊµãËØï 3ËøôÊòØÁ¨¨‰∏ÄÈ°µÁ¨¨2È°µÁ¨¨3È°µ Ëøô‰∏™tabÊ†áÁ≠æÂú®Êîæjs‰ª£Á†ÅÂùóÁöÑÊó∂ÂÄôÊúâbugÔºöÈ¶ñÂÖàÂÆÉ‰∏çËÉΩ‰ΩøÁî® ‚Äú ``` ‰ª£Á†ÅÂùó ``` ‚Äù ËøôÊ†∑ÁöÑÊ†áÁ≠æ. ‰∏çÁÑ∂ÂÆÉÂ∞±‰ºöÊòæÁ§∫ undefined„ÄÇËøôÊó∂ÂÄôË¶Å‰ΩøÁî®codeÊ†áÁ≠æ„ÄÇÂè¶Â§ñÂÆÉÂè™ËÉΩÂú®Á¨¨‰∏ÄÈ°µÊèíÂÖ•‰ª£Á†ÅÂùóÔºåÊèíÂÖ•‰πãÂêéÁ¨¨‰∫åÈ°µÂ∞±Áøª‰∏çÂá∫Êù•ÔºåÊÄª‰πãÊèíÂÖ•‰ª£Á†ÅÁöÑÊó∂ÂÄôBUG‰∏ÄÂ†ÜÊ†πÊú¨‰∏çËÉΩÁî®üò≠ labelÁöÑ‰ΩøÁî®labelÊµãËØïlabel‰ΩøÁî®ËØ¶ÊÉÖ Demo:ÊâÄË∞ìÁöÑlabel‰ªéÊïàÊûú‰∏äÁúãÁ•û‰ººËçßÂÖâÁ¨î ÂèçÊ≠£ÊàëÂ∞±ÊòØÂΩìËçßÂÖâÁ¨îÊù•ÂàíÈáçÁÇπÂêß ËøôÊòØDefault ËøôÊòØPrimary ËøôÊòØSuccess ËøôÊòØInfo ËøôÊòØWarning ËøôÊòØDanger ËøôÊòØÊúâÂà†Èô§Á∫øÁöÑDanger Code:1234567ËøôÊòØ&#123;% label default@Default%&#125;ËøôÊòØ&#123;% label primary@Primary%&#125;ËøôÊòØ&#123;% label success@Success%&#125;ËøôÊòØ&#123;% label info@Info%&#125;ËøôÊòØ&#123;% label warning@Warning%&#125;ËøôÊòØ&#123;% label danger@Danger%&#125;ËøôÊòØÊúâÂà†Èô§Á∫øÁöÑ~~&#123;% label danger@Danger%&#125;~~ ButtonÁöÑ‰ΩøÁî®buttonÁöÑËØ¶ÁªÜÁî®Ê≥ïDemo:ÊåâÈíÆÊàë‰πü‰∏çÁªèÂ∏∏Áî®Ôºå‰ΩÜÊòØÂ¶ÇÊûúÂÉèËøôÊ†∑ÂàÜ‰∫´‰∏ãËΩΩÈìæÊé•ÁöÑÊó∂ÂÄôÂèØËÉΩ‰ºöÊúâÁÇπÁî®Âêß ÁÇπÂáª‰ºö‰∏ãËΩΩËÄÅÂ©ÜÔºÅÊÖéÁÇπüòúÁÇπÂáª‰∏ãËΩΩ Â•ΩÂ§öbugÂïä¬∑¬∑¬∑¬∑Â¶ÇÊûúËøôÈáåÂä†‰∏Ä‰∏™noteÊ†áÁ≠æ‰∏ãÈù¢ÁöÑcodeÂíåÊ†áÈ¢òÂ∞±‰∏ç‰ºöÊ∏≤Êüì‰∫ÜÔºåüò∂ Code: 1&#123;% btn https://cooguyan-1252810309.cos.ap-chengdu.myqcloud.com/beaupic/gakki/5267d628b0f0b81855f15b80494d36c2.jpg, ÁÇπÂáª‰∏ãËΩΩ, download fa-lg fa-fw %&#125; noteÁöÑ‰ΩøÁî®noteÊ†áÁ≠æËØ¶ÁªÜÁî®Ê≥ï Demo: ÈªòËÆ§ Áî®Êù•Êõø‰ª£ÂºïÁî®Ê†áÁ≠æËøòË°å ÈªòËÆ§ ÊúâÂõæÊ†áÁöÑÂºïÁî®Ê†áÁ≠æÂêß ÂèØ‰ª•Ë°•ÂÖÖ‰∏Ä‰∫õ‰ø°ÊÅØ PrimaryÔºå‰∏çÁü•ÈÅìÊÄé‰πàÁî® WarningÔºåÂÜô‰∏Ä‰∫õË≠¶Âëä‰ø°ÊÅØ Danger,Â∞±ÊòØËøôÊ†∑ÂæàÂç±Èô©Âêß SuccessÔºåÊàêÂäüÊòØ‰ªÄ‰πàÈ¨ºÂïä Ê≤°ÊúâÂõæÊ†áÁöÑÊ†∑Â≠ê Code:12345678&#123;% note %&#125;ÈªòËÆ§ Áî®Êù•Êõø‰ª£ÂºïÁî®Ê†áÁ≠æËøòË°å&#123;%endnote%&#125;&#123;% note default %&#125; ÈªòËÆ§ ÊúâÂõæÊ†áÁöÑÂºïÁî®Ê†áÁ≠æÂêß&#123;%endnote%&#125;&#123;% note info %&#125;ÂèØ‰ª•Ë°•ÂÖÖ‰∏Ä‰∫õ‰ø°ÊÅØ&#123;%endnote%&#125;&#123;% note primary%&#125;PrimaryÔºå‰∏çÁü•ÈÅìÊÄé‰πàÁî®&#123;%endnote%&#125;&#123;% note warning%&#125;WarningÔºåÂÜô‰∏Ä‰∫õË≠¶Âëä‰ø°ÊÅØ&#123;%endnote%&#125;&#123;% note danger %&#125;Danger,Â∞±ÊòØËøôÊ†∑ÂæàÂç±Èô©Âêß&#123;% endnote%&#125;&#123;% note success%&#125;SuccessÔºåÊàêÂäüÊòØ‰ªÄ‰πàÈ¨ºÂïä&#123;%endnote%&#125;&#123;% note success no-icon%&#125;Ê≤°ÊúâÂõæÊ†áÁöÑÊ†∑Â≠ê&#123;%endnote%&#125; ÂºïÁî®ÊñáÊú¨Â±Ö‰∏≠ÊòæÁ§∫ËØ¶ÁªÜÁî®Ê≥ï Demo: Êàë‰ª¨ËÉΩ‰ΩìÈ™åÁöÑÊúÄÁæéÂ•ΩÁöÑ‰∏úË•øÊòØÁ•ûÁßòÁöÑ‰∫ãÁâ©„ÄÇ‰ªñÊòØÊâÄÊúâÁúüÊ≠£Ëâ∫ÊúØÂíåÁßëÂ≠¶ÁöÑÊù•Ê∫ê„ÄÇ‚Äî‚ÄîÈòøÂ∞î‰ºØÁâπ¬∑Áà±Âõ†ÊñØÂù¶ „ÄäÊàëÁöÑ‰∏ñÁïåËßÇ„ÄãÔºå1930 Â¢ûÂ§öÊàë‰ª¨‰∏çÂä†ÊÄùÁ¥¢Â∞±ËÉΩÂÆåÊàêÁöÑÈáçË¶ÅÂ∑•‰ΩúÁöÑÊï∞ÈáèÔºåÊñáÊòé‰æøÊòØÂ¶ÇÊ≠§ËøõÊ≠•ÁöÑ„ÄÇ‚Äî‚ÄîAlfred North WhiteheadÔºåAn Introduction to MathematicsÔºå1911 ÊàëÂØπ‰∏äÂ∏ùËØ¥Ë•øÁè≠ÁâôËØ≠ÔºåÂØπÂ•≥‰∫∫ËØ¥ÊÑèÂ§ßÂà©ËØ≠ÔºåÂØπÁî∑‰∫∫ËØ¥Ê≥ïËØ≠ÔºåÂØπÊàëÁöÑÈ©¨ËØ¥Âæ∑ËØ≠„ÄÇ‚Äî‚ÄîÊü•ÁêÜ‰∫î‰∏ñÔºåÊ≥ïÂõΩÂõΩÁéãÔºà1337~1380Ôºâ Code:123456789101112&lt;!-- HTMLÊñπÂºè: Áõ¥Êé•Âú® Markdown Êñá‰ª∂‰∏≠ÁºñÂÜô HTML Êù•Ë∞ÉÁî® --&gt;&lt;!-- ÂÖ∂‰∏≠ class="blockquote-center" ÊòØÂøÖÈ°ªÁöÑ --&gt;&lt;blockquote class="blockquote-center"&gt;Êàë‰ª¨ËÉΩ‰ΩìÈ™åÁöÑÊúÄÁæéÂ•ΩÁöÑ‰∏úË•øÊòØÁ•ûÁßòÁöÑ‰∫ãÁâ©„ÄÇ‰ªñÊòØÊâÄÊúâÁúüÊ≠£Ëâ∫ÊúØÂíåÁßëÂ≠¶ÁöÑÊù•Ê∫ê„ÄÇ‚Äî‚ÄîÈòøÂ∞î‰ºØÁâπ¬∑Áà±Âõ†ÊñØÂù¶ „ÄäÊàëÁöÑ‰∏ñÁïåËßÇ„ÄãÔºå1930&lt;/blockquote&gt;&lt;!-- Ê†áÁ≠æ ÊñπÂºèÔºåË¶ÅÊ±ÇÁâàÊú¨Âú®0.4.5Êàñ‰ª•‰∏ä --&gt;&#123;% centerquote %&#125;Â¢ûÂ§öÊàë‰ª¨‰∏çÂä†ÊÄùÁ¥¢Â∞±ËÉΩÂÆåÊàêÁöÑÈáçË¶ÅÂ∑•‰ΩúÁöÑÊï∞ÈáèÔºåÊñáÊòé‰æøÊòØÂ¶ÇÊ≠§ËøõÊ≠•ÁöÑ„ÄÇ‚Äî‚ÄîAlfred North WhiteheadÔºåAn Introduction to MathematicsÔºå1911&#123;% endcenterquote %&#125;&lt;!-- Ê†áÁ≠æÂà´Âêç --&gt;&#123;% cq %&#125;ÊàëÂØπ‰∏äÂ∏ùËØ¥Ë•øÁè≠ÁâôËØ≠ÔºåÂØπÂ•≥‰∫∫ËØ¥ÊÑèÂ§ßÂà©ËØ≠ÔºåÂØπÁî∑‰∫∫ËØ¥Ê≥ïËØ≠ÔºåÂØπÊàëÁöÑÈ©¨ËØ¥Âæ∑ËØ≠„ÄÇ‚Äî‚ÄîÊü•ÁêÜ‰∫î‰∏ñÔºåÊ≥ïÂõΩÂõΩÁéãÔºà1337~1380Ôºâ &#123;% endcq %&#125;]]></content>
      <categories>
        <category>HexoÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RAID(Reduntant Arrays of Inexpensive Disks)]]></title>
    <url>%2F2018%2F12%2F30%2FRAID%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[‰ª•‰∏ãÊòØ„ÄäËÆ°ÁÆóÊú∫ÁªÑÊàê‰∏éËÆæËÆ°„ÄãÁ¨¨ÂõõÁâàÂéüÊñá,ÊàëÊëòÂΩï‰∫ÜÂÖ∂‰∏≠‰∫ÜÂÖ≥‰∫éRAIDÁöÑÈÉ®ÂàÜ Êó†ÂÜó‰Ωô (RAID 0) ‰ªÖ‰ªÖÊääÊï∞ÊçÆÂàÜÂÅöÂà∞Â§ö‰∏™Á£ÅÁõòÔºåÁß∞‰∏∫Êù°Â∏¶ÂåñÔºåËá™Âä®ÊääËÆøÈóÆÂº∫Âà∂ÂàÜÂ∏ÉÂà∞Âá†‰∏™Á£ÅÁõò‰∏ä„ÄÇÂú®‰∏ÄÁªÑÁ£ÅÁõò‰∏äËøõË°åÊù°Â∏¶Âåñ‰ΩøÂæóËøô‰∏ÄÁªÑÁ£ÅÁõòÂØπ‰∫éËΩØ‰ª∂Êù•ËØ¥ÊòØ‰∏Ä‰∏™Â§ßÁ£ÅÁõòÔºå‰ªéÈù¢ÁÆÄÂåñ‰∫ÜÂ≠òÂÇ®ÁÆ°ÁêÜ„ÄÇËÄå‰∏îÂØπÂ§ö‰∏™ÂêåÊó∂ÁöÑËÆøÈóÆÊù•ËØ¥ÊúâÂà©‰∫éÊîπËøõÊÄßËÉΩÔºåÂõ†‰∏∫Â§ö‰∏™Á£ÅÁõòÂèØ‰ª•ÂêåÊó∂Êìç‰Ωú„ÄÇ‰æãÂ¶ÇÔºåËßÜÈ¢ëÁºñËæëÁ≥ªÁªüÁªèÂ∏∏ÂØπÂÆÉ‰ª¨ÁöÑÊï∞ÊçÆËøõË°åÊù°Â∏¶ÂåñÔºå‰∏çÈúÄË¶ÅÂÉèÊï∞ÊçÆÂ∫ìÈÇ£Ê†∑ÂÖ≥ÂøÉÂèØÈù†ÊÄßÈóÆÈ¢ò„ÄÇ RAID 0ÁöÑÁß∞Ë∞ìÊúâ‰∫õ‰∏çÂ¶•ÔºåÂõ†‰∏∫ÂÆÉÊ†πÊú¨Ê≤°ÊúâÂÜó‰Ωô„ÄÇÁÑ∂ËÄåÔºåRAIDÁöÑÁ∫ßÂà´ÈÄöÂ∏∏Áî±Êìç‰ΩúÂëòÂú®ÂàõÂª∫Á≥ªÁªüÊó∂ËÆæÁΩÆÔºåËÄåRAIDOÁªèÂ∏∏Ë¢´Âàó‰∏∫ÂÖ∂‰∏≠‰∏Ä‰∏™ÈÄâÈ°π„ÄÇÂõ†Ê≠§ÔºåRAID 0ÁöÑËØ¥Ê≥ïÂ∞±Ë¢´ÂπøÊ≥õ‰ΩøÁî®‰∫Ü„ÄÇ ÈïúÂÉè(RAID 1) ËøôÁßç‰º†ÁªüÁöÑÂÆπÂøçÁ£ÅÁõòÂ§±ÊïàÁöÑÊñπÊ≥ïÔºåË¢´Áß∞‰∏∫ÈïúÂÉè‚ÄùÊàñËÄÖÂΩ±ÂÉè(shadowing), ‰ΩøÁî®ÊØîRAID O Â§ö‰∏ÄÂÄçÁöÑÁ£ÅÁõòÊï∞„ÄÇÊï∞ÊçÆÂÜô‰∫∫Êüê‰∏™ÁõòÊó∂ÔºåÂêåÊ†∑ÁöÑÊï∞ÊçÆ‰ºöÂÜô‰∫∫ÂÖ∂ÂÜó‰ΩôÁõòÔºåÂõ†Ê≠§ÂßãÁªàÂ≠òÂú®‰ø°ÊÅØÁöÑ‰∏§‰ªΩÂâØÊú¨„ÄÇÂ¶ÇÊûú‰∏Ä‰∏™Á£ÅÁõòÂá∫Áé∞ÊïÖÈöúÔºåÁ≥ªÁªüÂ∞±ËΩ¨ÂêëÂÖ∂‚ÄúÈïúÂÉè‚ÄùËØªÂèñÂÜÖÂÆπ‰ª•Ëé∑ÂæóÊâÄÈúÄ‰ø°ÊÅØ„ÄÇÈïúÂÉèÊòØÊúÄÊòÇË¥µÁöÑRAIDÊñπÊ°àÔºåÂõ†‰∏∫ÂÆÉÈúÄË¶ÅÊúÄÂ§öÁöÑÁ£ÅÁõò„ÄÇ ÈîôËØØÊ£ÄÊµãÂíåÁ∫†ÈîôÁ†Å (RAID 2) RAID 2ÂÄüÁî®‰∫Ü‰∏ªÂ≠òÂ∏∏Áî®ÁöÑÈîôËØØÊ†°È™åÂíåÊÅ¢Â§çÊäÄÊúØ(ÂèÇËßÅÂÖâÁõò‰∏≠ÁöÑÈôÑÂΩïC)„ÄÇRAID 2Â∑≤Áªè‰∏çÂÜç‰ΩøÁî®‰∫ÜÔºåÂõ†Ê≠§Êàë‰ª¨ËøôÈáå‰∏çÂÅö‰ªãÁªç„ÄÇ ‰Ωç‰∫§ÂèâÂ•áÂÅ∂Ê†°È™å (RAID 3) Â¢ûÂä†ÂèØÁî®ÊÄßÁöÑÂºÄÈîÄÂèØ‰ª•ÂáèËá≥I/n,ËøôÈáån‰∏∫‰øùÊä§ÁªÑ‚ÄúÂÜÖÁ£ÅÁõòÁöÑÊï∞ÁõÆ„ÄÇÊàë‰ª¨‰∏çÂÜç‰∏∫ÊØè‰∏™Á£ÅÁõòÂÅö‰∏Ä‰∏™ÂéüÂßãÊï∞ÊçÆÁöÑÂÆåÂÖ®Â§á‰ªΩÔºåÈù¢Âè™ÈúÄË¶ÅÂä†ÂÖ•Ë∂≥Â§üÁöÑÂÜó‰Ωô‰ø°ÊÅØ‰ª•‰æøÂú®Âá∫ÈîôÁöÑÊó∂ÂÄôÊÅ¢Â§ç‰∏¢Â§±ÁöÑ‰ø°ÊÅØ„ÄÇËØªÂÜôÊìç‰ΩúÂú®ÁªÑÂÜÖÊâÄÊúâÁ£ÅÁõò‰∏äËøõË°åÔºå‰∏Ä‰∏™È¢ùÂ§ñÁöÑÁ£ÅÁõòÂ≠òÊúâÊ†°È™å‰ø°ÊÅØ‰ª•Èò≤ÈîôËØØÁöÑÂèëÁîü„ÄÇRAID 3Âú®‰ΩøÁî®Â§ßÊï∞ÊçÆÈõÜÁöÑÂ∫îÁî®( Â¶ÇÂ§öÂ™í‰ΩìÂíåÁßëÂ≠¶ËÆ°ÁÆó)‰∏≠Êä•ÊµÅË°å„ÄÇ Â•áÂÅ∂Ê†°È™å(parity) Â∞±ÊòØËøôÊ†∑ÁöÑ‰∏Ä‰∏™Á≠ñÁï•„ÄÇ‰∏çÁÜüÊÇâÂ•áÂÅ∂Ê†°È™åÁöÑËØªËÄÖÂèØ‰ª•ÊääÂÜó‰ΩôÁ£ÅÁõòÊÉ≥Ë±°Êàê‰øùÂ≠òÊúâÂÖ∂‰ªñÁ£ÅÁõòÊâÄÊúâÊï∞ÊçÆÁöÑÂíå„ÄÇÂΩì.‰∏™Á£ÅÁõòÂá∫ÈîôÊó∂ÔºåÁî®Â•áÂÅ∂Ê†°È™åÁõòÂáèÂéªÊ≠£Â∏∏Á†¥ÁõòÁöÑÊï∞ÊçÆÁöÑÂíå;‰ΩôÊï∞Â∞±ÊòØ‰∏¢Â§±ÁöÑ‰ø°ÊÅØ„ÄÇÂ•áÂÅ∂Ê†°È™åÂ∞±ÊòØÊ®°2‰∏ãÁöÑÊ±ÇÂíå„ÄÇ ‰∏éRAID 1‰∏çÂêåÔºåRAID3 ÂøÖÈ°ªËØªÂæàÂ§öÁ£ÅÁõòÊâçËÉΩÁ°ÆÂÆö‰∏üÂ§±ÁöÑÊï∞ÊçÆ„ÄÇËØ•ÊäÄÊúØËÉåÂêéÁöÑÂÅáËÆæÂ∞±ÊòØÁî®Êõ¥ÈïøÁöÑÊó∂Èó¥Êù•ÊÅ¢Â§çÈîôËØØËÄåÁî®Êõ¥Â∞ëÁöÑÂÜó‰ΩôÂ≠òÂÇ®ÂæóÂà∞‰∏™Â•ΩÁöÑÂπ≥Ë°°„ÄÇ Âùó‰∫§ÂèâÂ•áÂÅ∂Ê†°È™å (RAID 4) RAID 4‰ΩøÁî®ÂêåRAID3Êï∞ÁõÆÊØîÁéá- -Ê†∑Â§ßÁöÑÊï∞ÊçÆÁ£ÅÁõòÂíåÊ†°È™åÁõò,‰ΩÜÊòØËÆøÈóÆÊï∞ÊçÆÁöÑÊñπÂºè‰∏çÂêå„ÄÇÂ•áÂÅ∂Ê†°È™åÁ†Å‰ª•Âùó‰∏∫Âçï‰ΩçÂ≠òÂÇ®ÔºåÂíå-ÁªÑÊï∞ÊçÆÂùóÁõ∏ÂÖ≥„ÄÇ Âú®RAID 3‰∏≠ÔºåÊØèÊ¨°ËÆøÂêëÈÉΩÁî®Âà∞ÊâÄÊúâÁ£ÅÁõò„ÄÇÁÑ∂Èù¢ÔºåÊüê‰∫õÂ∫îÁî®ÂÅèÈáç‰∫éËæÉÂ∞èÁöÑÊï∞ÊçÆËÆøÈóÆÔºåÂÖÅËÆ∏Âπ∂Ë°åÂú∞ÂèëÁîüÂ§ö‰∏™Áã¨Á´ãËÆøÈóÆ„ÄÇËøôÂ∞±ÊòØÂèëÊòéRAID4 - RAID 6ÁöÑÁõÆÁöÑ„ÄÇÁî±‰∫éËØªÊìç‰ΩúÈúÄË¶ÅÊ†°È™åÊØè‰∏™ÊâáÂå∫ÁöÑÈîôËØØÊ£ÄÊµã‰ø°ÊÅØÊù•Âà§Êñ≠Êï∞ÊçÆÊ≠£Á°Æ‰∏éÂê¶ÔºåÂè™Ë¶ÅÂ∞ëÈáèÁöÑËÆøÈóÆÊï∞ÊçÆ‰ªç‰∏∫Âêå‰∏Ä‰∏™ÊâáÂå∫ÔºåÂêÑÁ£ÅÁõò‰∏äËøô‰∫õ‚ÄúÂ∞è)Êï∞ÊçÆÈáèÁöÑËØªÊìç‰Ωú‚ÄùÂ∞±ÂèØ‰ª•Áã¨Á´ãÂú∞ËøõË°å„ÄÇÂú®RAIDÁéØÂ¢É‰∏≠ÔºåÂ∞èÊï∞ÊçÆËÆøÂêëÂú®‰øùÊä§ÁªÑ‰∏≠ÁöÑ‰∏Ä‰∏™Á£ÅÁõòÂèëÁîüÔºåËÄåÂ§ßÊï∞ÊçÆÈáèËÆøÈóÆÈúÄË¶ÅÁî®Âà∞‰øùÊä§ÁªÑ‰∏≠ÁöÑÊâÄÊúâÁ£ÅÁõò„ÄÇ ÂÜôÊìç‰ΩúÊòØÂè¶Â§ñ‰∏™ÈóÆÈ¢ò„ÄÇÁúã‰∏äÂéª‰ºº‰πéÊØè-Ê¨°Â∞èÊï∞ÊçÆÈáèÁöÑÂÜôÊìç‰ΩúÈÉΩÈúÄË¶ÅËÆøÈóÆÂÖ∂‰ªñÁ£ÅÁõò‰ø°ÊÅØÔºå‰ΩøÁî®Ëøô‰∫õ‰ø°ÊÅØËÆ°ÁÆóÊñ∞ÁöÑÂ•áÂÅ∂Ê†°È™åÂÄºÔºåÂ¶ÇÂõæÊâÄÁ§∫„ÄÇ‰∏ÄÊ¨°‚ÄúÂ∞èÊï∞ÊçÆÈáèÁöÑÂÜôÊìç‰Ωú‚ÄùÈúÄË¶ÅËØªÂèñÊóßÊï∞ÊçÆÂíåÊóßÂ•áÂÅ∂Ê†°È™åÔºåÊ∑ªÂä†Êñ∞‰ø°ÊÅØÔºåÊé•ÁùÄÊääÊñ∞ÁöÑÂ•áÂÅ∂Ê†°È™åÂÜôÂÖ•Ê†°È™åÁõòÔºåÊääÊñ∞ÁöÑÊï∞ÊçÆÂÜôÂÖ•Êï∞ÊçÆÁõò„ÄÇ Â∞èÊï∞ÊçÆÈáèÂÜôÊõ¥Êñ∞Âú®RAID 3 Âíå RAID 4 ‰∏äÁöÑÊØîËæÉ ÂØπÂ∞èÊï∞ÊçÆÈáèÂÜôÊìç‰ΩúÁöÑ‰ºòÂåñÂáèÂ∞ë‰∫ÜÁ£ÅÁõòËÆøÈóÆÁöÑÊï∞ÈáèÔºå‰πüÂáèÂ∞ë‰∫ÜÂç†Áî®Á£ÅÁõòÁ©∫Èó¥ÁöÑÊï∞Èáè„ÄÇÊú¨ÂõæÂÅáËÆæÊúâ4ÂùóÊï∞ÊçÆÂíå1ÂùóÊ†°È™åÁ†Å„ÄÇÂõæÂ∑¶‰æßÁöÑRAID 3Ê†°È™åËÆ°ÁÆóÂú®Âä†‰∫∫ÂùóD0‚Äô‰πãÂâçË¶ÅËØªÊï∞ÊçÆÂùóD1„ÄÅD2ÂíåD3ÊâçËÉΩËÆ°ÁÆóÊñ∞Ê†°È™åÁ†ÅP‚Äô„ÄÇ(ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÊñ∞Êï∞ÊçÆD0‚ÄôÁõ¥Êé•Êù•Ëá™CPU,ÊâÄ‰ª•‰∏çÈúÄË¶ÅËØªÁ£ÅÁõòÊù•Ëé∑Âèñ„ÄÇ)ÂõæÂè≥‰æßÁöÑRAID 4‰ºòÂåñÊñπÊ≥ïÊòØËØªÂèñÊóßÂÄºDOÂπ∂‰∏éÊñ∞ÂÄºDO‚ÄôÊØîËæÉÁúãÊòØÂê¶ÊîπÂèò„ÄÇÁÑ∂ÂêéËØªÂèñÊóßÊ†°È™åÁ†ÅP,‰øÆÊîπÂØπÂ∫îÁöÑ‰Ωç,ÂΩ¢ÊàêÊñ∞Ê†°È™åÁ†ÅP‚Äô„ÄÇ‰ΩøÁî®ÊàñÈÄªËæëÊìç‰ΩúÂç≥ÂèØÂÆûÁé∞„ÄÇÂõæ‰∏≠Êää‰∏âÊ¨°ËØªÁ£ÅÁõò(D1„ÄÅ D2„ÄÅD3)Âíå‰∏§Ê¨°ÂÜôÁ£ÅÁõò(D0‚Äô, P‚Äô) ÊõøÊç¢‰∏∫‰∏§Ê¨°ËØªÁ£ÅÁõò(DO, P)Âíå‰∏§Ê¨°ÂÜôÁ£ÅÁõò(D0‚Äô, P‚Äô),ÂâçËÄÖËÆøÈóÆ‰∫ÜÊâÄÊúâÁ£ÅÁõòÔºåËÄåÂêéËÄÖ‰ªÖËÆøÈóÆÂÖ∂‰∏≠ÁöÑ‰∏§‰∏™Á£ÅÁõò„ÄÇÈöèÁùÄÊ†°È™åÁªÑÂ§ßÂ∞èÁöÑÂ¢ûÂä†Â∞Ü‰ΩøÂæó‰ºòÂåñÁöÑÊïàÊûúÊõ¥Âä†ÊòéÊòæ„ÄÇRAID 5‰∫¶‰ΩøÁî®ÂêåÊ†∑ÁöÑÊñπÂºè„ÄÇ ÂáèÂ∞èÂºÄÈîÄÁöÑÂÖ≥ÈîÆÂú®‰∫éÊ†°È™åÁ†Å‰∏çËøáÊòØ‰ø°ÊÅØÁöÑ-‰∏Ä‰∏™ÊÄªÂíå;ÈÄöËøáËßÇÂØüÂÜô‰∫∫Êñ∞‰ø°ÊÅØÂêéÂì™‰∫õ‰ΩçÂèëÁîü‰∫ÜÂèòÂåñÔºåÊàë‰ª¨Âè™ÈúÄÊîπÂèòÊ†°È™åÁõò‰∏äÁöÑÂØπÂ∫î‰ΩçÁöÑ‰ø°ÊÅØÂç≥ÂèØ„ÄÇÂõæ6-13ÁöÑÂè≥ÂõæËØ¥Êòé‰∫ÜËØ•ÊñπÊ≥ï„ÄÇÊàë‰ª¨ÂøÖÈ°ª‰ªéË¶ÅÂÜôÁöÑÁ£ÅÁõòËØªÂèñÊóßÊï∞ÊçÆ,Áî®ÊóßÊï∞ÊçÆÂíåÊñ∞Êï∞ÊçÆÊØîËæÉ,ÁúãÂì™‰∫õ‰ΩçÂèëÁîü‰∫ÜÂèòÂåñ„ÄÇËØªÊóßÂ•áÂÅ∂Ê†°È™åÂíå,ÊîπÂèòÂØπÂ∫îÁöÑ‰ΩçÔºåÁÑ∂ÂêéÂÜô‰∫∫Êñ∞Êï∞ÊçÆ‰ª•ÂèäÊñ∞ÁöÑÊ†°È™åÂíå„ÄÇËøôÊ†∑Ôºå‰∏ÄÊ¨°Â∞èÊï∞ÊçÆÈáèÁöÑÂÜôÊìç‰ΩúÂåÖÂê´ÂØπ‰∏§‰∏™Á£ÅÁõòÁöÑ4Ê¨°ËÆøÈóÆÔºåËÄå‰∏çÊòØËÆøÈóÆÊâÄÊúâÁöÑÁ£ÅÁõò„ÄÇËøôÁßçÁªÑÁªáÁªìÊûÑÂ∞±ÊòØRAID 4 ÂàÜÂ∏ÉÂºèÂùó‰∫§ÂèâÂ•áÂÅ∂Ê†°È™åÔºàRAID 5ÔºâRAID 4 ÊúâÊïàÁöÑÊîØÊåÅ‰∫ÜÂ§ßÊï∞ÊçÆÈáèËØª„ÄÅÂ§ßÊï∞ÊçÆÈáèÂÜôÂíåÂ∞èÊï∞ÊçÆÈáèËØª„ÄÅÂ∞èÊï∞ÊçÆÈáèÂÜôÁöÑÊ∑∑ÂêàÊìç‰Ωú„ÄÇÂÆÉÁöÑÁº∫ÁÇπÊòØÊØèÊ¨°ÂÜôÊìç‰ΩúÈÉΩË¶ÅÊõ¥Êñ∞Ê†°È™åÁõò„ÄÅ‰ªéËÄåÊ†°È™åÁõòÊàê‰∏∫ËøûÁª≠ÂÜôÁöÑÁì∂È¢à„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ê†°È™å-ÂÜôÁì∂È¢àÔºåÊ†°È™å‰ø°ÊÅØÂèØÁî±ÂàÜÂ∏ÉÂà∞ÊâÄÊúâÁõò‰∏äÔºåÊòØÁöÑÂÜôÊìç‰Ωú‰∏çÂ≠òÂú®Âçï‰∏ÄÁöÑÁì∂È¢à„ÄÇËøôÁßçÂàÜÂ∏ÉÂºèÁöÑÂ•áÂÅ∂Ê†°È™åÁªÑÁªáÊñπÂºèÂ∞±ÊòØRAID 5„ÄÇ‰∏ãÂõæÂ±ïÁ§∫‰∫ÜÊï∞ÊçÆÂú®RAID 4 Âíå RAID 5 ‰∏äÊòØÂ¶Ç‰ΩïÂàÜÂ∏ÉÁöÑ„ÄÇÂè≥ÂõæÂ±ïÁ§∫ÁöÑÊòØRAID 5 ÁªÑÁªáÊñπÂºè„ÄÇÂÖ∂‰∏≠Êï∞ÊçÆÂùóÊØèË°åÁöÑÊ†°È™å‰ø°ÊÅØ‰∏çÂÜçÈôêÂÆöÂú®Âçï‰∏™Á£ÅÁõò„ÄÇÂè™Ë¶ÅÊ†°È™åÂùó‰∏çÂú®Áõ∏ÂêåÁöÑÁ£ÅÁõò‰∏äÔºåËøôÁßçÁªÑÁªáÊñπÂºèÂ∞±‰ΩøÂæóÂ§ö ‰∏™ÂÜôÊìç‰ΩúÂèØ‰ª•ÂêåÊó∂ÂèëÁîü„ÄÇ‰æãÂ¶ÇÔºåÂè≥‰æßÁ¨¨1ÂÜôÊìç‰ΩúÊòØÂêëÁ¨¨8ÂùóÂÜôÊï∞ÊçÆÔºåÈúÄÂßîÂêåÊó∂ËÆøÂêëP2‰∏≠ÁöÑÂ•áÂÅ∂ÂÄºÔºå‰ªéËÄåÈúÄË¶ÅËÆøÈóÆÁ¨¨I‰∏™ÂíåÁ¨¨3‰∏™Á£ÅÁõò„ÄÇÂè≥‰æßÁ¨¨2‰∏™ÂÜôÊìç‰ΩúÂØπÁ¨¨5ÂùóËøõË°åÂÜôÔºåÊÑèÂë≥ÁùÄË¶ÅÊõ¥Êñ∞ÂÖ∂Ê†°È™åÂùóP, ,‰ªéËÄåÈúÄË¶ÅËÆøÈóÆÁ¨¨2‰∏™ÂíåÁ¨¨4‰∏™Á£ÅÁõò,ÊâÄ‰ª•ÂÆÉÂèØ‰ª•ÂíåÂÜôÁ¨¨8‰∏™Êï∞ÊçÆÂπ∂ÂèëËøõË°å„ÄÇÂØπ‰∫éÂ∑¶‰æßÁöÑÁªÑÁªáÁªìÊûÑÊù•ËØ¥ÔºåÂêåÊ†∑ÁöÑÂÜôÊìç‰ΩúÂàôÈúÄË¶Å‰øÆÊîπÁ¨¨5‰∏™Á£ÅÁõò‰∏äÁöÑPIÂíåP2,ËøôÂ∞±ÊûÑÊàê‰∫ÜÁì∂È¢à„ÄÇ P+QÂÜó‰Ωô(RAID 6) Âü∫‰∫éÂ•áÂÅ∂Ê†°È™åÁöÑÊú∫Âà∂ÂèØ‰ΩøÁ≥ªÁªüÂÖçÂèóÂçï‰∏™ÂèØËá™Âä®ËØÜÂà´ÁöÑÈîôËØØÁöÑÁ†¥Âùè„ÄÇÂΩìÂçï‰∏™ÈîôËØØÁ∫†Ê≠£Êú∫Âà∂‰∏çË∂≥‰ª•‰øùÊä§Á≥ªÁªüÊó∂ÔºåÂèØÂà©Áî®Â•áÂÅ∂Ê†°È™åÂØπÊï∞ÊçÆÂíåÂè¶-‰∏™Ê†°È™åÁõòÁöÑ‰ø°ÊÅØËøõË°å‰∫åÊ¨°ËÆ°ÁÆó„ÄÇ‰∫åÊ¨°Ê†°È™åÂùóÂèØ‰ΩøÁ≥ªÁªü‰ªé‰∫åÊ¨°ÈîôËØØ‰∏≠ÊÅ¢Â§çËøáÊù•„ÄÇÂõ†Ê≠§ÔºåÂÆÉÁöÑÂ≠òÂÇ®ÂºÄÈîÄÊòØRAID 5 ÁöÑ‰∏§ÂÄç„ÄÇÂõæ6-13‰∏≠ÁöÑÂ∞èÊï∞ÊçÆÊñπÊ≥ïËøòËÉΩÊàêÁ´ãÔºåÂè™ÊòØÁé∞Âú®Êõ¥Êñ∞PÂíåQ‰ø°ÊÅØÈúÄË¶ÅËÆøÂêë6‰∏™ÁõòËÄå‰∏çÊòØËÆøÈóÆ4‰∏™Áõò„ÄÇ RAID Â∞èÁªì RAID 1 ÂíåRAID 5 ÂπøÊ≥õÁî®‰∫éÊúçÂä°Âô®Ôºõ‰∏ÄÈ°π‰º∞ËÆ°ÊòØÊúçÂä°Âô®‰∏≠ 80% ÁöÑÁ£ÅÁõòÈÉΩ‰ΩøÁî®‰∫ÜÊüêÁßçRAID. RAID Á≥ªÁªüÁöÑÂº±ÁÇπÊòØ‰øÆÂ§ç„ÄÇÈ¶ñÂÖàÔºå‰∏∫‰∫ÜÈÅøÂÖçÂú®‰øÆÂ§çÊó∂Êï∞ÊçÆ‰∏çÂèØÁî®ÔºåÈòµÂàóÂøÖÈ°ªËÆæËÆ°‰∏∫‰∏çÂøÖÁæéÈó≠Á≥ªÁªüÂ∞±ËÉΩÊõøÊç¢Âá∫ÈîôÁõò„ÄÇRAID Êã•ÊúâË∂≥Â§üÁöÑÂÜó‰ΩôÊÄß‰ª•‰øùËØÅ‰∏çÈó¥Êñ≠ÁöÑÊìç‰ΩúÔºå‰ΩÜÊòØÁÉ≠‰∫§Êç¢Á£ÅÁõòÂØπÈòµÂàóÂíåÁ£ÅÁõòÊé•Âè£ÁöÑÁâ©ÁêÜÂèäÁîµË∑ØËÆæËÆ°ÊèêÂá∫‰∫ÜË¶ÅÊ±Ç„ÄÇÂÖ∂Ê¨°Ôºå‰øÆÂ§ç‰∏≠ÂèØËÉΩÂá∫Áé∞Âè¶Â§ñÁöÑÈîôËØØÔºåËøôÊ†∑‰øÆÂ§çÊó∂Èó¥‰ºöÂΩ±Âìç‰∏¢Â§±Êï∞ÊçÆÁöÑÊ¶ÇÁéá:‰øÆÂ§çÊó∂Èó¥Ë∂äÈïøÔºåÂè¶‰∏ÄÈîôËØØÂºïËµ∑‰∏¢Â§±Êï∞ÊçÆÁöÑÊ¶ÇÁéáË∂äÂ§ß„ÄÇÊüê‰∫õÁ≥ªÁªüÂπ∂‰∏çÁî®Á≠âÂæÖÊìç‰ΩúÂëòÊù•Ë£Ö‰∏äÂ•ΩÁöÑÁ£ÅÁõòÔºåÂÆÉ‰ª¨ÂåÖÂê´Â∫îÊÄ•Â§áÁî®‚ÄùÔºåËøôÊ†∑‰∏ÄÊó¶Ê£ÄÊµãÂá∫ÈîôËØØÔºåÊï∞ÊçÆÂ∞±ÂèØ‰ª•Á´ãÂç≥ÈáçÂª∫„ÄÇÊìç‰ΩúÂëòÂ∞±ÂèØËΩªÊùæÂú∞Êõ¥Êç¢Âá∫ÈîôÁ£ÅÁõò„ÄÇÊúÄÂêéÔºåÊìç‰Ωú‰∫∫ÂëòÊúÄÁªàÂÜ≥ÂÆöÊííÊéâÂì™‰∏™Á£ÅÁõò„ÄÇÂ¶ÇÂõæ6-3ÊâÄÁ§∫ÔºåÊ≥®ÊÑèÔºåÊìç‰ΩúÂëòÊòØ‰∫∫ÔºåÂõ†Ê≠§‰ªñ‰ª¨ÊúâÊó∂ÂÄô‰ºöÊííÊéâÂ•ΩÁöÑÁ£ÅÁõòÂØºËá¥‰∏çÂèØÊÅ¢Â§çÁöÑÂ±ØÁõòÈîôËØØ„ÄÇ Èô§‰∫ÜËÆæËÆ°ÂèØ‰ª•‰øÆÂ§çÁöÑRAID,ËøòÂ≠òÂú®‰∏Ä‰∫õÂ¶Ç‰ΩïÈöèÁùÄÁ£ÅÁõòÊäÄÊúØÂèòÂåñÁöÑÈóÆÈ¢ò„ÄÇÂ∞ΩÁÆ°Á£ÅÁõòÂéÇÂïÜÊ†áÁß∞‰ªñ‰ª¨ÁöÑ‰∫ßÂìÅÂÖ∑ÊúâÂæàÈ´òÁöÑMTTF,‰ΩÜÊòØËøô‰∫õÊï∞ÊçÆÊòØÂú®ÂÅáËÆæÁöÑÊÉÖÂÜµ‰∏ãÂæóÂà∞ÁöÑ„ÄÇÂ¶ÇÊûúÊüê‰∏™ÁâπÂÆöÁ£ÅÁõòÈòµÂàóËøéÈÅá‰∫ÜÁî±‰∫éÁ©∫Ë∞ÉÁ≥ªÁªüÊïÖÈöú„ÄÅÁ≥üÁ≥ïÁöÑÁ£ÅÁõòÊû∂ËÆæËÆ°„ÄÅÊûÑÂª∫ÊàñËÄÖÂÆâË£ÖÂºïËµ∑ÈúáÂä®Èù¢ÂºïËµ∑Ê∏©Â∫¶Âë®ÊúüÂèòÂåñÔºåÂá∫ÈîôÁéáÂ∞ÜÂ§ßÂ§ßÂ¢ûÂä†ÔºåÂ¢ûÂä†3-6ÂÄç(ËßÅ6.12ËäÇ)„ÄÇRADÂèØÈù†ÊÄßÁöÑËÆ°ÁÆóÂÅáËÆæÂ§ö‰∏™Á£ÅÁõòÂ§±Êïà‰πãÈó¥ÊòØÁã¨Á´ãÁöÑÔºå‰ΩÜÂÆûÈôÖ‰∏äËøô‰∫õÂ§±ÊïàÂèØËÉΩÊòØÁõ∏ÂÖ≥ÁöÑÔºåÂõ†‰∏∫ÁéØÂ¢ÉÂºïËµ∑ÁöÑÊçü‰º§ÂèØËÉΩ‰ºöÂèëÁîüÂú®ÈòµÂàó‰∏≠ÁöÑÊâÄÊúâÁ£ÅÁõò‰∏ä„ÄÇÂè¶‰∏Ä‰∏™ÈóÆÈ¢òÊòØÁ£ÅÁõòÁöÑÂ∏¶ÂÆΩÁõ∏ÂØπÁ£ÅÁõòÁöÑÂÆπÈáèÂèòÂåñÂæóË∂äÊù•Ë∂äÊÖ¢ÔºåÂú®‰∏™RAIDÁ≥ªÁªü‰∏≠‰øÆÂ§ç‰∏Ä‰∏™Á£ÅÁõòÁöÑÊó∂Èó¥ÂèòÂæóË∂äÊù•Ë∂äÈïøÔºåËøô‰∏ÄÁÇπÂèçËøáÊù•Â¢ûÂä†Á¨¨‰∫åÊ¨°ÊïÖÈöúÂá∫Áé∞ÁöÑÊ¶ÇÁéá„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂÅáËÆæÊ≤°ÊúâÂπ≤Êâ∞Êó∂Ôºå‰∏Ä‰∏™1000 GB SATAÁ£ÅÁõòÂèØËÉΩÈúÄË¶ÅËä±Ë¥π3‰∏™Â∞èÊó∂Êù•È°∫Â∫èËØª„ÄÇÂÅáËÆæËøô‰∏™ÊçüÂùèÁöÑRAID ÂæàÂèØËÉΩË¢´Áª¥Áª≠Áî®Êù•Êèê‰æõÊï∞ÊçÆÔºåÈáçÂª∫ËøáÁ®ãÂ∞±‰ºöË¢´Âª∂ÈïøÂæàÂ§ö„ÄÇÈô§‰∫ÜÂ¢ûÂä†Êó∂Èó¥Â§ñÔºåÂè¶-‰∏™ÈóÆÈ¢òÊòØÂú®ÈáçÂª∫ËøáÁ®ã‰∏≠‰∏ÄÊ¨°ËØªÂæàÂ§öÊï∞ÊçÆÂ∞ÜÊÑèÂë≥ËÄÖÂ¢ûÂä†‰∏çÂèØÊÅ¢Â§çÁöÑËØªÂ™í‰ΩìÊïÖÈöúÂèëÁîüÁöÑÊ¶ÇÁéáÔºåËÄå‰∏çÂèØÊÅ¢Â§çÁöÑÊïÖÈöúÂ∞ÜÂØºËá¥Êï∞ÊçÆ‰∏¢Â§±„ÄÇÂÖ∂‰ªñÂÖ≥‰∫éÂêåÊó∂ÂèëÁîüÂ§ö‰∏™ÊïÖÈöúÁöÑÁúãÊ≥ïÊòØÂ¢ûÂä†ÈòµÂàó‰∏≠ÁöÑÁ£ÅÁõòÊï∞ÁõÆ‰ª•Âèä‰ΩøÁî®SATAÁ£ÅÁõòÔºåËøôÊ†∑ÊØî‰º†ÁªüÁöÑÂïÜÁî®Á£ÅÁõòÊÖ¢-‰∫õÔºå‰ΩÜÂÖ∑ÊúâÊõ¥È´òÁöÑÂÆπÈáè„ÄÇ Âõ†Ê≠§ÔºåËøô‰∫õË∂ãÂäøÂØºËá¥ÂØπÈò≤Ê≠¢Á≥ªÁªüÂÖçÂèóÂ§öÈáçÊïÖÈöúÁöÑÁ†îÁ©∂ÂÖ¥Ë∂£Â§ßÂ§ßÂ¢ûÂä†„ÄÇÊâÄ‰ª•RAID6Êàê‰∏∫‰∏ÄÁßçÂèØÈÄâÈ°πÔºåÂú®ÂÆûÈôÖ‰∏≠Ë¢´‰ΩøÁî®„ÄÇ Êù°Â∏¶Âåñ(striping)ÔºöÂ∞ÜÈÄªËæë‰∏äËøûÁª≠ÁöÑÊï∞ÊçÆÂùóÂàÜÂ∏ÉÂà∞‰∏çÂêåÁöÑÁ£ÅÁõòt,ÂæóÂà∞ÊØîÂçï‰∏™ÂæÆÁõòÊõ¥È´òÁöÑÊÄßËÉΩ„ÄÇ ÈïúÂÉè(mirroring)ÔºöÂ∞ÜÁõ∏ÂêåÁöÑÊï∞ÊçÆÂÜôÂà∞ÂÆ¢‰∏™Êï∞Áõò‰∏äÔºåÁõÆÁöÑÊòØÂ¢ûÂä†Êï∞ÊçÆÁöÑÂèØÁî®ÊÄß„ÄÇ ‰øùÊä§ÁªÑ(protection group):ÂÖ±‰∫´‰∏Ä‰∏™ÂÖ¨ÂÖ±Ê†°È™åÁ£ÅÁöÑÊï∞ÊçÆÁ£ÅÁõòÁªÑÊàñËÄÖÊï∞ÊçÆÂùó„ÄÇ]]></content>
      <categories>
        <category>ËØª‰π¶Á¨îËÆ∞</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TampermonkeyÁöÑËøêË°åÂéüÁêÜ]]></title>
    <url>%2F2018%2F12%2F26%2FTem%2F</url>
    <content type="text"><![CDATA[TamperMonkey is a Google Chrome (and Opera and Chromium) plugin similar to GreaseMonkey for Firefox. It allows you to inject additional JavaScript into web pages you load in your browser, adding features, removing features, or in our case doing hacky, automated things]]></content>
      <categories>
        <category>TampermonkeyÁõ∏ÂÖ≥</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ChromeÊèí‰ª∂ÂºÄÂèëÂéüÁêÜ]]></title>
    <url>%2F2018%2F12%2F26%2FChrome%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Extensions are small software programs that customize the browsing experience. They enable users to tailor Chrome functionality and behavior to individual needs or preferences. They are built on web technologies such as HTML, JavaScript, and CSS.]]></content>
      <categories>
        <category>ChromeÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>Êèí‰ª∂ÂºÄÂèë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VSCodeÂ∏∏Áî®Âø´Êç∑ÈîÆ]]></title>
    <url>%2F2018%2F12%2F26%2FVSCode%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[ËÆ∞ÂΩïÂá†‰∏™Âø´Êç∑ÈîÆ‰∫Ü‰∫ã¬∑¬∑¬∑¬∑ ÂÆòÊñπÂø´Êç∑ÈîÆÊñáÊ°£ Âø´Êç∑ÈîÆ ËØ¥Êòé Ctrl + Shift + P, F1 ÂëΩ‰ª§Èù¢Êùø Ctrl + , Áî®Êà∑ËÆæÁΩÆ Ctrl + F Êü•Êâæ Ctr + =/- ÊîæÂ§ßÁ∏ÆÂ∞è]]></content>
      <categories>
        <category>VSCodeÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>Âø´Êç∑ÈîÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PycharmÂ∏∏Áî®Âø´Êç∑ÈîÆ]]></title>
    <url>%2F2018%2F12%2F26%2FPycharm%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[ËÆ∞ÂΩï‰∏Ä‰∫õPycharm‰∏≠Â∏∏Áî®ÁöÑÂø´Êç∑ÈîÆ ÂàõÂª∫ÂíåÁºñËæë ÂäüËÉΩ ÊèèËø∞ Ê≥®Èáä Show Intention Actions Alt+Enter . Basic Code Completion Ctrl+Space . Smart Code Completion Ctrl+Shift+Space . Type Name Completion Ctrl+Alt+Space Á±ªÂûãÂêçË°•ÂÖ®ÔºåÂá†‰πéÂèØ‰ª•Ë°•ÂÖ®‰ªª‰Ωï‰∏úË•ø Quick Definition Ctrl+Shift+I ÂèØ‰ª•Âú®ÂΩìÂâçÈ°µÈù¢ÊµèËßàÂÆö‰πâ.‰∏çÂèØÁõ¥Êé•ÁºñËæë ÂÆÉ‰ºöÂ±ïÁ§∫ÊñπÊ≥ïÁöÑÂÜÖÂÆπ Quick/External Documentation Ctrl+Q / Shift+F1 ÂèØ‰ª•Âø´ÈÄüÊü•ÁúãÂáΩÊï∞ÊñáÊ°£,Êåâ‰∏§Ê¨°qÂèØ‰ª•ÂºÄ‰∏Ä‰∏™ÂçïÁã¨ÁöÑ‰æßËæπÁ™óÂè£ Ôºå Surround With‚Ä¶ Ctrl+Alt+T ÂèØ‰ª•Â∞ÜÈÄâÊã©ÁöÑ‰ª£Á†ÅÁî®‰ªÄ‰πàÊ°Ü‰Ωè„ÄÇÊØîÂ¶ÇifËØ≠Âè• whileËØ≠Âè• try catch‰πãÁ±ª Extend/Shrink Selection Ctrl+W / Ctrl+Shift+W ‰ªéÂÖâÊ†áÂ§ÑÈÄêÊ∏êÊâ©Â§ßÈÄâÂå∫ Copy Document Path Ctrl+Shift+C ÂèØ‰ª•Áõ¥Êé•Â§çÂà∂ÂΩìÂâçÊâìÂºÄÊñá‰ª∂ÁöÑÁªùÂØπË∑ØÂæÑ Duplicate Current Line or Selection Ctrl+D Âú®ÈúÄË¶ÅËøûÁª≠Â§çÂà∂‰ª£Á†ÅÁöÑÊó∂ÂÄôÁâπÂà´ÊúâÁî®ÔºåÊØîÂ¶ÇÈúÄË¶ÅËøûÁª≠ÂÜôÂæàÂ§öprintÂáΩÊï∞ Move Line Up Ctrl+Shift+UpMove Line Down Ctrl+Shift+DownDelete Line at Caret Ctrl+YJoin/Split Line Ctrl+Shift+J / Ctrl+EnterStart New Line Shift+EnterToggle Case Ctrl+Shift+UExpand/Collapse Code Block Ctrl+NumPad +/-Expand/Collapse All Ctrl+NumPad +/-Save All Ctrl+S Analyze and ExploreShow Error Description Ctrl+F1Next/Previous Highlighted Error F2 / Shift+F2Run Inspection by Name‚Ä¶ Ctrl+Alt+Shift+IType Hierarchy Ctrl+HCall Hierarchy Ctrl+Alt+H VERSION CONTROL ÁâàÊú¨ÊéßÂà∂VCS Operations Popup‚Ä¶ Alt+`Commit Ctrl+KUpdate Project Ctrl+TRecent Changes Alt+Shift+CRevert Ctrl+Alt+ZPush‚Ä¶ Ctrl+Shift+KNext Change Ctrl+Alt+Shift+DownPrevious Change Ctrl+Alt+Shift+Up MASTER YOUR IDE ÁÆ°ÁêÜIDEFind Action‚Ä¶ Ctrl+Shift+AOpen a Tool Window Alt+[0-9]Synchronize Ctrl+Alt+YQuick Switch Scheme‚Ä¶ Ctrl+`ÂèØ‰ª•Âø´ÈÄüÂàáÊç¢‰∏ªÈ¢ò ÊàëËßâÂæóÊå∫ÂÆûÁî®ÁöÑ Settings‚Ä¶ Ctrl+Alt+S ÊâìÂºÄËÆæÁΩÆJump to Source/Navigation Bar F4 / Alt+HomeJump to Last Tool Window F12Hide Active/All Tool Windows Shift+Esc / Ctrl+Shift+F12Go to Next/Previous Editor Tab Alt+Right / Alt+LeftGo to Editor (from a Tool Window) EscClose Active Tab/Window Ctrl+Shift+F4 / Ctrl+F4 FIND EVERYTHING Êü•Êâæ ÂäüËÉΩ ÊèèËø∞ Ê≥®Èáä Search Everywhere Double Shift Êåâ‰∏§‰∏ãshift Declaration Ctrl+B ÂèØ‰ª•ËΩ¨Âà∞ÂáΩÊï∞ÂíåÁ±ªÁ≠âÁöÑÂ£∞ÊòéÂ§Ñ,‰∏é‰∏ãÈù¢ÁöÑÂø´Êç∑ÈîÆ‰∏çÂêåÔºåÂ¶ÇÊûúÂ£∞ÊòéÁöÑÂáΩÊï∞‰∏çÂú®Âêå‰∏Ä‰∏™Êñá‰ª∂Âàô‰ºöÂºÄ‰∏Ä‰∏™Êñ∞ÁöÑÈ°µÈù¢ÂéªÊü•ÁúãÔºåÂπ∂‰∏îÂèØ‰ª•ÁºñËæë Find Usages / Find Usages in File Alt+F7 / Ctrl+F7 Êü•ÊâæÂºïÁî®ÔºåËøô‰∏™Êü•ÊâæÊòØÂÖ®Â±ÄÁöÑÔºåÊØîÂ¶ÇÊü•ÊâæprintÁöÑÂºïÁî®Â∞±‰ºöÂèëÁé∞ÊúâÊï∞Áôæ‰∏™ÂºïÁî® Find/Replace Ctrl+F / Ctrl+RFind/Replace in Path Ctrl+Shift+F / Ctrl+Shift+RNext/Previous Occurence F3 / Shift+F3Find Word at Caret Ctrl+F3Go to Class/File Ctrl+N / Ctrl+Shift+NGo to File Member Ctrl+F12Go to Symbol Ctrl+Alt+Shift+N NAVIGATE FROM SYMBOLS ‰ªéÁ¨¶Âè∑Â§ÑÂÆö‰Ωç Type Declaration (JavaScript only) Ctrl+Shift+BSuper Method Ctrl+UImplementation(s) Ctrl+Alt+B Highlight Usages in File Ctrl+Shift+F7 ÊääÂºïÁî®È´ò‰∫ÆÊòæÁ§∫Âá∫Êù•Show Usages Ctrl+Alt+F7 Áî®‰∏Ä‰∏™Â∞èÊ°ÜÁõ¥Êé•Â±ïÁ§∫ÂºïÁî®ÔºåÁúãËµ∑Êù•ÊØîËæÉÊñπ‰æøNAVIGATE IN CONTEXT Âú®‰∏ä‰∏ãÊñá‰∏≠ÂÆö‰ΩçSelect In‚Ä¶ Alt+F1Recently Viewed/Changed Files Ctrl+E / Ctrl+Shift+ELast Edit Location Ctrl+Shift+BackNavigate Back/Forward Ctrl+Alt+Left / Ctrl+Alt+RightGo to Previous/Next method Alt+Up / Alt+DownLine/Column‚Ä¶ Ctrl+GGo to Code Block End/Start Ctrl+] / Ctrl+[Add to Favorites Alt+Shift+FToggle Bookmark F11Toggle Bookmark with Mnemonic Ctrl+F11Go to Numbered Bookmark Ctrl+[0-9]Show Bookmarks Shift+F BUILD, RUN, AND DEBUG ÊûÑÂª∫ ËøêË°åÂíåË∞ÉËØïRun context configuration Ctrl+Shift+F10Run/Debug Selected Configuration Alt+Shift+F10 / Alt+Shift+F9Run/Debug Current Configuration Shift+F10 / Shift+F9Step Over F8Step Into F7Smart Step Into Shift+F7Step Out Shift+F8Run to Cursor Alt+F9Force Run to Cursor Ctrl+Alt+F9Show Execution Point Alt+F10Evaluate Expression‚Ä¶ Alt+F8Stop Ctrl+F2Stop Background Processes‚Ä¶ Ctrl+Shift+F2Resume Program F9Toggle Line Breakpoint Ctrl+F8Toggle Temporary Line Breakpoint Ctrl+Alt+Shift+F8Edit breakpoint Ctrl+Shift+F8View Breakpoints‚Ä¶ Ctrl+Shift+F8REFACTOR AND CLEAN UPRefactor This‚Ä¶ Ctrl+Alt+Shift+TCopy‚Ä¶ F5Move‚Ä¶ F6Safe Delete‚Ä¶ Alt+DeleteRename‚Ä¶ Shift+F6Change Signature‚Ä¶ Ctrl+F6Inline‚Ä¶ Ctrl+Alt+NExtract Method Ctrl+Alt+MIntroduce Variable Ctrl+Alt+VIntroduce Field Ctrl+Alt+FIntroduce Constant Ctrl+Alt+CIntroduce Parameter Ctrl+Alt+PReformat Code Ctrl+Alt+L]]></content>
      <categories>
        <category>PythonÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>Âø´Êç∑ÈîÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MarkDownÁöÑÂéüÁêÜ]]></title>
    <url>%2F2018%2F12%2F26%2FMarkDown%E7%9A%84%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[ÊúâÊ≤°ÊúâÂ•ΩÂ•áËøáÔºå‰∏∫‰ªÄ‰πà‰Ω†Áî®‰∫ÜËøô‰∫õÁÆÄÂçïÁöÑÊ†áËÆ∞Â∞±ÂèØ‰ª•ÂØπÊñáÁ´†ËøõË°åÊéíÁâàÂíåÁæéÂåñÔºüÂÆÉÊòØÊÄé‰πàÂÅöÂà∞ÁöÑÔºü]]></content>
      <categories>
        <category>MarkDownÁõ∏ÂÖ≥</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MarkDownÂ∏∏Áî®ËØ≠Ê≥ïÂíåÂø´Êç∑ÈîÆ]]></title>
    <url>%2F2018%2F12%2F26%2FMarkDown%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95%E5%92%8C%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[ËÆ∞ÂΩï‰∏Ä‰∫õÂø´Êç∑ÈîÆ]]></content>
      <categories>
        <category>MarkDownÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>Âø´Êç∑ÈîÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HexoÁöÑÁªìÊûÑÂíåÂéüÁêÜ]]></title>
    <url>%2F2018%2F12%2F26%2FHexo%E7%9A%84%E7%BB%93%E6%9E%84%E5%92%8C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[ÂêçÂ≠óÊúâÁÇπÂêì‰∫∫ÔºåÂÖ∂ÂÆûÊàëÂ∞±ÊòØÊÉ≥ÊêûÊòéÁôΩHexoÁöÑÂ§ßËá¥ÁªìÊûÑÊòØ‰ªÄ‰πàÔºåÂÆÉÊòØÊÄé‰πàËøêË°åËµ∑Êù•ÁöÑ :-DÊàë‰∏çÁü•ÈÅìËøôÈúÄË¶ÅÂ§öÂ∞ë‰∏ì‰∏öÁöÑÂâçÁ´ØÁü•ËØÜÔºåÂè™Ê±ÇÂ∞ΩÈáèÊêûÊáÇÂøÉ‰∏≠ÊúâÊï∞Â∞±Â•Ω HexoÁöÑÊñá‰ª∂Â§πÁªìÊûÑ123456789.‚îú‚îÄ‚îÄ _config.yml ‚îú‚îÄ‚îÄ db.json‚îú‚îÄ‚îÄ node_modules ‚îú‚îÄ‚îÄ package.json‚îú‚îÄ‚îÄ public ‚îú‚îÄ‚îÄ scaffolds ‚îú‚îÄ‚îÄ source #ÊâÄÊúâÊñáÁ´†Êñá‰ª∂ÊîæÂú®ËøôÈáå‚îî‚îÄ‚îÄ themes #‰∏ªÈ¢òÊñá‰ª∂Â§π Node.js Node.js¬Æ is a JavaScript runtime built on Chrome‚Äôs V8 JavaScript engine. npm npm is the package manager for JavaScript and the world‚Äôs largest software registry. Discover packages of reusable code ‚Äî and assemble them in powerful new ways. yarn Yarn is a package manager for your code. It allows you to use and share code with other developers from around the world. Yarn does this quickly, securely, and reliably so you don‚Äôt ever have to worry. nodeÂåÖ]]></content>
      <categories>
        <category>HexoÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ëá™Áî®ChromeÊèí‰ª∂Ê±áÊÄª]]></title>
    <url>%2F2018%2F12%2F25%2F%E8%87%AA%E7%94%A8Chrome%E6%8F%92%E4%BB%B6%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[Âú®ËøôÈáåÂ∞ÜËá™Â∑±ÈÄõÂ∫îÁî®ÂïÜÂ∫óÂèëÁé∞ÁöÑÂÆûÁî®Êèí‰ª∂ÂÆâÂà©‰∏Ä‰∏ãÔºåÂåÖÊã¨‰∏™ÊÄßÂåñÊèí‰ª∂‰ª•Âèä‰∏Ä‰∫õÊØîËæÉ‰∏ì‰∏öÁöÑÂ∑•ÂÖ∑„ÄÇChromeÊ≤°‰∫ÜÊèí‰ª∂ÂèØ‰∏çË°åÔºåÊØïÁ´üÊèí‰ª∂ÊâçÊòØÊú¨‰Ωì :) ‰∏™ÊÄßÂåñ Infinity Êñ∞Ê†áÁ≠æÈ°µ È¢úÂÄºÂæàÈ´òÁöÑ‰∏™ÊÄßÂåñ‰∏ªÈ°µ„ÄÇÂØπÊàëÊù•ËØ¥Â∞±ÊòØÊääÊàëÂ∏∏Áî®ÁöÑÂá†‰∏™ÁΩëÁ´ôÊîæÂú®‰∏äÂéª„ÄÇÂΩìÁÑ∂ÂèØ‰ª•ÊîæÂú®‰π¶Á≠æÊ†èÈáå‰∏çËøáÊàëÁöÑ‰π¶Á≠æÊ†èÂÆûÂú®Â§™ËáÉËÇø‰∫ÜÔºåËøôÊ†∑‰ΩìÈ™å‰∏çÊòØÂæàÂ•Ω„ÄÇ ‰π¶Á≠æ‰æßËæπÊ†è ÂÆÉÂ∞±ÊòØ‰∏∫‰∫ÜÊãØÊïëÊàëËáÉËÇøÁöÑ‰π¶Á≠æÊ†èËÄåÊù•ÁöÑ„ÄÇÂèØÈöêËóèÁöÑ‰æßËæπÊ†èËÆæËÆ°ÁâπÂà´ÊúâË∂£Ë∑üÂÖ∂‰ªñÁöÑÈÉΩ‰∏ç‰∏ÄÊ†∑„ÄÇÁº∫ÁÇπÂ∞±ÊòØÊúâÊó∂ÂÄô‰ºöËØØËß¶„ÄÇ ÂºÄÂèëËÄÖÂ∑•ÂÖ∑Êó•Â∏∏Â∑•ÂÖ∑bilibiliÂìîÂì©ÂìîÂì©‰∏ãËΩΩÂä©ÊâãÂÖ≥‰∫é‰∏ãËΩΩÁΩëÈ°µËßÜÈ¢ëÁöÑÂ∑•ÂÖ∑ÂæàÂ§öÔºåÊØîÂ¶ÇÁ°ïÈº†„ÄÇ‰∏çËøáËøô‰∏™ÊòØBÁ´ô‰∏ìÁî®ÁöÑ,ÂäüËÉΩÂçï‰∏ÄÁî®Ëµ∑Êù•‰πüÂ∞±Êñπ‰æø :) ÊàëÁî®ÂÆÉ‰∏ãËΩΩ‰∫ÜÂæàÂ§öBÁ´ôÂ§ß‰ºöÂëò‰∏ì‰∫´ÁöÑÈ´òÊ∏ÖËµÑÊ∫ê Tampermonkey+GreasyFork‰∏ÄÂº†ÂõæÂëäËØâ‰Ω†ÂÆÉ‰ª¨ÁöÑ‰ΩúÁî®Ôºö ÊÑüÂèóÂà∞ÂÆÉÁöÑÂº∫Â§ß‰∫ÜÂêßÔºü Full Page Screen Capture‰∏Ä‰∏™Êà™ÁΩëÈ°µÂÖ®Â±èÁöÑÂ∑•ÂÖ∑„ÄÇ ChromeÊ≤°ÊúâËá™Â∏¶ÁöÑÁΩëÈ°µÊà™Â±èÂ∑•ÂÖ∑ÔºàÊàëÊ≤°ÊâæÂà∞Ôºâ‰∏çÊîæÂõæ‰∫Ü Google Dictionary (by Google)Ë∞∑Ê≠åÂá∫ÁöÑÁΩëÈ°µËØçÂÖ∏Êèí‰ª∂ÔºåÁî®Ëµ∑Êù•ÂæàÈ°∫Êâã„ÄÇÂè¶Â§ñÂÆÉËøòÂèØ‰ª•ÊääÊü•ËØçÂéÜÂè≤ËÆ∞ÂΩï‰∏ãÊù•ÂêåÊ≠•Âà∞‰∫ëÁ´Ø ÁºñÁ†ÅËß£Á†ÅHasher‰∏ÄÊ¨æÂèØ‰ª•ÂêÑÁßçÂ≠óÁ¨¶‰∏≤hashÂÄºÁöÑÂ∑•ÂÖ∑ÔºåÁßçÁ±ªÂæàÂÖ®ÂÅ∂Â∞îÁî®Áî® ÁΩëÁ´ôÂºÄÂèëÊé•Âè£ÊµãËØïÂ∑•ÂÖ∑Restlet Client - REST API TestingÈ¢úÂÄºÂæàÈ´òÁöÑ‰∏Ä‰∏™REST API ÊµãËØïÂ∑•ÂÖ∑„ÄÇÊúâËØÑËÆ∫ËØ¥ ‚Äúbetter than postman‚Äú,ÊàëËßâÂæóËøòÂ•ΩÂêßÔºåÂäüËÉΩÈÉΩÂ∑Æ‰∏çÂ§öÂè™‰∏çËøáÊèí‰ª∂Êõ¥Êñπ‰æø„ÄÇ ÂèñËâ≤Â∑•ÂÖ∑‰∏§Ê¨æÂèñËâ≤Â∑•ÂÖ∑ÂêÑÊúâÂçÉÁßãÂêßÔºåÊÑüËßâÂ∑Æ‰∏çÂ§öColorPick Eyedropper ColorZilla]]></content>
      <categories>
        <category>ÊùÇË¥ßÈì∫</category>
      </categories>
      <tags>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂÖ≥‰∫éssËäÇÁÇπË¥πÁéá]]></title>
    <url>%2F2018%2F07%2F07%2F%E5%85%B3%E4%BA%8ESS%E7%9A%84%E8%B4%B9%E7%8E%87%2F</url>
    <content type="text"><![CDATA[‰ª•‰∏ãÂÜÖÂÆπÊù•Ëá™ÂÆ¢ÊúçÂõûÂ§çÔºö ÊàëÂÄëÈôêÂà∂ÊµÅÈáèÁöÑÂàùË°∑ÔºåÊòØÁà≤‰∫ÜÈò≤Ê≠¢ÂÆ¢Êà∂ÂàÜ‰∫´Ëá™Â∑±ÁöÑÈÖçÁΩÆ‰ø°ÊÅØ‰ª•ÂèäÊø´Áî®ÊàëÂÄëÁöÑÊúçÂãô„ÄÇÂ¶ÇÊûúÊÇ®Âú®ÊúàÂ∫ïÂâç‰ΩøÁî®ÊµÅÈáèË∂ÖÈÅé‰∫Ü 50GBÔºå Á≥ªÁµ±ÊúÉËá™ÂãïÊö´ÂÅúÊÇ®ÁöÑÊúçÂãô„ÄÇÊÇ®ÂèØ‰ª•ÈÄöÈÅéÂ∑•ÂñÆËÅØÁπ´ÊàëÂÄëÈáçÁΩÆÊÇ®ÁöÑÊµÅÈáèÊàëÂÄëÁõÆÂâç‰πüÊõ¥Êñ∞‰∫ÜÊµÅÈáèÁµ±Ë®àÊîøÁ≠ñÔºåÊÇ®ÁöÑÂØ¶ÈöõÂèØ‰ΩøÁî®ÊµÅÈáèÂπ∂‰∏çÊòØ 50GB ÔºåËàáÊÇ®ÊâÄ‰ΩøÁî®ÁöÑÁØÄÈªûÁöÑÊµÅÈáèÁµ±Ë®àÂÄçÁéáÊúâÈóú„ÄÇÊúçÂãôÂô®Ë©≥Á¥∞ÁöÑÁµ±Ë®àÂÄçÁéáË´ãË¶ãÊÇ®ÁöÑÊúçÂãôÂô®Ë©≥ÊÉÖÂÇôÊ≥®ÔºåÈÉ®ÂàÜÊúçÂãôÂô®ÊµÅÈáèÁµ±Ë®àÂÄçÁéáÂ¶Ç‰∏ãÔºöhk1 -&gt; 100%jp1 -&gt; 150%us2 -&gt; 40%us1 -&gt; 1%hk3 -&gt; 20%ali-hk2 -&gt; 150%ali-jp2 -&gt; 120%ali-sg1 -&gt; 120%ult ‰∏≠ËΩâÁØÄÈªû -&gt; 200%ÊÇ®‰ΩøÁî®ÁöÑÊµÅÈáèÊ†πÊìöÊÇ®‰ΩøÁî®ÁöÑÊúçÂãôÂô®ÊúÉÊåâÁÖß‰∏äËø∞ÂÄçÁéáÁµ±Ë®àÔºåÂ¶ÇÊûúÊÇ®ÈúÄË¶Å‰ΩøÁî® 1GB ÊµÅÈáèÔºå‰ΩøÁî® hk1 (100%) ÁöÑË©±ÔºåÊµÅÈáèÁµ±Ë®àÁÇ∫Ê≠£Â∏∏ÁöÑ 1GB , ‰ΩøÁî® jp1 (150%) ÂâáÊúÉÁµ±Ë®àÁÇ∫ 1.5GB Ôºå‰ΩøÁî® us1 (1%) ÂâáË°πÊúÉÁµ±Ë®àÁÇ∫ 10M ÊµÅÈáèË®àÂÖ•ÊÇ®ÁöÑÁ∏Ç‰ΩøÁî®ÊµÅÈáè„ÄÇÂ¶ÇÊûúÊÇ®Âú®ÊúàÂ∫ïÂâç‰ΩøÁî®Ë∂ÖËøá 50GB ÊµÅÈáèÔºåÊàë‰ª¨ÁöÑÁ≥ªÁªü‰ºöËá™Âä®ÊöÇÂÅúÊÇ®ÁöÑÊúçÂä°„ÄÇ‰ΩÜÊòØÊÇ®ËØ∑‰∏çË¶ÅÊãÖÂøÉÔºåÂ¶ÇÊûúÊÇ®Âú®ÊúàÂ∫ïÂâç‰ΩøÁî®ÁöÑÊµÅÈáèÂ∞ÜÊé•Ëøë‰∫é 50GB Êó∂ÔºåËØ∑ËÅîÁ≥ªÊàë‰ª¨ÔºåÊàë‰ª¨‰ºöÊâãÂä®‰∏∫ÊÇ®Â¢ûÂä†ÊµÅÈáèÔºåÁ°Æ‰øùÊÇ®ÁöÑÊ≠£Â∏∏‰ΩøÁî®„ÄÇ Âª∫Ë≠∞ÊÇ®ÈÄ≤Ë°åÂ§ßÊµÅÈáèÁöÑÊìç‰ΩúÊôÇ‰ΩøÁî®‰ΩéÂÄçÁéáÁöÑÊúçÂãôÂô®ÔºàÂ¶Ç us2 Ôºâ„ÄÇÂ¶ÇÊûúÊÇ®ËøòÊúâÊõ¥Â§öÁñëÈóÆÔºåËØ∑‰∏çË¶ÅÁäπË±´ËÅîÁ≥ªÊàë‰ª¨Ë∞¢Ë∞¢Best regardsFuse MidoriShadowsocks.com Team ‰πüÂ∞±ÊòØËØ¥Âú®ÊØè‰∏™ÊúàÊúâ50GÊµÅÈáèÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ΩøÁî®1.0ÂÄçÁéáÁöÑËäÇÁÇπ‰ª£Ë°®‰Ω†Áî®1GBÊµÅÈáèÊúçÂä°Âô®Â∞±Êâ£‰Ω†1GBÔºå‰ΩøÁî®1.5ÂÄçË¥πÁéáÁöÑËäÇÁÇπÁî®1GBÂàôÊâ£‰Ω†1.5GB„ÄÇ‰æùÊ≠§Á±ªÊé®]]></content>
      <categories>
        <category>ÊùÇË¥ßÈì∫</category>
      </categories>
      <tags>
        <tag>ÁßëÂ≠¶‰∏äÁΩë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂçöÂÆ¢Êõ¥Êñ∞Êó•Âøó]]></title>
    <url>%2F2018%2F07%2F07%2F%E5%8D%9A%E5%AE%A2%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[2018Âπ¥7Êúà7Êó•Ôºö 1.Â¢ûÂä†Â•ΩÁ´ôÔºöÂú®Á∫øË°®Ê†ºÁîüÊàêÂô® ‰∏Ä‰∏™Áî®‰∫éÂú®Á∫øÁîüÊàêÂêÑÁßçÁ±ªÂûãË°®Ê†ºÁöÑÂ∑•ÂÖ∑ÔºåÊîØÊåÅLaTex„ÄÅHTML„ÄÅText(Â∞±ÊòØÁî®Âä†Âè∑ÂíåÁü≠Ê®™Ê°ÜËµ∑Êù•ÁöÑË°®Ê†º)„ÄÅMarkdown„ÄÅMediaWiki Ê†ºÂºèÁî®Ëµ∑Êù•ÂæàÊñπ‰æø 2.Â¢ûÂä†Â•ΩÁ´ôÔºöÁßëÂ≠¶‰∏äÁΩëÔºà‰ªòË¥πÔºâ Êàë‰∏ÄÁõ¥Âú®Áî®ÁöÑÁøªÂ¢ôÂ∑•ÂÖ∑„ÄÇÂÆ¢Êà∑Á´Ø‰ΩøÁî®ÁöÑÊòØShadowSocksÔºåËäÇÁÇπÂæàÂ§öÔºåÂåÖÊã¨È¶ôÊ∏ØÁæéÂõΩÊó•Êú¨‰øÑÁΩóÊñØ„ÄÇÊîØÊåÅÊîØ‰ªòÂÆùÔºåÂÖ•Èó®ÁâàÂπ¥Ë¥πÂè™Ë¶Å98.74ÂÖÉÔºåÂêà‰∏Ä‰∏™Êúà8ÂùóÈí±„ÄÇÊØèÊúàÈôêÊµÅ50GÔºåÁî®ÂÆåÂàôÊñ≠ÁΩëÔºåÂÜçÁî®ÂàôÈúÄË¶ÅËÅîÁ≥ªÂÆ¢ÊúçÂ¢ûÂä†ÊµÅÈáè„ÄÇ‰∏™‰∫∫ËßâÂæó50GË∂≥Â§üÂà∑Ê≤πÁÆ°ÂíåinsÁî®‰∫Ü„ÄÇÁõÆÂâçÊàëËøòÊ≤°ÊúâÁî®ÂÆÉ‰∏ãËΩΩËøáÂ§ßÊñá‰ª∂ÔºåÂÆòÊñπÊòØÁ¶ÅÊ≠¢Êª•Áî®ÁöÑ„ÄÇ Âè¶Â§ñÊØè‰∏™ËäÇÁÇπÁöÑË¥πÁéá‰∏ç‰∏ÄÊ†∑ ÂÖ≥‰∫éË¥πÁéáÁöÑËØ¶ÊÉÖËßÅÊàëÁöÑÊñáÁ´† 3.Ê∑ªÂä†ÊñáÁ´†ÔºöÂÖ≥‰∫éssËäÇÁÇπË¥πÁéá 4. ÂêØÁî®‰∫ÜÊï∞Â≠¶ÂÖ¨ÂºèÊ∏≤ÊüìÁöÑÈÄâÈ°π 2018Âπ¥7Êúà6Êó•Ôºö Ê∑ªÂä†ÊñáÁ´†ÔºöMIPSÂ∏∏Áî®Ë°®Ê†º]]></content>
      <tags>
        <tag>ÂçöÂÆ¢Êõ¥Êñ∞Êó•Âøó</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIPSÂ∏∏Áî®Ë°®Ê†º]]></title>
    <url>%2F2018%2F07%2F06%2FMIPS%E5%B8%B8%E7%94%A8%E8%A1%A8%E6%A0%BC%2F</url>
    <content type="text"><![CDATA[‰ª•‰∏ãÊòØÂú®Â≠¶‰π†ÂíåÂÅöÈ¢ò‰∏≠ÁªèÂ∏∏Ë¶ÅÁî®Âà∞ÁöÑMIPSÊ±áÁºñÊåá‰ª§Ë°®Ê†º RÂûãÊåá‰ª§ op rs rt rd shamt funct 6‰Ωç 5‰Ωç 5‰Ωç 5‰Ωç 5‰Ωç 6‰Ωç IÂûãÊåá‰ª§ op rs rt constant or address 6‰Ωç 5‰Ωç 5‰Ωç 16‰Ωç MIPS‰∏≠32‰∏™ÈÄöÁî®ÂØÑÂ≠òÂô® Name number Usage Reserved on call \$zero 0 constant value =0(ÊÅí‰∏∫0) n.a \$at 1 reserved for assembler n.a. \$v0 ~ \$v1 2~3 values for results(ËøáÁ®ãË∞ÉÁî®ËøîÂõûÂÄº) no \$a0~\$a3 4~7 Arguments(ËøáÁ®ãË∞ÉÁî®ÂèÇÊï∞) Âê¶ \$t0~\$t7 8~15 Temporaries(‰∏¥Êó∂ÂèòÈáè) no \$s0~\$s7 16~23 Saved(‰øùÂ≠ò) yes \$t8~\$t9 24~25 more temporaries(ÂÖ∂‰ªñ‰∏¥Êó∂ÂèòÈáè) no \$k0~\$k1 26~27 reserved for kernel(‰∏∫OS‰øùÁïô) ‰∏çÈÄÇÁî® \$gp 28 global pointer(ÂÖ®Â±ÄÊåáÈíà) yes \$sp 29 stack pointer (Ê†àÊåáÈíà) yes \$fp 30 frame pointer (Â∏ßÊåáÈíà) ÊòØ \$ra 31 return address (ËøáÁ®ãË∞ÉÁî®ËøîÂõûÂú∞ÂùÄ) ÊòØ ÁÆóÊúØËøêÁÆóÊåá‰ª§ Instruction Example Meaning Comments add add \$1,\$2,\$3 \$1 = \$2 + \$3 3 operands; excep. possible subtract sub \$1,\$2,\$3 \$1 = \$2 ‚Äì \$3 3 operands; excep. possible add immed. addi \$1,\$2,100 \$1 = \$2 + 100 + constant; excep. Possible multiply mult \$2,\$3 Hi, Lo = \$2√ó\$3 64-bit signed product divide div \$2,\$3 Lo = \$2 √∑ \$3 Hi = \$2 mod \$3 Lo = quotient,Hi = remainder ÈÄªËæëËøêÁÆóÊåá‰ª§ Instruction Example Meaning Comments and and \$1,\$2,\$3 \$1 = \$2 &amp; \$3 Logical AND or immed. ori \$1,\$2,20 \$1 = \$2 &#124; 20 Bitwise-OR of constant xor xor \$1,\$2,\$3 \$1 = \$2‚àß\$3 Logical XOR nor nor \$1,\$2,\$3 \$1 = ~(\$2 &#124; \$3) Logical NOR Êï∞ÊçÆ‰º†ËæìÊåá‰ª§ Instruction Example Meaning Comments sw sw \$3, 500(\$4) \$3 ‚Üí(\$4+ 500) Store word sh sh \$3, 502(\$2) Low Half of \$3 ‚Üí(\$2+ 502) Store half sb sb \$2, 41(\$3) LQ of \$2 ‚Üí(\$3+ 41) Store byte lw lw \$1, -30(\$2) (\$2-30) ‚Üí \$1 Load word lh lh \$1, 40(\$3) (\$3+ 40) ‚Üí LH of \$1 Load half lb lb \$1, 40(\$3) (\$3+ 40) ‚Üí LQ of \$1 Load byte Êù°‰ª∂ÂàÜÊîØÊåá‰ª§ Instruction Example Meaning Comments beq beq \$1,\$2,100 if (\$1Ôºù\$2) go to PC+4+100 branch on equal bne bne \$1,\$2,100 if (\$1 != \$2) go to PC+4+100 branch on not eq. slt slt \$1,\$2,\$3 if (\$2 &lt; \$3) \$1=1; else \$1=0 set on less than slti slti \$1,\$2,100 if (\$2 &lt; 100) \$1=1; else \$1=0 set less than imm. Êó†Êù°‰ª∂Ë∑≥ËΩ¨Êåá‰ª§ Instruction Example Meaning Comments j j 10000 go to 10000 jump jal jal 10000 \$31 = PC + 4; go to 10000 for procedure call jump and link jr jr \$31 go to \$31 for switch, procedure return jump register]]></content>
      <tags>
        <tag>MIPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[„ÄäÁºñËØëÂéüÁêÜ‰∏≠Â≠êÈõÜÊ≥ïÁöÑÊïôÂ≠¶Êé¢ËÆ®„Äã‚Äî‚ÄîËíãÂáå‰∫ë-Â≠¶‰π†Á¨îËÆ∞]]></title>
    <url>%2F2018%2F05%2F21%2F%E3%80%8A%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E4%B8%AD%E5%AD%90%E9%9B%86%E6%B3%95%E7%9A%84%E6%95%99%E5%AD%A6%E6%8E%A2%E8%AE%A8%E3%80%8B%E2%80%94%E2%80%94%E8%92%8B%E5%87%8C%E4%BA%91-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Áî®ËØÜÂà´ÁöÑËßÇÁÇπÁúãÂ≠êÈõÜÊ≥ïÁöÑÂÆö‰πâËÆæËÆ°Â≠êÈõÜÊ≥ïÁöÑÁõÆÁöÑÊòØÂ∞ÜËΩ¨Êç¢Á≥ªÁªüÊûÑÈÄ†Âá∫Á°ÆÂÆöÊúâÁ©∑Ëá™Âä®Êú∫ÔºàDFAÔºâÔºåÊâÄ‰ª•ÂèØ‰ª•‰ªéËØÜÂà´ÁöÑËßÇÁÇπÁêÜËß£Â≠êÈõÜÊ≥ï„ÄÇÂØπÁä∂ÊÄÅÂ≠ê ÈõÜIÁöÑŒµ-Èó≠ÂåÖÂíåÂ≠êÈõÜIaËøõË°åÈáçÊñ∞ÂÆö‰πâ„ÄÇÂõæ1‰∏∫Ê≠£ËßÑÂºèeÔºù(aÔΩú b)*(aaÔΩúbb)(aÔΩúb)*ÂØπÂ∫îÁöÑËΩ¨Êç¢Á≥ªÁªü„ÄÇ‰ª•Âõæ1‰∏∫‰æãÊù•ËØ¥ÊòéÈáç Êñ∞ÂÆö‰πâÁöÑÁä∂ÊÄÅÂ≠êÈõÜIÁöÑŒµ-Èó≠ÂåÖÂíåÂ≠êÈõÜIa„ÄÇ Áä∂ÊÄÅÂ≠êÈõÜIÁöÑŒµ-Èó≠ÂåÖÈáçÊñ∞ÂÆö‰πâ‰∏∫‚Äú‰ªéÁä∂ÊÄÅÂ≠êÈõÜI‰∏≠ÁöÑÊØè‰∏™Áä∂ÊÄÅÂºÄÂßãËØÜÂà´ŒµÊâÄËææÂà∞ÁöÑÁä∂ÊÄÅÁöÑÂÖ®‰Ωì‚Äù„ÄÇÂõ†‰∏∫ŒµÔºùŒµŒµÔºùŒµ‚Ä¶ŒµÔºùŒµÔºåÊâÄ‰ª•‰ªéÊüê‰∏™Áä∂ÊÄÅÂá∫ÂèëËØÜ Âà´‰∏Ä‰∏™ŒµÂíåËØÜÂà´Ëã•Âπ≤‰∏™ŒµÔºåÂÖ∂Êú¨Ë¥®ÊòØÁõ∏ÂêåÁöÑÔºåÈÉΩÊòØËØÜÂà´‰∏Ä‰∏™Œµ„ÄÇ‰æãÂ¶ÇÔºåÂÅáËÆæIÔºù{S}ÔºåÂõæ1‰∏≠‰ªéSÂá∫ÂèëËØÜÂà´‰∏Ä‰∏™ŒµÂà∞Ëææ 5Áä∂ÊÄÅÔºåÂç≥M(S,Œµ)Ôºù5Ôºõ‰ªéSÂá∫ÂèëËØÜÂà´2‰∏™ŒµÂà∞Ëææ1Áä∂ÊÄÅÔºåÂç≥ M(S,ŒµŒµ)ÔºùM(M(S,Œµ),Œµ)ÔºùM(5,Œµ)Ôºù1„ÄÇËÄåŒµÊòØ‰∏Ä‰∏™Á©∫Á¨¶Âè∑‰∏≤ÔºåÊòæÁÑ∂ÊúâM(S,Œµ)ÔºùS„ÄÇÈÇ£‰πà‰ªéSÁä∂ÊÄÅÂá∫ÂèëËØÜÂà´ŒµÊâÄÂà∞Ëææ ÁöÑÁä∂ÊÄÅÂÖ®‰Ωì‰∏∫{S,5,1}„ÄÇ Â≠êÈõÜIaÈáçÊñ∞ÂÆö‰πâ‰∏∫‚Äú‰ªéÁä∂ÊÄÅÂ≠êÈõÜI‰∏≠ÁöÑÊØè‰∏™Áä∂ÊÄÅÂºÄÂßãËØÜÂà´aÁ¨¶Âè∑ÊâÄËææÂà∞ÁöÑÁä∂ÊÄÅÁöÑÂÖ®‰Ωì‚Äù„ÄÇÂõ†‰∏∫aÔºùŒµaÔºùaŒµÔºùŒµ‚Ä¶ŒµaŒµ‚Ä¶ŒµÔºùaÔºåÊâÄ‰ª•‰ªéÊüê‰∏™Áä∂ÊÄÅÂá∫ÂèëËØÜÂà´‰∏Ä‰∏™aÁ¨¶Âè∑ÂíåËØÜÂà´Œµ‚Ä¶ŒµaŒµ‚Ä¶ŒµÔºåÂÖ∂Êú¨Ë¥®ÊòØÁõ∏ÂêåÁöÑÔºåÈÉΩÊòØËØÜÂà´‰∏Ä‰∏™aÁ¨¶Âè∑„ÄÇ‰æãÂ¶ÇÔºåÂÅáËÆæIÔºù{S,5,1}ÔºåÈ¶ñÂÖà‰ªéSÂá∫ÂèëËØÜÂà´‰∏Ä‰∏™ŒµaÂà∞Ëææ5Áä∂ÊÄÅÔºåÂç≥M(S,Œµa)ÔºùM(M(S,Œµ),a)Ôºù(5,a)Ôºù5Ôºõ‰ªéSÂá∫ÂèëËØÜÂà´ŒµaŒµÂà∞Ëææ1Áä∂ÊÄÅÔºåÂç≥M(S,ŒµaŒµ)ÔºùM(M(S,Œµ),aŒµ)ÔºùM(5,aŒµ)ÔºùM(M(5,a),Œµ)ÔºùM(5,Œµ)Ôºù1ÔºåÈÇ£‰πà‰ªéSÁä∂ÊÄÅÂá∫ÂèëËØÜÂà´aÊâÄÂà∞ËææÁöÑÁä∂ÊÄÅÂÖ®‰Ωì‰∏∫{5,1}„ÄÇÂêåÁêÜÔºå‰ªé5Áä∂ÊÄÅÂá∫ÂèëËØÜÂà´aÊâÄÂà∞ËææÁöÑÁä∂ÊÄÅÂÖ®‰Ωì‰∏∫{5,1}Ôºå‰ªé1Áä∂ÊÄÅÂá∫ÂèëËØÜÂà´aÊâÄÂà∞ËææÁöÑÁä∂ÊÄÅÂÖ®‰Ωì‰∏∫{3}ÔºåÁªºÂêàÂæóÂà∞ IaÔºù{5,3,1}„ÄÇ Âà©Áî®ÈáçÊñ∞ÂÆö‰πâÂêéÁöÑÁä∂ÊÄÅÂ≠êÈõÜIÁöÑŒµ-Èó≠ÂåÖÂíåÂ≠êÈõÜIaËøõË°åÊïôÂ≠¶ÔºåÂ≠¶ÁîüÂè™ÈúÄË¶ÅÂà©Áî®ÂéüÊù•Ëá™Âä®Êú∫Áä∂ÊÄÅËΩ¨Êç¢ÂõæËØÜÂà´Á¨¶Âè∑‰∏≤ÁöÑÁü•ËØÜÂ∞±ÂèØ‰ª•Ê±ÇÂá∫Â≠êÈõÜIÁöÑŒµ-Èó≠ÂåÖÂíåÂ≠êÈõÜIaÔºå‰ΩøÂéüÊù•ÊäΩË±°ÁöÑÂÆö‰πâÂèòÂæóÊõ¥ÂèØÊìç‰ΩúÔºåÊõ¥ÂÆπÊòìÊéåÊè°ÂíåÁêÜËß£„ÄÇ Âà©Áî®Â≠êÈõÜÊ≥ïËß£ÂÜ≥‚Äú‰ªéNFAËΩ¨Êç¢Âà∞DFA‚ÄùÈóÆÈ¢òÂ∞ÜËΩ¨Êç¢Á≥ªÁªüÊûÑÈÄ†Âá∫Á°ÆÂÆöÊúâÁ©∑Ëá™Âä®Êú∫ÔºàDFAÔºâÔºåÂÖ∂Êú¨Ë¥®ÊòØÂ∞Ü‰∏Ä‰∏™Â∏¶Á©∫ËΩ¨ÁöÑÈùûÁ°ÆÂÆöÁöÑÊúâÁ©∑Ëá™Âä®Êú∫ÔºàNFAÔºâËøõË°åÁ°ÆÂÆöÂåñ„ÄÇÂõ†Ê≠§ÔºåÂà©Áî®ÈáçÊñ∞ÂÆö‰πâÂêéÁöÑÁä∂ÊÄÅÂ≠êÈõÜIÁöÑŒµ-Èó≠ÂåÖÂíåÂ≠êÈõÜIaËøõË°åÊïôÂ≠¶ÔºåÂèØ‰ª•Â∞ÜÂ≠êÈõÜÊ≥ïÊé®ÂπøÂà∞Ëß£ÂÜ≥‚Äú‰ªéNFAËΩ¨Êç¢Âà∞DFA‚ÄùÈóÆÈ¢ò„ÄÇ I Ia Ib K 0 1 {1} {0} {–§} A B –§ {0} {0,1} {1} B C A {0,1} {0,1} {1} C C A Âõæ3 Â≠êÈõÜÊ≥ïËΩ¨Êç¢Áü©Èòµ ‰æãÂ¶ÇÔºåÂõæ2‰∏∫‰∏Ä(NFA)MÔºù({0,1},{a,b},M,{1},{0})ÁöÑÁä∂ÊÄÅËΩ¨Êç¢ÂõæÔºåÂ≠êÈõÜÊ≥ïÁöÑÁ¨¨‰∏ÄË°åÁ¨¨‰∏ÄÂàó‰∏∫ÂàùÂßãÁä∂ÊÄÅÈõÜ{1}‰∏≠ÁöÑÊØè‰∏™Áä∂ÊÄÅÂºÄÂßãËØÜÂà´ŒµÊâÄËææÂà∞ÁöÑÁä∂ÊÄÅÁöÑÂÖ®‰ΩìÔºåÂõ†NFAÁöÑÁä∂ÊÄÅËΩ¨Êç¢Âõæ‰∏≠Ê≤°ÊúâŒµÔºåÊâÄ‰ª•Á¨¨‰∏ÄË°åÁ¨¨‰∏ÄÂàóÂ∞±‰∏∫ÂàùÂßãÁä∂ÊÄÅÈõÜ{1}„ÄÇÁ¨¨‰∏ÄË°åIaÂíåIbÂàÜÂà´ÊòØ‰ªé1Áä∂ÊÄÅÂºÄÂßãËØÜÂà´aÁ¨¶Âè∑‰∏ébÁ¨¶Âè∑ÊâÄËææÂà∞ÁöÑÁä∂ÊÄÅÁöÑÂÖ®‰Ωì„ÄÇÂÖ∂‰ΩôËΩ¨Êç¢ÂèÇËßÅÂõæ3„ÄÇËΩ¨Êç¢ÂêéÁöÑ(DFA)MÔºù({A,B,C},{a,b},M,{1},{0})ÔºåM(A,0)ÔºùBÔºåM(B,0)ÔºùCÔºåM(B,1)ÔºùAÔºåM(C,0)ÔºùCÔºåM(C,1)ÔºùA„ÄÇ]]></content>
      <tags>
        <tag>ÁºñËØëÂéüÁêÜ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂÖ≥‰∫éUnity3D‰∏≠ÁöÑÂçèÁ®ã-Coroutine]]></title>
    <url>%2F2018%2F05%2F10%2F%E5%85%B3%E4%BA%8EUnity3D%E4%B8%AD%E7%9A%84%E5%8D%8F%E7%A8%8B-Coroutine%2F</url>
    <content type="text"><![CDATA[ËøôÂ≠¶ÊúüÔºàÂ§ß‰∏â ‰∏ãÔºâÂ≠¶Unity3DÊ∏∏ÊàèÂºÄÂèë„ÄÇÁÑ∂ËÄåÊàëÂØπU3DÂèØ‰ª•ÂÆûÁé∞ÁöÑÈÇ£‰∫õÂçé‰∏ΩÊïàÊûúÂπ∂‰∏çÊÑüÂÖ¥Ë∂£¬∑¬∑¬∑¬∑¬∑¬∑ÂÖ∂ÂÆûÂêßÔºå‰πü‰∏çÊòØ‰∏çÊÑüÂÖ¥Ë∂£ÔºåËÄåÊòØÂÆûÂú®Ê≤°ÊúâËâ∫ÊúØÁªÜËÉûÔºå‰∏çÁü•ÈÅìÊÄé‰πàÂºÑÊâçËÉΩÂ•ΩÁúã„ÄÇÊâÄ‰ª•ÊàëÂ∏åÔºàzhiÔºâÊúõ(neng)Â§öÂ≠¶‰π†U3DÁöÑËÑöÊú¨ÁºñÁ®ãÊäÄÊúØ„ÄÇÊàëÂ§ß‰∏ÄÂºÄÂßãÂ∞±Â≠¶ÁöÑC#ÔºåÊâÄ‰ª•‰∏ÄÁõ¥‰ª•Êù•Ê≤°ÊúâÈÅáÂà∞‰ªÄ‰πà‰∏çÁêÜËß£ÁöÑÂú∞Êñπ„ÄÇÂèØÊòØÊàëÂèëÁé∞‚ÄúÂçèÁ®ãÔºàCoroutine,/k…ôru:‚Äôti:n/Ôºâ‚Äù Ëøô‰∏™Â•Ω‰∏úË•øÔºåÂú®Êàë‰πãÂâçÂ≠¶‰π†C#‰∏™ËøáÁ®ã‰∏≠Âπ∂Ê≤°ÊúâÈÅáËßÅ„ÄÇËøôÊ¨°Â∞±Â•ΩÂ•ΩÊÄªÁªì‰∏Ä‰∏ãÂÆÉÁöÑÂéüÁêÜÂíåÁî®Ê≥ï„ÄÇ Âç†Âùë]]></content>
      <tags>
        <tag>Unity3D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÁªÉ‰π†È°πÁõÆÔºöÂå∫ÂùóÈìæÂú®ÂåªÈô¢ÁöÑÂ∫îÁî®]]></title>
    <url>%2F2018%2F05%2F07%2F%E7%BB%83%E4%B9%A0%E9%A1%B9%E7%9B%AE%EF%BC%9A%E5%8C%BA%E5%9D%97%E9%93%BE%E5%9C%A8%E5%8C%BB%E9%99%A2%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Ê¶ÇËø∞È¶ñÂÖàÂ£∞ÊòéÔºöÁªèËøáÂØπÈ°πÁõÆÁöÑÂàÜÊûê‰ª•ÂèäÂØπÂå∫ÂùóÈìæÁâπÊÄßÁöÑ‰∫ÜËß£ÔºåÊàëËÆ§‰∏∫ÔºöÂåªÈô¢ÁßÅÊúâÂå∫ÂùóÈìæÊòØÊ≤°ÊúâÊÑè‰πâÁöÑ„ÄÇÂå∫ÂùóÈìæÁöÑÁßÅÊúâÂåñÔºå‰ºöÂØºËá¥Âå∫ÂùóÈìæ‰∏ßÂ§±ÂÖ∂‚ÄúÂéª‰∏≠ÂøÉÂåñ‚ÄùÁöÑÁâπÂæÅÔºå‰πüÂ∞±ÊÑèÂë≥ÁùÄÁ¨¨‰∏âÊñπÊó†Ê≥ïÊâøËÆ§ËØ•ÈìæÊâÄÂåÖÂê´ÁöÑ‰ø°ÊÅØ„ÄÇ Êû∂ÊûÑ ËÆæËÆ° P2PÊúçÂä°Ê®°Âùó ÊåñÁüøÊ®°Âùó P2PÊúçÂä°Ê®°ÂùóP2PÊúçÂä°Ê®°ÂùóË¥üË¥£‰∏éÂÖ∂‰ªñËäÇÁÇπÂª∫Á´ãËøûÊé•„ÄÇÁ®ãÂ∫èËøêË°åÂêéÂ∏∏È©ªÂÜÖÂ≠òÔºå‰∏ªË¶ÅË¥üË¥£ÁõëÂê¨ÂÖ∂‰ªñËäÇÁÇπÁöÑÂπøÊí≠‰ø°ÊÅØÔºåÂêåÊó∂ÊåñÁüøÊ®°Âùó‰πüÈÄöËøáÂÆÉÊù•ÂÆ£Â∏ÉËá™Â∑±ÁöÑÊåñÁüøÊàêÊûú„ÄÇ ËäÇÁÇπÂàóË°®ÔºöÁî®Êù•Â≠òÊîæÁΩëÁªúÁéØÂ¢É‰∏≠ÊâÄÊúâËäÇÁÇπÁöÑIPÂíåÁ´ØÂè£Âè∑ ÁõëÂê¨Á∫øÁ®ãÔºöwhile(true) ÁõëÂê¨ÊâÄÊúâËäÇÁÇπÂπøÊí≠ÁöÑÊ∂àÊÅØ Êñá‰ª∂Êé•Êî∂Á∫øÁ®ãÔºöË¥üË¥£Êñá‰ª∂ÁöÑÊé•Êî∂ÂíåÁºìÂÜ≤ÔºåÊñá‰ª∂ÊåâÊµÅÊé•Êî∂ TCPÂçèËÆÆÂèëÈÄÅÊñá‰ª∂ÁöÑÊó∂ÂÄôÊòØÂèëÈÄÅÁöÑÂ≠óËäÇÊµÅÔºåÈÇ£‰πàÂÆÉÊòØÊó†Ê≥ïÁü•ÈÅì‰∏Ä‰∏™Êñá‰ª∂ÊòØÂê¶Â∑≤Áªè‰º†ÈÄÅÂÆå‰∫ÜÁöÑ„ÄÇÊåâÁÖßËÄÅÂ∏àÁöÑËÆæÊÉ≥Ôºå‰ªñÊâìÁÆóÂú®ÂåªÈô¢ÊúçÂä°Âô®ÂêëÊï∞ÊçÆÂ∫ì‰º†ËæìÊñá‰ª∂ÁöÑÊó∂ÂÄôÔºåÈÄöËøáÊüêÁßçÊñπÂºèËé∑ÂèñÂà∞ÂÆÉ„ÄÇÂÆûÁé∞ÊâÄË∞ìÁöÑÔºå‰∏ä‰º†‰∏Ä‰∏™Â§ÑÁêÜ‰∏Ä‰∏™„ÄÇ ÊåñÁüøÊ®°ÂùóÊÉ≥‰∏çÂà∞‰ªÄ‰πàÂ•ΩÂêçÂ≠óÔºåÂ∞±Âè´ÊåñÁüøÊ®°ÂùóÂêß„ÄÇÊåñÁüøÊ®°ÂùóÂåÖÊã¨ ÊåñÁüøÁ∫øÁ®ãÔºàSM3ËøêÁÆóÔºâ Âå∫ÂùóÊúâÊïàÊÄßÈ™åËØÅÔºöÂΩìP2PÊúçÂä°Ê®°ÂùóÁõëÂê¨Âà∞ÂÖ∂‰ªñËäÇÁÇπÁöÑÂå∫ÂùóÂπøÊí≠ÂêéÔºåÁ´ãÂç≥ÊöÇÂÅúÊåñÁüøÁ∫øÁ®ãÔºåÂπ∂ÂØπ‰º†ÂÖ•ÁöÑÊ®°ÂùóËøõË°åËÆ§ËØÅ Êñá‰ª∂ÈòüÂàó ÂΩìÊñá‰ª∂ÈòüÂàó‰∏∫Á©∫ÁöÑÊó∂ÂÄô Âå∫ÂùóÁªìÊûÑÂå∫ÂùóÂ§¥ 12345678910&#123; "hash": "00000000d1145790a8694403d4063f323d499e655c83426834d4ce2f8dd4a2ee", "ver": 1, "prev_block": "000000002a22cfee1f2c846adbd12b3e183d4f97683f85dad08a79780a84bd55", "mrkl_root": "7dac2c5666815c17a3b36427de37bb9d2e2c5ccec3f8633eb91a4205cb4c10ff", "time": 1231731025, "bits": 486604799, "nonce": 1889418792 &#125; Âå∫Âùó‰ΩìÂå∫Âùó‰Ωì‰∏≠Â≠òÊîæÊñá‰ª∂ÁöÑHashÂÄºÔºåËøô‰∏™ÂÄºÂèØ‰ª•‰ΩøÁî®‰ªª‰ΩïÊï£ÂàóÂáΩÊï∞ÂæóÂá∫ÔºåÂΩìÁÑ∂‰πüÂèØ‰ª•Áî®SM3„ÄÇÂå∫Âùó‰Ωì‰∏≠Â≠òÊîæÂ§öÂ∞ë‰∏™Êñá‰ª∂ÁöÑHashÂÄº‰ª•ÂèäÂ≠òÊîæ‰ø°ÊÅØÁöÑÂÖ∑‰ΩìÁªìÊûÑËÆæËÆ°ÊöÇÊú™Á°ÆÂÆö„ÄÇÂè¶Â§ñÔºåËÄÅÂ∏àÁöÑÊÑèËßÅÊòØ‰∏ç‰øùÁïôÂå∫Âùó‰ΩìÔºåÊâÄÊúâËäÇÁÇπÂè™‰øùÁïôÂå∫ÂùóÂ§¥‰ª•ËäÇÁúÅÁ©∫Èó¥„ÄÇÊàëËÆ§‰∏∫‰øùÁïôÂå∫Âùó‰ΩìÂèØ‰ª•‰ΩøÂå∫ÂùóÈìæ‰∏éÊï∞ÊçÆÂ∫ìÁöÑÂØπÂ∫îÂÖ≥Á≥ªÁöÑÂª∫Á´ãÂèòÂæóÁÆÄÂçï„ÄÇÁßÅÊúâÈìæÁöÑ‰∏Ä‰∏™ÁâπÁÇπÂ∞±ÊòØÁî®Êà∑ÈáèÂ∞èÔºå‰∫ßÁîüÁöÑ‰ø°ÊÅØ‰πüÁõ∏ÂØπËæÉÂ∞ë„ÄÇÊâÄ‰ª•‰øùÁïôhashÂÄº‰ª•ÂèäÂÖ∂‰ªñËæÖÂä©‰ø°ÊÅØÔºåÂπ∂‰∏ç‰ºöÂ§™Âç†Áî®Á©∫Èó¥„ÄÇ ÊäÄÊúØÁñëÈöæ Âå∫ÂùóÈìæ‰∏éÊï∞ÊçÆÂ∫ìÊñá‰ª∂ÂØπÂ∫îÂÖ≥Á≥ªÁöÑÂª∫Á´ã P2PÁΩëÁªúÂèëÁé∞„ÄÇÂç≥Â¶Ç‰ΩïÂÆûÁé∞Êú∫ÊàøÂÜÖËäÇÁÇπÂú®ÂêØÂä®Êó∂ÂèØ‰ª•‰∫íÁõ∏ÂèëÁé∞Ôºü TCP‰º†ËæìÂ§ßÊñá‰ª∂ÁöÑÂ§ÑÁêÜ ÂΩì‰º†ËæìÊñá‰ª∂ËæÉÂ§öÔºå‰∏ä‰º†ÁöÑÈÄüÂ∫¶Â§ß‰∫éÊâÄÊúâËäÇÁÇπÁöÑÂ§ÑÁêÜÈÄüÂ∫¶Êó∂ÔºåÊòØË¶ÅÊ±ÇÊúçÂä°Âô®Á≠âÂæÖ ÊåñÁüøÂéüÁêÜ Á±ªÂûã ÂêçÁß∞ ËØ¥Êòé int32_t nVersion ÁâàÊú¨Âè∑Ôºå4Â≠óËäÇ uint256 hashPrevBlock Ââç‰∏Ä‰∏™Âå∫ÂùóÁöÑÂå∫ÂùóÂ§¥hashÂÄºÔºå32Â≠óËäÇ uint256 hashMerkleRoot ÂåÖÂê´ËøõÊú¨Âå∫ÂùóÁöÑÊâÄÊúâ‰∫§ÊòìÊûÑÈÄ†ÁöÑMerkleÊ†ëÊ†πÔºå32Â≠óËäÇ uint32_t nTime UnixÊó∂Èó¥Êà≥Ôºå4Â≠óËäÇ uint32_t nBits ËÆ∞ÂΩïÊú¨Âå∫ÂùóÈöæÂ∫¶Ôºå4Â≠óËäÇ uint32_t nNonce ÈöèÊú∫Êï∞Ôºå4Â≠óËäÇ Â¶Ç‰∏äÊØîÁâπÂ∏ÅÊØè‰∏ÄÊ¨°ÊåñÁüøÂ∞±ÊòØÂØπËøô80‰∏™Â≠óËäÇËøûÁª≠ËøõË°å‰∏§Ê¨°SHA256ËøêÁÆó(SHA256D)ÔºåËøêÁÆóÁªìÊûúÊòØÂõ∫ÂÆöÁöÑ32Â≠óËäÇ(‰∫åËøõÂà∂256‰Ωç)„ÄÇ Ê≥®:SHA256DÂç≥Ôºå SHA256D(x) =SHA256(SHA256(x)) ‰ª•‰∏ä3‰∏™Â≠óÊÆµÂèØ‰ª•ÁêÜËß£‰∏∫ÊòØÂõ∫ÂÆöÁöÑÔºåÂØπ‰∫éÊØè‰∏™ÁüøÂ∑•Êù•ËØ¥ÈÉΩ‰∏ÄÊ†∑„ÄÇÁüøÂ∑•ÂèØ‰ª•Ëá™Áî±Ë∞ÉÊï¥ÁöÑÂú∞ÊñπÊòØÂâ©‰∏ãÁöÑ3‰∏™Â≠óÊÆµÔºånNonceÔºåÊèê‰æõ2^32ÁßçÂèØËÉΩÂèñÂÄºnTimeÔºåÂÖ∂ÂÆûÊú¨Â≠óÊÆµËÉΩÊèê‰æõÁöÑÂÄºÁ©∫Èó¥ÈùûÂ∏∏ÊúâÈôêÔºåÂõ†‰∏∫ÂêàÁêÜÁöÑÂå∫ÂùóÊó∂Èó¥Êúâ‰∏Ä‰∏™ËåÉÂõ¥ÔºåËøô‰∏™ËåÉÂõ¥ÊòØÊ†πÊçÆÂâç‰∏Ä‰∏™Âå∫ÂùóÊó∂Èó¥Êù•ÂÆöÔºåÊØîÂâç‰∏Ä‰∏™Âå∫ÂùóÊó∂Èó¥Â§™Êó©ÊàñËÄÖÂ§™Ë∂ÖÂâçÈÉΩ‰ºöË¢´ÂÖ∂‰ªñËäÇÁÇπÊãíÁªù„ÄÇÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºåÂêé‰∏Ä‰∏™Âå∫ÂùóÁöÑÂå∫ÂùóÊó∂Èó¥Áï•Êó©‰∫éÂâç‰∏Ä‰∏™Âå∫ÂùóÊó∂Èó¥ÔºåËøôÊòØÂÖÅËÆ∏ÁöÑ„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåÁüøÂ∑•‰ºöÁõ¥Êé•‰ΩøÁî®Êú∫Âô®ÂΩìÂâçÊó∂Èó¥Êà≥„ÄÇhashMerkleRootÔºåÁêÜËÆ∫‰∏äÊèê‰æõ2^256ÁßçÂèØËÉΩÔºåÊú¨Â≠óÊÆµÁöÑÂèòÂåñÊù•Ëá™‰∫éÂØπÂåÖÂê´ËøõÂå∫ÂùóÁöÑ‰∫§ÊòìËøõË°åÂ¢ûÂà†ÔºåÊàñÊîπÂèòÈ°∫Â∫èÔºåÊàñËÄÖ‰øÆÊîπCoinbase‰∫§ÊòìÁöÑËæìÂÖ•Â≠óÊÆµ„ÄÇ ‰ª•‰∏äÂõæÊñáÊù•Ëá™Â∑¥ÊØîÁâπ ÊàëÊâÄË¶ÅÂÆûÁé∞ÁöÑËøô‰∏™Âå∫ÂùóÈìæÊâÄ‰ΩøÁî®ÁöÑHashÂáΩÊï∞ÔºåÊåâÁÖßËÄÅÂ∏àÁöÑË¶ÅÊ±ÇÔºåÊàë‰ª¨‰ΩøÁî®ÂõΩÂØÜÁÆóÊ≥ïSM3ÔºåÂØπ80‰∏™Â≠óËäÇËøõË°å‰∏§Ê¨°SHA256ËøêÁÆóÂ§ßÊ¶Ç‰πüÂèØ‰ª•ÊõøÊç¢‰∏∫ÂØπËøô80‰∏™Â≠óËäÇËøõË°å‰∏§Ê¨°SM3ËøêÁÆóÔºåÂç≥SM3(SM3(x)) SM3ÂõΩÂØÜÁÆóÊ≥ïSM3ÊòØ‰∏Ä‰∏™Êï£ÂàóÁÆóÊ≥ïÔºåÂú®‰øùÈöúÂÆâÂÖ®ÁöÑÂâçÊèê‰∏ãÔºåÁªºÂêàÊÄßËÉΩÊåáÊ†á‰∫éSHA-256ÂêåÁ≠âÊù°‰ª∂‰∏ãÁõ∏ÂΩì„ÄÇSM3ÂØÜÁ†ÅÊùÇÂáëÁÆóÊ≥ïÁªìÊûÑ‰∏ä‰∏éSHA-256Áõ∏‰ººÔºåÂπ∂‰∏îÈìæÊé•ÂèòÈáèÈïøÂ∫¶ÔºåÊ∂àÊÅØÂàÜÁªÑÂ§ßÂ∞èÂíåÊ≠•Êï∞Âùá‰∏éSHA-256Áõ∏Âêå„ÄÇÊàë‰ª¨‰ΩøÁî®ÂÆÉÊù•ËÆ°ÁÆóÊñá‰ª∂ÁöÑHashÂÄº ÂÖ≥‰∫éSM3ÁöÑÊÄßËÉΩÊàëÊÉ≥Áü•ÈÅìSM3ÂØπÊñá‰ª∂ËøõË°åHashÁöÑÊÄßËÉΩÔºå‰∫éÊòØÁîüÊàê‰∫Ü‰∏çÂêåÂ§ßÂ∞èÁöÑÊñá‰ª∂ËøõË°åÂ§öÊ¨°ÊµãËØïÔºåÊµãËØïÁªìÊûúÂ¶Ç‰∏ãÔºö Êìç‰ΩúÁ≥ªÁªüÔºöwindows10 CPUÔºö4Ê†∏ 2.5GHz‰∏ªÈ¢ë ÂÜÖÂ≠òÔºö12GB Êñá‰ª∂Â§ßÂ∞è ËÄóÊó∂ ÔºàÂåÖÊã¨Á°¨ÁõòËØªÂèñÊñá‰ª∂ÁöÑÊó∂Èó¥Ôºâ 1KB 0.004s 4KB 0.004s 1MB 0.026s 10MB 0.367s 20MB 0.723s 100MB 3.466s 200MB 6.853s 500MB 17.082s 1GB 34.894s 2GB 69.084s Ê†πÊçÆÊµãËØïÁªìÊûúÊàë‰ª¨ËÆ§‰∏∫ÔºåËÆ°ÁÆóHashÂÄºÊ∂àËÄóÁöÑÊó∂Èó¥‰∏éÊñá‰ª∂ÁöÑÂ§ßÂ∞èÊòØÊàêÊ≠£ÊØîÁöÑ„ÄÇÂπ∂‰∏îÂú®ÂΩìÂâçÊµãËØïÁéØÂ¢É‰∏ãÔºåSM3‰∏çÈÄÇÂêàÁî®Êù•ËÆ°ÁÆóÂ§ßÂ∞èÂú®20MB‰ª•‰∏äÁöÑÊñá‰ª∂ÁöÑHashÂÄº„ÄÇÂõ†‰∏∫Ë∂ÖËøá20MB‰πãÂêéÔºåËøêÁÆóÂçï‰∏™Êñá‰ª∂ÁöÑÊó∂Èó¥‰ºöËææÂà∞ÁßíÁ∫ßÔºåÁ≥ªÁªü‰ºöÊúâÊòéÊòæÁöÑÁ≠âÂæÖ„ÄÇËøôÊ†∑‰ºöÈôç‰ΩéÂå∫ÂùóÈìæÁöÑËøêË°åÊïàÁéá„ÄÇÂ¶ÇÊûúÈúÄË¶ÅËøõ‰∏ÄÊ≠•Áº©Áü≠ËÄóÊó∂ÔºåÈÇ£‰πàÂèØ‰ª•ÈáçÁÇπËÄÉËôë‰ΩøÁî®ÊüêÁßçÁÆóÊ≥ïÁº©Áü≠Â§ßÊñá‰ª∂ÁöÑËØªÂèñÊó∂Èó¥ Ê≥®ÔºöÂÜçWindows‰∏ãÁîüÊàêÊåáÂÆöÂ§ßÂ∞èÊñá‰ª∂ÁöÑÂëΩ‰ª§‰∏∫Ôºöfsutil file createnew (Êñá‰ª∂Âêç) &lt;Â≠óËäÇÊï∞&gt; 1fsutil file createnew 1KB.testfile 1024 Âè¶Ê≥®: ‰ª•‰∏äÁöÑÊµãËØïËøáÁ®ãÊòØ‰∏Ä‰∏™Â∞èÁôΩÈöè‰æøÂºÑÁöÑÔºå‰∏ãÈù¢ÁöÑÊµãËØïÁªìÊûúÊù•Ëá™ÁéãÂ∞è‰∫ëÈô¢Â£´ÂèëË°®ÁöÑÊúüÂàäÊñáÁ´† ËΩØÁ°¨‰ª∂ÂÆûÁé∞ÊÄßËÉΩÔºà‰∏ì‰∏öÔºâ ÈÄöËøá‰∏ì‰∏öÁöÑËØÑÊµãÂèØ‰ª•Áü•ÈÅìÔºåÈÄâÊã©SM3Âπ∂Ê≤°Êúâ‰ªÄ‰πà‰∏çÂ¶•„ÄÇÂõ†‰∏∫SM3Ë∑üSHA-256ÈöæÂàÜ‰ºØ‰ª≤„ÄÇ Âå∫ÂùóÈìæÂÆâÂÖ®ÈóÆÈ¢òÈ¶ñÂÖàÊàë‰ª¨Áü•ÈÅìÊØîÁâπÂ∏ÅÂå∫ÂùóÈìæÁöÑÂÆâÂÖ®ÈóÆÈ¢òÊúâ ‚ÄúÂèåËä±‚Äù ‚Äú51%ÊîªÂáª‚Äù ÂΩìÁÑ∂‰∏äËø∞ÈóÆÈ¢òÂÜçÊØîÁâπÂ∏ÅÂå∫ÂùóÈìæ‰∏≠ÈÉΩÂ∑≤ÁªèËß£ÂÜ≥„ÄÇÈÇ£‰πàÂåªÈô¢ÁßÅÊúâÂå∫ÂùóÈìæÊòØÂê¶‰πüÂ≠òÂú®ËøôÊ†∑ÁöÑÈóÆÈ¢òÔºü ÂèåËä±ÂèåËä±ÁöÑÂéüÁêÜÊöÇÁï•„ÄÇÂèåËä±ÂèØ‰ª•ÊàêÂäüÁöÑÂÖ≥ÈîÆÂú®‰∫é‰∫§ÊòìÁöÑÊî∂Èí±ÊñπÂú®‰∫§ÊòìÊ≤°ÊúâË¢´Ê∞∏‰πÖÁ°ÆËÆ§ÁöÑÊÉÖÂÜµ‰∏ãÊääËá™Â∑±ÁöÑÊä´Ëê®Áªô‰ªòÈí±Êñπ„ÄÇÂèåËä±ÂøÖÁÑ∂‰∏ç‰ºöÊàêÂäüÔºåÂõ†‰∏∫ÊØîÁâπÂ∏ÅÁöÑÂçèËÆÆÂ∑≤ÁªèÈôêÂà∂Ê≠ªÔºåÂà∞Êó∂ÂÄô‰∏§Á¨î‰∫§ÊòìÂøÖÁÑ∂Âè™ËÉΩÁúüÊ≠£ÂÆåÊàê‰∏ÄÁ¨î„ÄÇÂè¶Â§ñ‰∏ÄÁ¨î‰ºöÂõ†‰∏∫Â§Ñ‰∫éÁü≠ÈìæËÄåË¢´‰∏¢ÂºÉ„ÄÇÈÄ†ÊàêÊî∂Èí±ÊñπÂú®Ê≤°ÊúâÊî∂Âà∞ÊØîÁâπÂ∏ÅÁöÑÊÉÖÂÜµ‰∏ãÂ∞±ÂÆåÊàê‰∫Ü‰∫§ÊòìÊú¨Ë∫´ÔºåÊØîÂ¶Ç‰π∞‰∏Ä‰∏™Êä´Ëê®„ÄÇÂØπ‰∫éÂçñÊä´Ëê®ÁöÑ‰∫∫Êù•ËØ¥ÔºåÂ∞±ÊòØ‰∫èÊçü‰∫Ü„ÄÇ‰ΩÜÊòØÂå∫ÂùóÈìæÂπ∂Ê≤°ÊúâÂΩ±ÂìçÊâÄ‰ª•ÂåªÈô¢Âå∫ÂùóÈìæ‰πüÊòØ‰∏çÂ≠òÂú®Ëøô‰∏™ÈóÆÈ¢òÁöÑ„ÄÇ 51%ÊîªÂáªÂÆÉÊòØÊåá‰∏Ä‰∏™‰∫∫ÊàñËÄÖ‰∏Ä‰∏™ÁªÑÁªáÊéåÊè°‰∫ÜÊï¥‰∏™Á≥ªÁªüÁôæÂàÜ‰πã51‰ª•‰∏äÁöÑÁÆóÂäõÔºå‰ªéËÄåÂèØ‰ª•‰øÆÊîπËá™Â∑±ÁöÑ‰∫§ÊòìËÆ∞ÂΩïÁ≠âÁ≠â„ÄÇËøôÂØπÊØîÁâπÂ∏ÅÊù•ËØ¥Âá†‰πé‰∏çÂèØËÉΩÂèëÁîüÔºå‰ΩÜÊòØÂØπ‰∫éËøô‰πàÂ∞èÂûãÁöÑ‰∏Ä‰∏™Âå∫ÂùóÈìæÊù•ËØ¥ÔºåËøòÊòØÂæàÊúâÂèØËÉΩÁöÑ„ÄÇÊâÄ‰ª•ÁùÄÂ∞±ÂÖ≥Á≥ªÂà∞ÂåªÈô¢Âå∫ÂùóÈìæÊòØÂê¶Ë¶ÅÈù¢ÂêëÁ§æ‰ºöÊãõÂãüÁüøÂ∑•‰∫Ü„ÄÇ‰∏ÄÊó¶Èù¢ÂêëÁ§æ‰ºöÂºÄÊîæÔºåËÄå‰∏îËßÑÊ®°Âèà‰∏çÂ§üÊó∂ÔºåÈÇ£‰πàÂ∞ÜÂ§ß‰∏çË∂≥‰∏éÊäµÂæ°51%ÊîªÂáª„ÄÇÂ¶ÇÊûúÊòØÂ∞ÅÈó≠ÁöÑÂ±ÄÂüüÁΩëÁéØÂ¢ÉÔºåÂπ∂‰∏îÁΩëÁªúÂÆâÂÖ®ÂÅöÁöÑÂ§üÂ•ΩÔºåÂ∞±ÂèØ‰ª•ÈÅøÂÖçÊù•Ëá™Â§ñÁïåÁöÑÊîªÂáª„ÄÇÂè™ÈúÄË¶ÅÂÅöÂ•ΩÂåªÈô¢ÂÜÖÈÉ®ÁöÑÈò≤ËåÉÂç≥ÂèØ„ÄÇÂπ∂‰∏îÔºå‰∏çÂØπÂ§ñÂºÄÊîæÔºåÂ∞±ÊÑèÂë≥ÁùÄ‰ªª‰Ωï‰∫∫ÊõøÂåªÈô¢ÊåñÁüøÈÉΩÊòØÊó†Âà©ÂèØÂõæÁöÑÔºåÂåªÈô¢Ëá™Ë¥üÁîµË¥π„ÄÇ Â∑≤ÁªèÂÜôÂÖ•ÁöÑ‰ø°ÊÅØÊòØÂê¶ÂèØÊõ¥ÊîπÁêÜËÆ∫‰∏äÂΩìÁÑ∂ÂèØ‰ª•Êõ¥Êîπ„ÄÇÊÉ≥Ë¶Å‰øÆÊîπÊüê‰∏Ä‰∏™ËÆ∞ÂΩïÁöÑËØùÔºåÂè™Ë¶ÅÊääÂåªÈô¢ÁôæÂàÜ‰πã51‰ª•‰∏äÁöÑËäÇÁÇπ‰∏äÁöÑÂå∫ÂùóÈìæ‰ø°ÊÅØÂÅöÁõ∏Â∫î‰øÆÊîπ„ÄÇÁÑ∂ÂêéÊääÊú∫ÊàøÂÅúÁîµÔºåËÆ©ÂÖ®ÈÉ®ËäÇÁÇπÈáçÂêØ„ÄÇËøôÊ†∑ÔºåËäÇÁÇπÂú®ÂàùÂßãÂåñÁöÑÊó∂ÂÄô‰ºö‰ªéÂÖ∂‰ªñËäÇÁÇπËé∑ÂèñÊúÄÊñ∞ÁöÑÂå∫ÂùóÈìæ‰ø°ÊÅØ„ÄÇÁî±‰∫é 51%ÁöÑËäÇÁÇπÂ≠òÊîæÁöÑÈÉΩÊòØÁØ°ÊîπËøáÁöÑ‰ø°ÊÅØ„ÄÇÈÇ£‰πàÂâ©‰ΩôÁöÑ49%ÁöÑËäÇÁÇπÂ∞±‰ºöÊîæÂºÉËá™Â∑±Â≠òÂÇ®ÁöÑÂå∫ÂùóÈìæ‰ø°ÊÅØËÄåÂéªÈÄâÊã©Áõ∏‰ø°Âè¶Â§ñÁöÑ51%ÁöÑËäÇÁÇπ„ÄÇÁ≠âÂÖ®ÈÉ®ËäÇÁÇπÂàùÂßãÂåñÂÆåÊàêÔºåÈÇ£‰πà‰∏ÄÊù°Â¥≠Êñ∞ÁöÑÈìæÂ∞±Âá∫Áé∞‰∫Ü„ÄÇÊòØ‰∏çÊòØÂæàÁÆÄÂçïÔºü ÊîªÂáªÁöÑÂä®Êú∫ÂÜçÊù•ËÆ®ËÆ∫‰∏Ä‰∏ãÊîªÂáªÁöÑÂä®Êú∫„ÄÇÊúâÂä®Êú∫ÂØπÂå∫ÂùóÈìæËøõË°å‰øÆÊîπÁöÑÂèØËÉΩÊòØÂºÄÈîôËçØÁöÑÂåªÁîüÔºåÂèàÊàñËÄÖÁßëÂÆ§‰πãÈó¥Âêà‰ΩúÁöÑÊó∂ÂÄôÂá∫Áé∞Â§±ËØØÁöÑ‰∏ÄÊñπ„ÄÇÊØîÂ¶ÇÔºåAÁßëÂÆ§Ë¶ÅÊ±ÇBÈÉ®Èó®Êèê‰æõÊüê‰∫õÊï∞ÊçÆÔºå‰ΩÜÊòØBÈÉ®Èó®ÂèëÈÄÅÁöÑÊï∞ÊçÆÂá∫ÈîôÔºåÂØºËá¥AÁßëÂÆ§ÁöÑÊ≤ªÁñóÂá∫Áé∞‰∫ãÊïÖÔºåÊ≠§Êó∂ÈúÄË¶ÅËøΩË¥£ÔºåÈÇ£‰πàBÈÉ®Èó®Â∞±Êúâ‰∫ÜÂØπÂå∫ÂùóÈìæÊõ¥ÊîπÁöÑÂä®Êú∫„ÄÇÂç≥‰ΩøÊúâÂä®Êú∫ÔºåÂè™Ë¶Å‰ªñ‰ª¨Êó†Ê≥ïÂπ≤È¢ÑÊú∫ÊàøËäÇÁÇπÁöÑËøêË°åÔºåÈÇ£‰πà‰ªñ‰ª¨Â∞±Âü∫Êú¨‰∏çÂèØËÉΩÂéª‰øÆÊîπ„ÄÇ ÊÄªÁªìÂú®Âå∫ÂùóÈìæËßÑÊ®°‰∏çÂ§ßÁöÑÊó∂ÂÄôÔºåÊúÄÂ•Ω‰ΩøÂÖ∂ËøêË°åÂú®Â∞ÅÈó≠ÁöÑÂ±ÄÂüüÁΩëÁéØÂ¢É‰∏ã„ÄÇËøôÊ†∑Êó¢ÂèØ‰ª•ËßÑÈÅøÈ£éÈô©ÔºåÂèàÈôç‰Ωé‰∫ÜÂºÄÂèëÈöæÂ∫¶„ÄÇ ÂÖ±ËØÜÊú∫Âà∂Âç†Âùë Â∑•‰ΩúÈáèËØÅÊòéÂ∑•‰ΩúÈáèËØÅÊòéÊòØ‰∏∫‰∫ÜÂä†Â§ßÂå∫ÂùóÁîüÊàêÁöÑÈöæÂ∫¶ÔºåËÆ©Âçï‰∏™ËäÇÁÇπ‰∏çËÉΩËøûÁª≠ÁöÑ‰∫ßÁîüÂå∫Âùó„ÄÇÂõ†Ê≠§ËäÇÁÇπ‰πãÈó¥ÊòØÊúâÁ´û‰∫âÂÖ≥Á≥ªÁöÑ Âå∫ÂùóÈìæÁöÑÊåÅ‰πÖÂåñÊàëÂÜ≥ÂÆöÈááÁî®JSONÊ†ºÂºèÊù•‰øùÂ≠òÊú¨Âú∞ÁöÑÂå∫ÂùóÈìæ‰ø°ÊÅØÔºåÂÆÉÁöÑ‰ºòÁÇπÂæàÊòéÊòæÔºåÁªìÊûÑÊ∏ÖÊô∞„ÄÇÂõ†‰∏∫ÊúâÁé∞ÊàêÁöÑÂ∫ìÂèØ‰ª•‰ΩøÁî®ÔºåÊâÄ‰ª•ÁâπÂà´Êñπ‰æø„ÄÇ]]></content>
      <tags>
        <tag>Âå∫ÂùóÈìæ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VSCodeÊèí‰ª∂ÂºÄÂèë]]></title>
    <url>%2F2018%2F05%2F05%2FVSCode%E6%8F%92%E4%BB%B6%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[ÊëòË¶ÅÔºö ÊÉ≥‰ª•ÂêéÁ†îÁ©∂‰∏Ä‰∏ãVSCodeÁöÑÊèí‰ª∂ÂºÄÂèëÔºåÊâÄ‰ª•Â∞±ÂÜ≥ÂÆöÊääÂÆòÊñπÊñáÊ°£ÁøªËØë‰∏Ä‰∏ã„ÄÇÂΩìÁÑ∂‰∫ÜÔºåÂ§ßÈÉ®ÂàÜÊòØÊú∫Âô®ÁøªËØëÔºåÊàëÂè™ÊòØÊääÁøªËØëÁöÑÂ•áÊÄ™ÁöÑÂú∞ÊñπÂíå‰∏çÈÄöÈ°∫ÁöÑÂú∞ÊñπÁ®çÂæÆÊîπÊîπÔºå‰∏çÂΩ±ÂìçÁêÜËß£Â∞±Êàê„ÄÇü§£ Ê¶ÇË≤åÂ¶ÇÊûú‰Ω†ÊúâÂÖ¥Ë∂£Êâ©Â±ïVS CodeÔºå‰Ω†ÊòØÂú®Ê≠£Á°ÆÁöÑÂú∞Êñπ„ÄÇ Âú®ËøôÈáåÔºåÊàë‰ª¨Êèê‰æõVS CodeÊâ©Â±ïÊÄßÊñáÊ°£ÁöÑÊ¶ÇË¶Å‰ª•ÂèäÂ¶Ç‰ΩïÂø´ÈÄüÊûÑÂª∫ÊÇ®ÁöÑÁ¨¨‰∏Ä‰∏™VS CodeÊâ©Â±ï„ÄÇ Â¶ÇÊûúÊÇ®ÂØπÊàë‰ª¨ÁöÑVS CodeÁöÑÊâ©Â±ïÊÄßËÆæËÆ°ÊñπÊ≥ïÊÑüÂà∞Â•ΩÂ•áÔºåÂèØ‰ª•Âú®ËøôÈáåÈòÖËØª„ÄÇ Â¶ÇÊûúÊÇ®Âè™ÊÉ≥‰ΩøÁî®Áé∞ÊúâÁöÑÊâ©Â±ïÔºåËØ∑ÂèÇÈòÖExtension Marketplace‰∏ªÈ¢òÔºåÊàë‰ª¨Â∞ÜÂêëÊÇ®Â±ïÁ§∫Â¶Ç‰Ωï‰ªéVS Code Marketplace‰∏≠Êü•ÊâæÂíåÂÆâË£ÖÊâ©Â±ï„ÄÇ ÊâÄÊúâVS‰ª£Á†ÅÊâ©Â±ïÂÖ±‰∫´Ë¥°ÁåÆÔºàÊ≥®ÂÜåÔºâÔºåÊøÄÊ¥ªÔºàÂä†ËΩΩÔºâÂíåËÆøÈóÆVS‰ª£Á†ÅÊâ©Â±ïÊÄßAPIÁöÑÈÄöÁî®Ê®°Âûã„ÄÇ ÁÑ∂ËÄåÔºåVS‰ª£Á†ÅÊâ©Â±ïÊúâ‰∏§ÁßçÁâπÊÆäÁöÑÈ£éÊ†ºÔºåËØ≠Ë®ÄÊúçÂä°Âô®ÂíåË∞ÉËØïÂô®ÔºåÂÆÉ‰ª¨ÈÉΩÊúâËá™Â∑±ÁöÑÈôÑÂä†ÂçèËÆÆÔºåÂπ∂Âú®ÊñáÊ°£ÁöÑÂêÑ‰∏™ÈÉ®ÂàÜ‰∏≠ËøõË°å‰∫Ü‰ªãÁªç„ÄÇ Êâ©Â±ï - Âü∫Á°ÄÊûÑÂª∫Âùó ËØ≠Ë®ÄÊúçÂä°Âô® - Áî®‰∫éÈ´òÊàêÊú¨ÁöÑIOÊàñCPUÂØÜÈõÜÂûã‰ªªÂä° Ë∞ÉËØïÂô® - ÈÄöËøáË∞ÉËØïÈÄÇÈÖçÂô®ËøûÊé•Â§ñÈÉ®Ë∞ÉËØïÂô® ExtensionsÊøÄÊ¥ªÊó∂ÊâÄÊúâÊâ©Â±ïÂú®Êàë‰ª¨ÁöÑÂÖ±‰∫´Êâ©Â±ïÂÆø‰∏ªËøõÁ®ã‰∏≠ËøêË°å„ÄÇ ËøôÁßçÊâ©Â±ïÁöÑÂçïÁã¨ÊµÅÁ®ãÂèØÁ°Æ‰øùVS‰ª£Á†ÅÂßãÁªà‰øùÊåÅÂìçÂ∫î„ÄÇ Êâ©Â±ïÂåÖÊã¨ÊîØÊåÅÔºö ÊøÄÊ¥ª - Âú®Ê£ÄÊµãÂà∞ÁâπÂÆöÊñá‰ª∂Á±ªÂûãÊó∂ÔºåÁâπÂÆöÊñá‰ª∂Â≠òÂú®Êó∂ÔºåÊàñÈÄöËøáÂëΩ‰ª§ÈÄâÈ°πÊùøÊàñÁªÑÂêàÈîÆÈÄâÊã©ÂëΩ‰ª§Êó∂Âä†ËΩΩÊâ©Â±ïÂêç ÁºñËæë - Â§ÑÁêÜÁºñËæëÁöÑÂÜÖÂÆπ - ÈòÖËØªÂíåÊìç‰ΩúÊñáÊú¨ÔºåÂà©Áî®ÈÄâÊã©ÔºàsÔºâ Â∑•‰ΩúÂå∫ - ËÆøÈóÆÊâìÂºÄÁöÑÁºñËæëÂô®ÔºåÁä∂ÊÄÅÊ†èÔºå‰ø°ÊÅØÊ∂àÊÅØÁ≠âÁ≠â ‰∫ã‰ª∂ - ËøûÊé•Âà∞ÁºñËæëÂô®ÁîüÂëΩÂë®Êúü‰∫ã‰ª∂Ôºå‰æãÂ¶ÇÔºöÊâìÂºÄÔºåÂÖ≥Èó≠ÔºåÊõ¥ÊîπÁ≠â ÊºîÂèòÁºñËæë - ‰∏∫‰∏∞ÂØåÁöÑËØ≠Ë®ÄÊîØÊåÅÂàõÂª∫Êèê‰æõÂïÜÔºåÂåÖÊã¨IntelliSenseÔºåPeekÔºåÊÇ¨ÂÅúÔºåËØäÊñ≠Á≠âÁ≠â Êàë‰ª¨Êúâ‰∏§‰∏™Á´ØÂà∞Á´ØÁöÑÊïôÁ®ãÔºåËÆ©ÊÇ®‰∫ÜËß£Êâ©Â±ïÂü∫Á°ÄÁü•ËØÜÔºö Hello World - ÁîüÊàêÂü∫Êú¨Êâ©Â±ïÂêçÔºåÁêÜËß£Êâ©Â±ïÁöÑÊñá‰ª∂Â§πÁªìÊûÑÔºåÊâ©Â±ïÂêçÊ∏ÖÂçïÔºå‰∫ÜËß£ÊøÄÊ¥ªÂ¶Ç‰ΩïÂ∑•‰ΩúÔºåËøêË°åÂíåË∞ÉËØïÊâ©Â±ïÂπ∂Âú®Êú¨Âú∞ÂÆâË£Ö„ÄÇ Â≠óÊï∞ - Ê†πÊçÆÁâπÂÆöÊñá‰ª∂Á±ªÂûãÊøÄÊ¥ªÔºåÊõ¥Êñ∞Áä∂ÊÄÅÊ†èÔºåÂìçÂ∫îÊñáÊú¨ÁºñËæëÂô®‰∏≠ÁöÑÊõ¥ÊîπÔºåÂπ∂Âú®Á¶ªÂºÄÊñá‰ª∂Êó∂Â§ÑÁΩÆÊâ©Â±ïÂêç„ÄÇ Êâ©Â±ïÊÄßÂéüÂàôÂíåÊ®°Âºè‰πüÂæàÊúâÁî®ÔºåÂÆÉÊèèËø∞‰∫ÜÊï¥‰∏™Êâ©Â±ïÊÄßAPI‰ΩøÁî®ÁöÑÂÖ±‰∫´ÁºñÁ®ãÊ®°Âºè„ÄÇ Language ServersËØ≠Ë®ÄÊúçÂä°Âô®ÂèØËÆ©ÊÇ®‰∏∫ÊÇ®ÁöÑÊâ©Â±ïÁ®ãÂ∫èÂàõÂª∫‰∏ìÁî®ÊµÅÁ®ã„ÄÇ ÂΩìÊÇ®ÁöÑÊâ©Â±ïÁ®ãÂ∫èËøêË°åÈ´òÊàêÊú¨ÁöÑCPUÊàñIOÂØÜÈõÜÂûã‰ªªÂä°Êó∂ÔºåËøôÂèØËÉΩ‰ºöÂª∂ÁºìÂÖ∂‰ªñÊâ©Â±ïÔºåËøôÂØπ‰∫éÊÇ®ÁöÑÊâ©Â±ïÊòØ‰∏Ä‰∏™ÊúâÁî®ÁöÑËÆæËÆ°ÈÄâÊã©„ÄÇ ËøôÂØπ‰∫éÂú®Â∑•‰ΩúÂå∫‰∏≠ÁöÑÊâÄÊúâÊñá‰ª∂‰∏≠Â∑•‰ΩúÁöÑ‰ªªÂä°ÊòØÂæàÂ∏∏ËßÅÁöÑÔºå‰æãÂ¶Ç (linters or static analysis suites)Ê£âÁªíÊàñÈùôÊÄÅÂàÜÊûêÂ•ó‰ª∂ËØ¶ÁªÜ‰∫ÜËß£ËØ≠Ë®ÄÊúçÂä°Âô®„ÄÇ Debug AdapterVS CodeÂÆûÁé∞‰∫Ü‰∏Ä‰∏™ÈÄöÁî®ÁöÑË∞ÉËØïÂô®UIÔºåÂπ∂‰æùËµñ‰∫éË∞ÉËØïÂô®Êâ©Â±ïÂíåÊâÄË∞ìÁöÑ‚ÄúË∞ÉËØïÈÄÇÈÖçÂô®‚ÄùÊù•Â∞ÜË∞ÉËØïÁî®Êà∑ÁïåÈù¢ËøûÊé•Âà∞ÁúüÂÆûÁöÑË∞ÉËØïÂô®ÊàñËøêË°åÊó∂„ÄÇ Ë∞ÉËØïÈÄÇÈÖçÂô®ÊòØ‰∏Ä‰∏™‰∏ìÁî®ËøõÁ®ãÔºåÈÄöËøáVS‰ª£Á†ÅË∞ÉËØïÂçèËÆÆ‰∏éVS‰ª£Á†ÅËøõË°åÈÄö‰ø°ÔºåÂπ∂‰∏îÂèØ‰ª•Áî®‰ªª‰ΩïËØ≠Ë®ÄÂÆûÁé∞„ÄÇ ‰∫ÜËß£Êõ¥Â§öÂÖ≥‰∫éÂàõÂª∫Ë∞ÉËØïÂô®Êâ©Â±ïÁöÑ‰ø°ÊÅØ„ÄÇ Êü•ÁúãVS‰ª£Á†ÅÊâ©Â±ïÁöÑÊúÄÁÆÄÂçïÊñπÊ≥ïÊòØÈÄöËøáÊâ©Â±ïÂ∏ÇÂú∫„ÄÇ ‰Ω†ÂèØ‰ª•ÊµèËßàÊúâÁî®ÁöÑÊâ©Â±ïÔºåÂÆâË£ÖÂÆÉ‰ª¨Êù•ËØïÁî®ÂÆÉ‰ª¨ÔºåÂπ∂‰∫ÜËß£Â¶Ç‰Ωï‰∏∫Ëá™Â∑±ÁöÑÂºÄÂèëÂú∫ÊôØÊâ©Â±ïVS Code„ÄÇ Language Extension GuidelinesËØ≠Ë®ÄÊâ©Â±ïÊåáÂçó‰∏ªÈ¢òÂèØ‰ª•Â∏ÆÂä©ÊÇ®ÂÜ≥ÂÆöÊÇ®ÁöÑÊâ©Â±ïÊîØÊåÅÂì™‰∫õËØ≠Ë®ÄÂäüËÉΩ„ÄÇ ÂÆÉÊòæÁ§∫‰∫ÜVS Code‰∏≠Êèê‰æõÁöÑÂêÑÁßçËØ≠Ë®ÄÂäüËÉΩÔºà‰æãÂ¶ÇÔºå‰ª£Á†ÅÂª∫ËÆÆÂíåÊìç‰ΩúÔºåÊ†ºÂºèËÆæÁΩÆÔºåÈáçÂëΩÂêçÔºâÔºå‰ª•ÂèäÂ¶Ç‰ΩïÈÄöËøáËØ≠Ë®ÄÊúçÂä°Âô®ÂçèËÆÆÊàñÁõ¥Êé•‰ΩøÁî®Êâ©Â±ïAPI‰ªéÊÇ®ÁöÑÊâ©Â±ïÂÆûÁé∞ÂÆÉ‰ª¨„ÄÇ Themes, Snippets, and ColorizersÊÇ®ÂèØ‰ª•ÈÄöËøáËØ≠Ê≥ïÁ™ÅÂá∫ÊòæÁ§∫ÔºåÊúâÁî®ÁöÑÁâáÊÆµÂíåÁ≤æÂøÉËÆæËÆ°ÁöÑÈ¢úËâ≤‰∏ªÈ¢òÁ≠âÁÆÄÂçïÁöÑ‰∏úË•øÔºå‰∏∫ÁºñÁ®ãËØ≠Ë®ÄÊèê‰æõÂá∫Ëâ≤ÁöÑÁºñËæë‰ΩìÈ™å„ÄÇ TextMateËá™ÂÆö‰πâÊñá‰ª∂Êèê‰æõ‰∫ÜËøôÁßçÊîØÊåÅÔºåVS CodeÂÖÅËÆ∏ÊÇ®ËΩªÊùæÊâìÂåÖÂíåÈáçÁî®Ëøô‰∫õÊñá‰ª∂Ôºå‰ª•‰æøÊÇ®ÂèØ‰ª•Âú®Êâ©Â±ï‰∏≠Áõ¥Êé•‰ΩøÁî®.tmThemeÔºå.tmSnippetsÂíå.tmLanguageÊñá‰ª∂„ÄÇ Êàë‰ª¨ÁöÑ‰∏ªÈ¢òÔºåÁâáÊÆµÂíåÁùÄËâ≤Âô®‰∏ªÈ¢òÂêëÊÇ®Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂåÖÂê´TextMateÊñá‰ª∂ÔºåÂπ∂Â∞±Â¶Ç‰ΩïÂàõÂª∫Ëá™Â∑±ÁöÑ‰∏ªÈ¢òÔºåÁâáÊÆµÂíåËØ≠Ë®ÄÁùÄËâ≤Âô®Êèê‰æõÊåáÂØº„ÄÇ Writing an ExtensionÊúâ‰∏Ä‰∏™YeomanÊâ©Â±ïÁîüÊàêÂô®ÔºåÂèØ‰ª•ÈùûÂ∏∏ÂÆπÊòìÂú∞ÂàõÂª∫ÁÆÄÂçïÁöÑÊâ©Â±ïÈ°πÁõÆ„ÄÇ Ëøô‰∫õÂØπ‰∫éÂºÄÂßãÂæàÊúâÁî®ÔºåÊÇ®ËøòÂèØ‰ª•ÊâæÂà∞Áé∞ÊúâÁöÑÊâ©Â±ïÁ§∫‰æã„ÄÇ Êâ©Â±ïÂèØ‰ª•Áî®TypeScriptÊàñJavaScriptÁºñÂÜô„ÄÇ VS CodeÊèê‰æõ‰∫Ü‰∏ÄÊµÅÁöÑÊâ©Â±ïÂºÄÂèëÁªèÈ™åÔºåÊÇ®ÂèØ‰ª•Âú®VS CodeÊú¨Ë∫´ÂÜÖÂºÄÂèëÔºåÊûÑÂª∫ÔºåËøêË°åÔºåÊµãËØïÂíåË∞ÉËØïÊâÄÊúâÂÜÖÂÆπ„ÄÇ Testing ExtensionsÊàë‰ª¨‰πüÈùûÂ∏∏ÊîØÊåÅ‰∏∫ÊÇ®ÁöÑÊâ©Â±ïÁ®ãÂ∫èÁºñÂÜôÂíåËøêË°åÊµãËØï„ÄÇ ÊÇ®ÂèØ‰ª•ËΩªÊùæÂàõÂª∫Ë∞ÉÁî®VS Code APIÁöÑÈõÜÊàêÊµãËØïÔºåÂπ∂Âú®ËøêË°åÁöÑVS CodeÂÆû‰æã‰∏≠ÊµãËØïÊÇ®ÁöÑ‰ª£Á†Å„ÄÇ Extension IdeasVS CodeÂäüËÉΩÁöÑËÆ∏Â§ö‰ºüÂ§ßÁöÑÁ§æÂå∫ÁêÜÂøµÊõ¥Â•ΩÂú∞ÂÆûÁé∞‰∏∫Êâ©Â±ïËÄå‰∏çÊòØÊ†∏ÂøÉ‰∫ßÂìÅÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ ËøôÊ†∑Áî®Êà∑Â∞±ÂèØ‰ª•ÈÄöËøáÂÆâË£ÖÊ≠£Á°ÆÁöÑÊâ©Â±ïÈõÜÊù•ËΩªÊùæÈÄâÊã©‰ªñ‰ª¨ÊÉ≥Ë¶ÅÁöÑÂäüËÉΩ„ÄÇ VS CodeÂõ¢ÈòüÂ∞ÜÂèØËÉΩÁöÑÊâ©Â±ïË∑üË∏™‰∏∫vscodeÂ≠òÂÇ®Â∫ì‰∏≠Ê†áËÆ∞‰∏∫ * extension-candidateÁöÑGitHubÈóÆÈ¢ò„ÄÇ Â¶ÇÊûú‰Ω†Ê≠£Âú®ÂØªÊâæ‰∏Ä‰∏™‰ºüÂ§ßÁöÑÊâ©Â±ïÊù•ÊûÑÂª∫ÔºåÁúãÁúã * Êâ©Â±ïÂÄôÈÄâÈóÆÈ¢ò„ÄÇ Next StepsYo Code - Extension GeneratorPrerequisitesInstall the GeneratorRun Yo CodeGenerator OptionsYour extensions folderNext StepsÁ§∫‰æã - Hello World‰Ω†ÁöÑÁ¨¨‰∏Ä‰∏™Êèí‰ª∂Êú¨ÊñáÊ°£Â∞ÜÊåáÂØºÊÇ®ÂàõÂª∫ÊÇ®ÁöÑÁ¨¨‰∏Ä‰∏™VS‰ª£Á†ÅÊâ©Â±ïÔºà‚ÄúHello World‚ÄùÔºâÂπ∂Ëß£ÈáäÂü∫Êú¨ÁöÑVS‰ª£Á†ÅÊâ©Â±ïÊÄßÊ¶ÇÂøµ„ÄÇ Âú®Êú¨ÊºîÁªÉ‰∏≠ÔºåÊÇ®Â∞ÜÂêëVS CodeÊ∑ªÂä†‰∏Ä‰∏™Êñ∞ÂëΩ‰ª§ÔºåËØ•ÂëΩ‰ª§Â∞ÜÊòæÁ§∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‚ÄúHello World‚ÄùÊ∂àÊÅØ„ÄÇ Á®çÂêéÂú®ÊºîÁªÉ‰∏≠ÔºåÊÇ®Â∞Ü‰∏éVS CodeÁºñËæëÂô®‰∫§‰∫íÂπ∂Êü•ËØ¢Áî®Êà∑ÂΩìÂâçÈÄâÂÆöÁöÑÊñáÊú¨„ÄÇ ÂÖàÂÜ≥Êù°‰ª∂ÔºàPrerequisitesÔºâÊÇ®ÈúÄË¶ÅÂÆâË£ÖNode.jsÂπ∂Âú®ÊÇ®ÁöÑ$ PATH‰∏≠ÂèØÁî®„ÄÇ Node.jsÂåÖÂê´npmÔºåÂç≥Node.jsÂåÖÁÆ°ÁêÜÂô®ÔºåÂÆÉÂ∞ÜÁî®‰∫éÂÆâË£ÖÊâ©Â±ïÁîüÊàêÂô®„ÄÇ ÁîüÊàê‰∏Ä‰∏™Êñ∞ÁöÑÊâ©Â±ïÂ∞ÜËá™Â∑±ÁöÑÂäüËÉΩÊ∑ªÂä†Âà∞VS CodeÁöÑÊúÄÁÆÄÂçïÊñπÊ≥ïÊòØÈÄöËøáÊ∑ªÂä†ÂëΩ‰ª§„ÄÇ ‰∏Ä‰∏™ÂëΩ‰ª§Ê≥®ÂÜå‰∏Ä‰∏™ÂõûË∞ÉÂáΩÊï∞ÔºåËØ•ÂáΩÊï∞ÂèØ‰ª•‰ªéÂëΩ‰ª§Èù¢ÊùøÊàñÈîÆÁõòÁªëÂÆö‰∏≠Ë∞ÉÁî®„ÄÇ Êàë‰ª¨Â∑≤ÁªèÂÜô‰∫Ü‰∏Ä‰∏™ Yeoman generator Êù•Â∏ÆÂä©‰Ω†ÂºÄÂßã„ÄÇ ÂÆâË£ÖYeomanÂíåYeoman VS‰ª£Á†ÅÊâ©Â±ïÁîüÊàêÂô®Âπ∂Êê≠Âª∫‰∏Ä‰∏™Êñ∞ÁöÑÊâ©Â±ïÔºö 12npm install -g yo generator-codeyo code ÂØπ‰∫éhello worldÊâ©Â±ïÔºåÊÇ®ÂèØ‰ª•ÂàõÂª∫TypeScriptÊâ©Â±ïÊàñJavaScript„ÄÇ ÂØπ‰∫éËøô‰∏™‰æãÂ≠êÔºåÊàë‰ª¨ÈÄâÊã©‰∏Ä‰∏™TypeScriptÊâ©Â±ï„ÄÇ Running your extension ÂêØÂä®VS‰ª£Á†ÅÔºåÈÄâÊã©Êñá‰ª∂&gt;ÊâìÂºÄÊñá‰ª∂Â§πÔºåÁÑ∂ÂêéÈÄâÊã©ÊÇ®ÁîüÊàêÁöÑÊñá‰ª∂Â§π„ÄÇ ÊåâF5ÊàñÂçïÂáªË∞ÉËØïÂõæÊ†áÔºåÁÑ∂ÂêéÂçïÂáªÂºÄÂßã„ÄÇ VS CodeÁöÑÊñ∞ÂÆû‰æãÂ∞Ü‰ª•ÁâπÊÆäÊ®°ÂºèÔºàÊâ©Â±ïÂºÄÂèë‰∏ªÊú∫ÔºâÂêØÂä®ÔºåÂπ∂‰∏îÊ≠§Êñ∞ÂÆû‰æãÁé∞Âú®Áü•ÈÅìÊÇ®ÁöÑÊâ©Â±ï„ÄÇ Êåâ‰∏ãCtrl + Shift + PÂπ∂ËøêË°åÂêç‰∏∫Hello WorldÁöÑÂëΩ‰ª§„ÄÇ ÊÅ≠ÂñúÔºÅ ‰Ω†ÂàöÂàöÂàõÂª∫Âπ∂ÊâßË°å‰∫Ü‰Ω†ÁöÑÁ¨¨‰∏Ä‰∏™VS CodeÂëΩ‰ª§ÔºÅ Êâ©Â±ïÁöÑÁªìÊûÑAfter running, the generated extension should have the following structure: 123456789101112131415161718192021222324252627.‚îú‚îÄ‚îÄ .gitignore‚îú‚îÄ‚îÄ .vscode // VS Code integration‚îÇ ‚îú‚îÄ‚îÄ launch.json‚îÇ ‚îú‚îÄ‚îÄ settings.json‚îÇ ‚îî‚îÄ‚îÄ tasks.json‚îú‚îÄ‚îÄ .vscodeignore // files ignored when publishing extension‚îú‚îÄ‚îÄ README.md‚îú‚îÄ‚îÄ src‚îÇ ‚îî‚îÄ‚îÄ extension.ts // the source of the extension entry point‚îú‚îÄ‚îÄ test // test folder‚îÇ ‚îú‚îÄ‚îÄ extension.test.ts // extension.test.js, in case of JavaScript extension‚îÇ ‚îî‚îÄ‚îÄ index.ts // index.js, in case of JavaScript extension‚îú‚îÄ‚îÄ node_modules‚îÇ ‚îú‚îÄ‚îÄ vscode // include vscode type definition file for extension development‚îÇ ‚îî‚îÄ‚îÄ typescript // compiler for typescript (TypeScript only)‚îú‚îÄ‚îÄ out // compilation output (TypeScript only)‚îÇ ‚îú‚îÄ‚îÄ extension.js // the extension entry point‚îÇ ‚îú‚îÄ‚îÄ extension.js.map‚îÇ ‚îî‚îÄ‚îÄ test‚îÇ ‚îú‚îÄ‚îÄ extension.test.js‚îÇ ‚îú‚îÄ‚îÄ extension.test.js.map‚îÇ ‚îú‚îÄ‚îÄ index.js‚îÇ ‚îî‚îÄ‚îÄ index.js.map‚îú‚îÄ‚îÄ package.json // extension&apos;s manifest‚îú‚îÄ‚îÄ tsconfig.json // jsconfig.json, in case of JavaScript extension‚îî‚îÄ‚îÄ vsc-extension-quickstart.md // extension development quick start ËÆ©Êàë‰ª¨ÈÄöËøáÊâÄÊúâËøô‰∫õÊñá‰ª∂ÁöÑÁõÆÁöÑÔºåÂπ∂Ëß£Èáä‰ªñ‰ª¨ÂÅö‰∫Ü‰ªÄ‰πàÔºö Êâ©Â±ïÂêçÊ∏ÖÂçïÔºöpackage.json ÊØè‰∏™VS‰ª£Á†ÅÊâ©Â±ïÂøÖÈ°ªÊúâ‰∏Ä‰∏™ÊèèËø∞ÂÆÉÂèäÂÖ∂ÂäüËÉΩÁöÑpackage.jsonÊñá‰ª∂„ÄÇ VS‰ª£Á†ÅÂú®ÂêØÂä®ËøáÁ®ã‰∏≠ËØªÂèñÊ≠§Êñá‰ª∂ÔºåÂπ∂Á´ãÂç≥ÂìçÂ∫îÊØè‰∏™Êñá‰ª∂„ÄÇ ËØ∑ÈòÖËØªpackage.jsonÊâ©Â±ïÊ∏ÖÂçïÂèÇËÄÉ„ÄÇ ÊúâÂÖ≥package.jsonË¥°ÁåÆÁÇπÁöÑÊõ¥Â§ö‰ø°ÊÅØ„ÄÇ Á§∫‰æãTYPESCRIPT EXTENSION MANIFEST1234567891011121314151617181920212223242526272829303132333435&#123; &quot;name&quot;: &quot;myFirstExtension&quot;, &quot;description&quot;: &quot;&quot;, &quot;version&quot;: &quot;0.0.1&quot;, &quot;publisher&quot;: &quot;&quot;, &quot;engines&quot;: &#123; &quot;vscode&quot;: &quot;^1.5.0&quot; &#125;, &quot;categories&quot;: [ &quot;Other&quot; ], &quot;activationEvents&quot;: [ &quot;onCommand:extension.sayHello&quot; ], &quot;main&quot;: &quot;./out/extension&quot;, &quot;contributes&quot;: &#123; &quot;commands&quot;: [&#123; &quot;command&quot;: &quot;extension.sayHello&quot;, &quot;title&quot;: &quot;Hello World&quot; &#125;] &#125;, &quot;scripts&quot;: &#123; &quot;vscode:prepublish&quot;: &quot;tsc -p ./&quot;, &quot;compile&quot;: &quot;tsc -watch -p ./&quot;, &quot;postinstall&quot;: &quot;node ./node_modules/vscode/bin/install&quot;, &quot;test&quot;: &quot;node ./node_modules/vscode/bin/test&quot; &#125;, &quot;devDependencies&quot;: &#123; &quot;typescript&quot;: &quot;^2.0.3&quot;, &quot;vscode&quot;: &quot;^1.5.0&quot;, &quot;mocha&quot;: &quot;^2.3.3&quot;, &quot;@types/node&quot;: &quot;^6.0.40&quot;, &quot;@types/mocha&quot;: &quot;^2.2.32&quot; &#125;&#125; Note: JavaScriptÊâ©Â±ï‰∏çÈúÄË¶ÅËÑöÊú¨Â≠óÊÆµÔºåÂõ†‰∏∫‰∏çÈúÄË¶ÅÁºñËØë„ÄÇ Ëøô‰∏™ÁâπÂÆöÁöÑpackage.jsonÊèèËø∞‰∫Ü‰∏Ä‰∏™Êâ©Â±ïÔºö ‰ΩøÁî®Ê†áÁ≠æ‚ÄúHello world‚Äù‰∏∫Command PaletteÔºàCtrl + Shift + PÔºâ‰ΩúÂá∫Ë¥°ÁåÆÔºåËØ•Ê†áÁ≠æÂ∞ÜË∞ÉÁî®ÂëΩ‰ª§‚Äúextension.sayHello‚Äù„ÄÇ ËØ∑Ê±ÇÂú®Ë∞ÉÁî®‚Äúextension.sayHello‚ÄùÂëΩ‰ª§Êó∂Âä†ËΩΩÔºàactivationEventsÔºâ„ÄÇ Â∞ÜÂÖ∂‰∏ªË¶ÅJavaScript‰ª£Á†ÅÊîæÂú®Âêç‰∏∫‚Äú./out/extension.js‚ÄùÁöÑÊñá‰ª∂‰∏≠„ÄÇ Note: VS‰ª£Á†Å‰∏ç‰ºöÂú®ÂêØÂä®Êó∂ÊÄ•ÂàáÂú∞Âä†ËΩΩÊâ©Â±ïÁöÑ‰ª£Á†Å„ÄÇ Êâ©Â±ïÂøÖÈ°ªÈÄöËøáactivationEventsÂ±ûÊÄßÂú®‰ªÄ‰πàÊù°‰ª∂‰∏ãÊøÄÊ¥ªÔºàÂä†ËΩΩÔºâÊù•ÊèèËø∞„ÄÇ ÁîüÊàêÁöÑ‰ª£Á†ÅÁîüÊàêÁöÑÊâ©Â±ïÁöÑ‰ª£Á†Å‰Ωç‰∫éextension.tsÔºàÊàñÊâ©Â±ïÂêç‰∏∫JavaScriptÁöÑÊâ©Â±ïÂêçÁöÑÊÉÖÂÜµ‰∏ãÔºâÔºö 123456789101112131415161718192021222324// The module 'vscode' contains the VS Code extensibility API// Import the module and reference it with the alias vscode in your code belowimport * as vscode from 'vscode';// this method is called when your extension is activated// your extension is activated the very first time the command is executedexport function activate(context: vscode.ExtensionContext) &#123; // Use the console to output diagnostic information (console.log) and errors (console.error) // This line of code will only be executed once when your extension is activated console.log('Congratulations, your extension "my-first-extension" is now active!'); // The command has been defined in the package.json file // Now provide the implementation of the command with registerCommand // The commandId parameter must match the command field in package.json let disposable = vscode.commands.registerCommand('extension.sayHello', () =&gt; &#123; // The code you place here will be executed every time your command is executed // Display a message box to the user vscode.window.showInformationMessage('Hello World!'); &#125;); context.subscriptions.push(disposable);&#125; ÊØè‰∏™Êâ©Â±ïÂ∫î‰ªéÂÖ∂‰∏ªÊñá‰ª∂‰∏≠ÂØºÂá∫‰∏Ä‰∏™Âêç‰∏∫activateÔºàÔºâÁöÑÂáΩÊï∞ÔºåÂΩìÂèëÁîüpackage.jsonÊñá‰ª∂‰∏≠ÊèèËø∞ÁöÑ‰ªª‰ΩïÊøÄÊ¥ª‰∫ã‰ª∂Êó∂ÔºåËØ•VS‰ª£Á†ÅÂ∞ÜÂè™Ë∞ÉÁî®‰∏ÄÊ¨°„ÄÇ Â¶ÇÊûúÊâ©Â±ï‰ΩøÁî®OSËµÑÊ∫êÔºà‰æãÂ¶ÇË°çÁîüËøõÁ®ãÔºâÔºåÂàôÊâ©Â±ïÂèØ‰ª•‰ªéÂÖ∂‰∏ªÊñá‰ª∂‰∏≠ÂØºÂá∫‰∏Ä‰∏™Âêç‰∏∫deactivateÔºàÔºâÁöÑÂáΩÊï∞ÔºåÂú®ËØ•ÂáΩÊï∞‰∏≠ÂÆÉÂèØ‰ª•ÊâßË°åÊ∏ÖÁêÜÂ∑•‰ΩúÔºåVS‰ª£Á†ÅÂ∞ÜÂú®ÂÖ≥Èó≠Êó∂Ë∞ÉÁî®ËØ•ÂáΩÊï∞„ÄÇ Ëøô‰∏™ÁâπÂÆöÁöÑÊâ©Â±ïÂêçÂØºÂÖ•‰∫Üvscode APIÔºåÁÑ∂ÂêéÊ≥®ÂÜå‰∏Ä‰∏™ÂëΩ‰ª§ÔºåÂ∞ÜÂëΩ‰ª§‚Äúextension.sayHello‚ÄùË∞ÉÁî®Êó∂ÂÖ≥ËÅî‰∏Ä‰∏™ÂáΩÊï∞„ÄÇ ËØ•ÂëΩ‰ª§ÁöÑÂÆûÁé∞Âú®VS Code‰∏≠ÊòæÁ§∫‚ÄúHello world‚ÄùÊ∂àÊÅØ„ÄÇ Ê≥®Ôºöpackage.jsonÁöÑcontributionÈÉ®ÂàÜÂêëCommand PaletteÊ∑ªÂä†‰∏Ä‰∏™Êù°ÁõÆ„ÄÇ extension.ts / .js‰∏≠ÁöÑ‰ª£Á†ÅÂÆö‰πâ‰∫Ü‚Äúextension.sayHello‚ÄùÁöÑÂÆûÁé∞„ÄÇ Âè¶ Ê≥®ÔºöÂØπ‰∫éTypeScriptÊâ©Â±ïÔºåÁîüÊàêÁöÑÊñá‰ª∂out / extension.jsÂ∞ÜÂú®ËøêË°åÊó∂Âä†ËΩΩÂπ∂Áî±VS‰ª£Á†ÅÊâßË°å„ÄÇ ÊùÇÈ°πÊñá‰ª∂(Miscellaneous files) .vscode / launch.jsonÂÆö‰πâÂú®Êâ©Â±ïÂºÄÂèëÊ®°Âºè‰∏ãÂêØÂä®VS‰ª£Á†Å„ÄÇ ÂÆÉËøò‰ΩøÁî®preLaunchTaskÊåáÂêëËøêË°åTypeScriptÁºñËØëÂô®ÁöÑ.vscode / tasks.json‰∏≠ÂÆö‰πâÁöÑ‰ªªÂä°„ÄÇ .vscode / settings.jsonÈªòËÆ§ÊéíÈô§outÊñá‰ª∂Â§π„ÄÇ ÊÇ®ÂèØ‰ª•‰øÆÊîπË¶ÅÈöêËóèÁöÑÊñá‰ª∂Á±ªÂûã„ÄÇ .gitignore - ÂëäËØâGitÁâàÊú¨ÊéßÂà∂Âì™‰∫õÊ®°ÂºèË¶ÅÂøΩÁï•„ÄÇ .vscodeignore - ÂëäÁü•ÊâìÂåÖÂ∑•ÂÖ∑Âú®ÂèëÂ∏ÉÊâ©Â±ïÊó∂ÂøΩÁï•Âì™‰∫õÊñá‰ª∂„ÄÇ README.md - ÊèèËø∞VS CodeÁî®Êà∑Êâ©Â±ïÁöÑËá™Ëø∞Êñá‰ª∂„ÄÇ vsc-extension-quickstart.md - Âø´ÈÄüÂÖ•Èó®ÊåáÂçó„ÄÇ test / extension.test.ts - ÊÇ®ÂèØ‰ª•Â∞ÜÊÇ®ÁöÑÊâ©Â±ïÂçïÂÖÉÊµãËØïÊîæÂú®ËøôÈáåÔºåÂπ∂ÈíàÂØπVS Code APIËøêË°åÊµãËØïÔºàËØ∑ÂèÇÈòÖÊµãËØïÊÇ®ÁöÑÊâ©Â±ïÔºâ Êâ©Â±ïÊøÄÊ¥ª Áé∞Âú®Êâ©Â±ï‰∏≠ÂåÖÂê´ÁöÑÊñá‰ª∂ÁöÑËßíËâ≤Â∑≤ÁªèÊòéÁ°ÆÔºå‰∏ãÈù¢ÊòØÊÇ®ÁöÑÊâ©Â±ïÂ¶Ç‰ΩïÊøÄÊ¥ªÔºö Êâ©Â±ïÂºÄÂèëÂÆû‰æãÂèëÁé∞Êâ©Â±ïÂπ∂ËØªÂèñÂÆÉÁöÑpackage.jsonÊñá‰ª∂„ÄÇ Á®çÂêéÂΩìÊÇ®ÊåâCtrl + Shift + PÊó∂Ôºö Ê≥®ÂÜåÁöÑÂëΩ‰ª§ÊòæÁ§∫Âú®ÂëΩ‰ª§Èù¢Êùø‰∏≠„ÄÇ Âú®Ëøô‰∏™ÂàóË°®‰∏≠ÔºåÁé∞Âú®Êúâ‰∏Ä‰∏™Âú®package.json‰∏≠ÂÆö‰πâÁöÑÊù°ÁõÆ‚ÄúHello world‚Äù„ÄÇ ÈÄâÊã©‚ÄúHello world‚ÄùÂëΩ‰ª§Êó∂Ôºö ÂëΩ‰ª§‚Äúextension.sayHello‚ÄùË¢´Ë∞ÉÁî®Ôºö ÊøÄÊ¥ª‰∫ã‰ª∂‚ÄúonCommandÔºöextension.sayHello‚ÄùË¢´ÂàõÂª∫„ÄÇ ÊâÄÊúâÂú®ÊøÄÊ¥ª‰∫ã‰ª∂‰∏≠ÂàóÂá∫ÊøÄÊ¥ª‰∫ã‰ª∂ÁöÑÊâ©Â±ïÈÉΩ‰ºöË¢´ÊøÄÊ¥ª„ÄÇ ./out/extension.js‰∏≠ÁöÑÊñá‰ª∂Ë¢´Âä†ËΩΩÂà∞JavaScript VM‰∏≠„ÄÇ VS‰ª£Á†ÅÊü•ÊâæÂØºÂá∫ÁöÑÂáΩÊï∞ÊøÄÊ¥ªÂπ∂Ë∞ÉÁî®ÂÆÉ„ÄÇ ÂëΩ‰ª§‚Äúextension.sayHello‚ÄùÂ∑≤Ê≥®ÂÜåÔºåÁé∞Âú®ÂÆö‰πâ‰∫ÜÂÖ∂ÂÆûÁé∞„ÄÇ Ë∞ÉÁî®ÂëΩ‰ª§‚Äúextension.sayHello‚ÄùÂÆûÁé∞ÂáΩÊï∞„ÄÇ ÂëΩ‰ª§ÂÆûÁé∞ÊòæÁ§∫‚ÄúHello World‚ÄùÊ∂àÊÅØ„ÄÇ Ë∞ÉËØï‰Ω†ÁöÑÊãìÂ±ï‰æãÂ¶ÇÔºåÂú®Ê≥®ÂÜåÁöÑÂëΩ‰ª§‰∏≠ËÆæÁΩÆÊñ≠ÁÇπÔºåÁÑ∂ÂêéÂú®Extension Development VS‰ª£Á†ÅÂÆû‰æã‰∏≠ËøêË°å‚ÄúHello world‚ÄùÂëΩ‰ª§„ÄÇ Ê≥®ÔºöÂØπ‰∫éTypeScriptÊâ©Â±ïÔºåÂç≥‰ΩøVS‰ª£Á†ÅÂä†ËΩΩÂπ∂ÊâßË°åout / extension.jsÔºåÁî±‰∫éÁîüÊàêÁöÑÊ∫êÊò†Â∞Ñout / extension.js.mapÂíåVS CodeÁöÑÊ∫ê‰ª£Á†ÅÊò†Â∞ÑÂô®Ë∞ÉËØïÂô®ÊîØÊåÅÔºåÊÇ®ÂÆûÈôÖ‰∏äÂèØ‰ª•Ë∞ÉËØïÂéüÂßãÁöÑTypeScript‰ª£Á†Å„ÄÇÊèêÁ§∫ÔºöË∞ÉËØïÊéßÂà∂Âè∞Â∞ÜÊòæÁ§∫ÊÇ®ÁôªÂΩïÂà∞ÊéßÂà∂Âè∞ÁöÑÊâÄÊúâÊ∂àÊÅØ„ÄÇ To learn more about the extension development environment. ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊîπÂèòÂú®extension.tsÔºàÊàñextension.jsÔºåÂú®JavaScriptÊâ©Â±ï‰∏≠ÔºâÔºåËØ∑Â∞ùËØïÊõøÊç¢extension.sayHelloÂëΩ‰ª§ÂÆûÁé∞‰ª•ÊòæÁ§∫Âú®ÁºñËæëÂô®‰∏≠ÈÄâÊã©ÁöÑÂ≠óÁ¨¶Êï∞Ôºö1234567891011121314let disposable = vscode.commands.registerCommand('extension.sayHello', () =&gt; &#123; // The code you place here will be executed every time your command is executed let editor = vscode.window.activeTextEditor; if (!editor) &#123; return; // No open text editor &#125; let selection = editor.selection; let text = editor.document.getText(selection); // Display a message box to the user vscode.window.showInformationMessage('Selected characters: ' + text.length);&#125;); ÊèêÁ§∫Ôºö‰∏ÄÊó¶ÊÇ®ÂØπÊâ©Â±ïÊ∫ê‰ª£Á†ÅËøõË°åÊõ¥ÊîπÔºåÊÇ®ÈúÄË¶ÅÈáçÊñ∞ÂêØÂä®VS CodeÁöÑExtension Development HostÂÆû‰æã„ÄÇ ÊÇ®ÂèØ‰ª•ÈÄöËøáÂú®Extension Development HostÂÆû‰æã‰∏≠‰ΩøÁî®Ctrl + RÔºàmacOSÔºöCmd + RÔºâÊàñÈÄöËøáÂçïÂáª‰∏ªVS‰ª£Á†ÅÂÆû‰æãÈ°∂ÈÉ®ÁöÑÈáçÂêØÊåâÈíÆÊù•ÂÆûÁé∞„ÄÇ ÂàõÂª∫‰∏Ä‰∏™Êñá‰ª∂ÔºàÊñá‰ª∂&gt;Êñ∞Âª∫Êñá‰ª∂ÔºâÔºåËæìÂÖ•‰∏Ä‰∫õÊñáÊú¨Âπ∂ÈÄâÊã©ÂÆÉ„ÄÇ ÂΩìÊÇ®ËøêË°åHello WorldÂëΩ‰ª§Êó∂ÔºåÊÇ®Áé∞Âú®Â∫îËØ•ÁúãÂà∞ÊâÄÈÄâÂ≠óÁ¨¶ÁöÑËÆ°Êï∞„ÄÇ Âú®Êú¨Âú∞ÂÆâË£ÖÊÇ®ÁöÑÊâ©Â±ïÂà∞ÁõÆÂâç‰∏∫Ê≠¢ÔºåÊÇ®ÊâÄÁºñÂÜôÁöÑÊâ©Â±ïÂè™Âú®VS CodeÁöÑ‰∏Ä‰∏™ÁâπÊÆäÂÆû‰æã‰∏≠ËøêË°åÔºåÂç≥Êâ©Â±ïÂºÄÂèëÂÆû‰æã„ÄÇ Ë¶ÅËÆ©ÊÇ®ÁöÑÊâ©Â±ïÁ®ãÂ∫èÂú®VS‰ª£Á†ÅÁöÑÊâÄÊúâÂÆû‰æã‰∏≠ËøêË°åÔºåÊÇ®ÈúÄË¶ÅÂ∞ÜÂÖ∂Â§çÂà∂Âà∞Êú¨Âú∞Êâ©Â±ïÊñá‰ª∂Â§π‰∏ãÁöÑÊñ∞Êñá‰ª∂Â§π‰∏≠Ôºö Windows: %USERPROFILE%.vscode\extensions macOS/Linux: $HOME/.vscode/extensions ÂèëÂ∏ÉÊÇ®ÁöÑÊâ©Â±ïÈòÖËØªÊúâÂÖ≥Â¶Ç‰ΩïÂÖ±‰∫´Êâ©Â±ïÁöÑ‰ø°ÊÅØ„ÄÇ Next StepsÂú®Êú¨ÊºîÁªÉ‰∏≠ÔºåÊàë‰ª¨ÁúãÂà∞‰∫Ü‰∏Ä‰∏™ÈùûÂ∏∏ÁÆÄÂçïÁöÑÊâ©Â±ï„ÄÇ ÊúâÂÖ≥Êõ¥ËØ¶ÁªÜÁöÑÁ§∫‰æãÔºåËØ∑ÂèÇÈòÖÂ≠óÊï∞ÁªüËÆ°Á§∫‰æãÔºåÂÖ∂‰∏≠ÊòæÁ§∫‰∫ÜÂ¶Ç‰ΩïÂÆö‰ΩçÁâπÂÆöËØ≠Ë®ÄÔºàMarkdownÔºâÂπ∂Êî∂Âê¨ÁºñËæëÂô®ÁöÑÊñáÊ°£Êõ¥Êîπ‰∫ã‰ª∂„ÄÇ Â¶ÇÊûúÊÇ®ÊÉ≥Êõ¥ËØ¶ÁªÜÂú∞ÈòÖËØªÊâ©Â±ïAPIÔºåËØ∑Â∞ùËØï‰ª•‰∏ã‰∏ªÈ¢òÔºö Êâ©Â±ïAPIÊ¶ÇËø∞ - ‰∫ÜËß£ÂÆåÊï¥ÁöÑVS‰ª£Á†ÅÊâ©Â±ïÊ®°Âûã„ÄÇ APIÂéüÂàôÂíåÊ®°Âºè - VS‰ª£Á†ÅÂèØÊâ©Â±ïÊÄßÂü∫‰∫éÂá†‰∏™ÊåáÂØºÂéüÂàôÂíåÊ®°Âºè„ÄÇ Ë¥°ÁåÆÁÇπ - ÂÖ≥‰∫éÂêÑÁßçVS CodeË¥°ÁåÆÁÇπÁöÑÁªÜËäÇ„ÄÇ ÊøÄÊ¥ª‰∫ã‰ª∂ - VS‰ª£Á†ÅÊøÄÊ¥ª‰∫ã‰ª∂ÂèÇËÄÉ ÂÖ∂‰ªñÊâ©Â±ïÁ§∫‰æã - Êü•ÁúãÊàë‰ª¨ÁöÑÁ§∫‰æãÊâ©Â±ïÈ°πÁõÆÂàóË°®„ÄÇ Example - Word CountÂéüÊñáÈìæÊé•https://code.visualstudio.com/docs/extensions/example-word-count OverviewRun the ExtensionUpdate the Status BarSubscribing to EventsCustomizing the Status BarDisposing Extension ResourcesInstalling your Extension LocallyPublishing your ExtensionNext StepsExample - Language ServerÂéüÊñáÈìæÊé• https://code.visualstudio.com/docs/extensions/example-language-server Implement your own Language ServerExplaining the ‚ÄòClient‚ÄôExplaining the ‚ÄòServer‚ÄôAdding a Simple ValidationDebugging both Client and ServerUsing Configuration Settings in the ServerAdding additional Language FeaturesAdditional Language Server featuresIncremental Text Document SynchronizationNext StepsExample - Debug AdapterÂéüÊñáÈìæÊé• The Mock Debug ExtensionDevelopment Setup for Mock DebugAnatomy of the package.json of a Debug ExtensionUsing a DebugConfigurationProviderPublishing your Debug AdapterAlternative approach to develop a Debug Extension]]></content>
      <categories>
        <category>VSCodeÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÁøªËØë</tag>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Text Buffer Reimplementation]]></title>
    <url>%2F2018%2F05%2F05%2FText-Buffer-Reimplementation%2F</url>
    <content type="text"><![CDATA[Visual Studio Code 1.21ÁâàÊú¨ÂåÖÂê´‰∏Ä‰∏™ÂÖ®Êñ∞ÁöÑÊñáÊú¨ÁºìÂÜ≤Âå∫ÂÆûÁé∞ÔºåÂÆÉÂú®ÈÄüÂ∫¶ÂíåÂÜÖÂ≠ò‰ΩøÁî®ÊñπÈù¢ÊÄßËÉΩÊõ¥È´ò„ÄÇÂú®ËøôÁØáÂçöÊñá‰∏≠ÔºåÊàëÊÉ≥ËÆ≤Ëø∞‰∏Ä‰∏ãÊàë‰ª¨Â¶Ç‰ΩïÈÄâÊã©ÂíåËÆæËÆ°ÂØºËá¥Ëøô‰∫õÊîπËøõÁöÑÊï∞ÊçÆÁªìÊûÑÂíåÁÆóÊ≥ïÁöÑÊïÖ‰∫ã„ÄÇÂÖ≥‰∫éJavaScriptÁ®ãÂ∫èÁöÑÊÄßËÉΩËÆ®ËÆ∫ÈÄöÂ∏∏Ê∂âÂèäÂÖ≥‰∫éÂú®Êú¨Êú∫‰ª£Á†Å‰∏≠Â∫îËØ•ÂÆûÁé∞Â§öÂ∞ëÁöÑËÆ®ËÆ∫„ÄÇÂØπ‰∫éVS‰ª£Á†ÅÊñáÊú¨ÁºìÂÜ≤Âå∫ÔºåËøô‰∫õËÆ®ËÆ∫ÊòØÂú®‰∏ÄÂπ¥Â§öÂâçÂºÄÂßãÁöÑ„ÄÇÂú®Ê∑±ÂÖ•Êé¢Á¥¢ËøáÁ®ã‰∏≠ÔºåÊàë‰ª¨ÂèëÁé∞ÊñáÊú¨ÁºìÂÜ≤Âå∫ÁöÑC ++ÂÆûÁé∞ÂèØ‰ª•ËäÇÁúÅÂ§ßÈáèÁöÑÂÜÖÂ≠òÔºå‰ΩÜÊòØÊàë‰ª¨Ê≤°ÊúâÁúãÂà∞Êàë‰ª¨ÊúüÊúõÁöÑÊÄßËÉΩÂ¢ûÂº∫„ÄÇÂú®Ëá™ÂÆö‰πâÊú¨Âú∞Ë°®Á§∫ÂíåV8ÁöÑÂ≠óÁ¨¶‰∏≤‰πãÈó¥ËΩ¨Êç¢Â≠óÁ¨¶‰∏≤‰ª£‰ª∑ÂæàÈ´òÔºåÂú®Êàë‰ª¨ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ºöÈôç‰ΩéÂú®C ++‰∏≠ÂÆûÁé∞ÊñáÊú¨ÁºìÂÜ≤Êìç‰ΩúÊâÄÂ∏¶Êù•ÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨Â∞ÜÂú®ËøôÁØáÊñáÁ´†ÁªìÂ∞æËØ¶ÁªÜËÆ®ËÆ∫Ëøô‰∏ÄÁÇπ„ÄÇ ‰∏çÊòØÂéüÁîüÁöÑÔºåÊàë‰ª¨ÂøÖÈ°ªÊâæÂà∞ÊîπËøõJavaScript / TypeScript‰ª£Á†ÅÁöÑÊñπÊ≥ï„ÄÇÂÉèVyacheslav EgorovËøôÊ†∑ÁöÑÊøÄÂä®‰∫∫ÂøÉÁöÑÂçöÂÆ¢ÊñáÁ´†Â±ïÁ§∫‰∫ÜÂ∞ÜJavaScriptÂºïÊìéÊé®ÂêëÊûÅÈôêÂπ∂Â∞ΩÂèØËÉΩÂ§öÂú∞Ë°®Áé∞Âá∫ÊÄßËÉΩÁöÑÊñπÊ≥ï„ÄÇÂç≥‰ΩøÊ≤°Êúâ‰ΩéÁ∫ßÂà´ÁöÑÂºïÊìéÊäÄÂ∑ßÔºåÈÄöËøá‰ΩøÁî®Êõ¥ÂêàÈÄÇÁöÑÊï∞ÊçÆÁªìÊûÑÂíåÊõ¥Âø´ÁöÑÁÆóÊ≥ïÔºå‰ªçÁÑ∂ÂèØ‰ª•Â∞ÜÈÄüÂ∫¶ÊèêÈ´ò‰∏Ä‰∏™ÊàñÂ§ö‰∏™Êï∞ÈáèÁ∫ß„ÄÇ Previous text buffer data structureÁºñËæëÁöÑÂøÉÁêÜÊ®°Âûã(mental model)ÊòØÂü∫‰∫éË°åÁöÑ„ÄÇÂºÄÂèë‰∫∫ÂëòÈÄêË°åËØªÂÜôÊ∫ê‰ª£Á†ÅÔºåÁºñËØëÂô®Êèê‰æõÂü∫‰∫éË°å/ÂàóÁöÑËØäÊñ≠ÔºåÂ†ÜÊ†àË∑üË∏™ÂåÖÂê´Ë°åÂè∑ÔºåÊ†áËÆ∞ÂåñÂºïÊìéÈÄêË°åËøêË°åÁ≠â„ÄÇËôΩÁÑ∂ÁÆÄÂçïÔºå‰ΩÜÊîØÊåÅVS‰ª£Á†ÅÁöÑÊñáÊú¨ÁºìÂÜ≤Âå∫ÂÆûÁé∞Âπ∂Ê≤°ÊúâÂ§™Â§ßÊîπÂèòÔºåÂõ†‰∏∫Á¨¨‰∏ÄÂ§©Êàë‰ª¨ÂºÄÂßã‰∫ÜÊë©Á∫≥Âì•È°πÁõÆ„ÄÇÊàë‰ª¨‰ΩøÁî®‰∫Ü‰∏ÄÁ≥ªÂàóÁ∫øÊù°ÔºåÂπ∂‰∏îÂÆÉÂ∑•‰ΩúÂæóÂæàÂ•ΩÔºåÂõ†‰∏∫ÂÖ∏ÂûãÁöÑÊñáÊú¨ÊñáÊ°£Áõ∏ÂØπËæÉÂ∞è„ÄÇÂΩìÁî®Êà∑ËæìÂÖ•Êó∂ÔºåÊàë‰ª¨ÊâæÂà∞Ë¶ÅÂú®Êï∞ÁªÑ‰∏≠‰øÆÊîπÁöÑË°åÂπ∂Â∞ÜÂÖ∂ÊõøÊç¢„ÄÇÊèíÂÖ•Êñ∞Ë°åÊó∂ÔºåÊàë‰ª¨Â∞ÜÊñ∞ÁöÑË°åÂØπË±°ÊãºÊé•Âà∞Ë°åÊï∞ÁªÑ‰∏≠ÔºåJavaScriptÂºïÊìéÂ∞Ü‰∏∫Êàë‰ª¨ÂÆåÊàêÁπÅÈáçÁöÑÂ∑•‰Ωú„ÄÇ ‰ΩÜÊòØÔºåÊàë‰ª¨‰∏çÊñ≠Êî∂Âà∞ÊúâÂÖ≥ÊâìÂºÄÊüê‰∫õÊñá‰ª∂‰ºöÂØºËá¥VS‰ª£Á†Å‰∏≠Âá∫Áé∞ÂÜÖÂ≠ò‰∏çË∂≥ÁöÑÊä•Âëä„ÄÇ‰æãÂ¶ÇÔºå‰∏Ä‰∏™Áî®Êà∑Êó†Ê≥ïÊâìÂºÄ‰∏Ä‰∏™35 MBÁöÑÊñá‰ª∂„ÄÇÊ†πÊú¨ÂéüÂõ†ÊòØËØ•Êñá‰ª∂ÁöÑË°åÊï∞Â§™Â§öÔºå‰∏∫1,370‰∏á„ÄÇÊàë‰ª¨Â∞Ü‰∏∫ÊØèË°åÂíåÊØè‰∏™‰ΩøÁî®Â§ßÁ∫¶40-60‰∏™Â≠óËäÇÁöÑÂØπË±°ÂàõÂª∫‰∏Ä‰∏™ModelLineÂØπË±°ÔºåÂõ†Ê≠§Ë°åÊï∞ÁªÑ‰ΩøÁî®Â§ßÁ∫¶600MBÂÜÖÂ≠òÊù•Â≠òÂÇ®ÊñáÊ°£„ÄÇËøôÂ§ßÁ∫¶ÊòØÊúÄÂàùÊñá‰ª∂Â§ßÂ∞èÁöÑ20ÂÄçÔºÅ Á∫øÈòµÂàóË°®Á§∫ÁöÑÂè¶‰∏Ä‰∏™ÈóÆÈ¢òÊòØÊâìÂºÄÊñá‰ª∂ÁöÑÈÄüÂ∫¶„ÄÇ‰∏∫‰∫ÜÊûÑÂª∫Á∫øÊù°Êï∞ÁªÑÔºåÊàë‰ª¨ÂøÖÈ°ªÈÄöËøáÊç¢Ë°åÁ¨¶ÂàÜÂâ≤ÂÜÖÂÆπÔºå‰ª•‰æøÊØèË°åËé∑Âæó‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÂØπË±°„ÄÇÊãÜÂàÜÊú¨Ë∫´‰ºö‰º§ÂÆ≥‰Ω†Âú®Âü∫ÂáÜÊµãËØï‰∏≠‰ºöÁúãÂà∞ÁöÑÊÄßËÉΩ„ÄÇ Finding a new text buffer implementationÊóßÁöÑÁ∫øÈòµÂàóË°®Á§∫ÂèØËÉΩÈúÄË¶ÅÂæàÈïøÊó∂Èó¥ÊâçËÉΩÂàõÂª∫Âπ∂Ê∂àËÄóÂ§ßÈáèÂÜÖÂ≠òÔºå‰ΩÜÂÆÉÂèØ‰ª•Âø´ÈÄüÊü•ÊâæÁ∫øË∑Ø„ÄÇ Âú®ÂÆåÁæéÁöÑ‰∏ñÁïå‰∏≠ÔºåÊàë‰ª¨Âè™‰ºöÂ≠òÂÇ®Êñá‰ª∂ÁöÑÊñáÊú¨Âπ∂‰∏î‰∏ç‰ºöÂ≠òÂÇ®È¢ùÂ§ñÁöÑÂÖÉÊï∞ÊçÆ„ÄÇ Âõ†Ê≠§ÔºåÊàë‰ª¨ÂºÄÂßãÂØªÊâæÈúÄË¶ÅËæÉÂ∞ëÂÖÉÊï∞ÊçÆÁöÑÊï∞ÊçÆÁªìÊûÑ„ÄÇ Âú®Êü•Áúã‰∫ÜÂêÑÁßçÊï∞ÊçÆÁªìÊûÑ‰πãÂêéÔºåÊàëÂèëÁé∞ËØ•ÁâáÊñ≠Ë°®ÂèØËÉΩÊòØ‰∏Ä‰∏™ÂæàÂ•ΩÁöÑÂÄôÈÄâËÄÖ„ÄÇ Avoiding too much meta-data by using a piece tablePiece tableÊòØ‰∏ÄÁßçÊï∞ÊçÆÁªìÊûÑÔºåÁî®‰∫éË°®Á§∫ÊñáÊú¨ÊñáÊ°£ÔºàTypeScript‰∏≠ÁöÑÊ∫ê‰ª£Á†ÅÔºâ‰∏äÁöÑ‰∏ÄÁ≥ªÂàóÁºñËæëÔºö 12345678910111213141516class PieceTable &#123; original: string; // original contents added: string; // user added contents nodes: Node[];&#125;class Node &#123; type: NodeType; start: number; length: number;&#125;enum NodeType &#123; Original, Added&#125; Êñá‰ª∂ÊúÄÂàùÂä†ËΩΩÂêéÔºåËØ•ÁâáÊÆµË°®Âú®ÂéüÂßãÂ≠óÊÆµ‰∏≠ÂåÖÂê´Êï¥‰∏™Êñá‰ª∂ÂÜÖÂÆπ„ÄÇ Ê∑ªÂä†ÁöÑÂ≠óÊÆµ‰∏∫Á©∫„ÄÇ Êúâ‰∏Ä‰∏™NodeType.OriginalÁ±ªÂûãÁöÑÂçï‰∏™ËäÇÁÇπ„ÄÇ ÂΩìÁî®Êà∑Âú®Êñá‰ª∂Êú´Â∞æÈîÆÂÖ•ÂÜÖÂÆπÊó∂ÔºåÊàë‰ª¨Â∞ÜÊñ∞ÂÜÖÂÆπÊ∑ªÂä†Âà∞Ê∑ªÂä†ÁöÑÂ≠óÊÆµ‰∏≠ÔºåÂπ∂‰∏îÊàë‰ª¨Â∞ÜÂú®ËäÇÁÇπÂàóË°®ÁöÑÊú´Â∞æÊèíÂÖ•‰∏Ä‰∏™Á±ªÂûã‰∏∫NodeType.AddedÁöÑÊñ∞ËäÇÁÇπ„ÄÇ ÂêåÊ†∑ÔºåÂΩìÁî®Êà∑Âú®ËäÇÁÇπ‰∏≠Èó¥ËøõË°åÁºñËæëÊó∂ÔºåÊàë‰ª¨‰ºöÊ†πÊçÆÈúÄË¶ÅÊãÜÂàÜËØ•ËäÇÁÇπÂπ∂ÊèíÂÖ•Êñ∞ËäÇÁÇπ„ÄÇ ‰∏ãÈù¢ÁöÑÂä®ÁîªÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂú®‰∏Ä‰∏™Ë°®Ê†ºÁªìÊûÑ‰∏≠ÈÄêË°åËÆøÈóÆÊñáÊ°£„ÄÇ ÂÆÉÊúâ‰∏§‰∏™ÁºìÂÜ≤Âå∫ÔºàÂéüÂßãÂíåÊ∑ªÂä†ÔºâÂíå‰∏â‰∏™ËäÇÁÇπÔºàËøôÊòØÁî±ÂéüÂßãÂÜÖÂÆπ‰∏≠Èó¥ÊèíÂÖ•ÂºïËµ∑ÁöÑÔºâ„ÄÇ]]></content>
      <tags>
        <tag>ÁøªËØë</tag>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bing-powered settings search in VS Code]]></title>
    <url>%2F2018%2F05%2F05%2FBing-powered-settings-search-in-VS-Code%2F</url>
    <content type="text"><![CDATA[April 25, 2018 by Rob Lourens @roblourens and Ankith Karat ankar@microsoft.com Âú®VS Code‰∏≠ÊâæÂà∞ÁâπÂÆöËÆæÁΩÆÊúâÊ≤°ÊúâÂõ∞ÈöæÔºü ‰Ω†‰∏çÊòØ‰∏Ä‰∏™‰∫∫„ÄÇ Á∫µËßÇÂ∏∏ËßÅÁöÑGitHubÈóÆÈ¢òÔºåÊàë‰ª¨Â∑≤ÁªèÂÆåÊàêÁöÑStackOverflowÈóÆÈ¢òÔºåÊé®ÊñáÂíåÁî®Êà∑Á†îÁ©∂ÔºåÊàë‰ª¨ÁúãÂà∞ËÆ∏Â§öÁî®Êà∑Âú®Êü•ÊâæËÆæÁΩÆÊó∂ÈÅáÂà∞ÈóÆÈ¢ò„ÄÇ ËøôÊØ´‰∏çÂ•áÊÄ™ÔºåÂõ†‰∏∫VS CodeÂåÖÊã¨Ë∂ÖËøá400‰∏™ÂºÄÁÆ±Âç≥Áî®ÁöÑËÆæÁΩÆÔºåÂπ∂‰∏îÂÆâË£Ö‰∫ÜÊâ©Â±ïÁ®ãÂ∫èÔºåËÆ∏Â§öÁî®Êà∑ÂèØ‰ª•Êã•ÊúâÊõ¥Â§öÁöÑËÆæÁΩÆ„ÄÇ Â¶ÇÊûúÊÇ®ÂåÖÂê´ÂÖ∏ÂûãÁöÑÁî®Êà∑ÈîôËØØÔºàÂ¶ÇÈîôÂà´Â≠óÂíåÊåëÈÄâÊ≠£Á°ÆÁöÑÊêúÁ¥¢Êù°‰ª∂ÁöÑÊåëÊàòÔºâÔºåÁî®Êà∑Â∞±ÂæàÈöæ„ÄÇ Âá†‰∏™ÊúàÂâçÔºåÊàë‰ª¨ÂºÄÂßã‰∏éBingÂõ¢ÈòüË∞àËÆ∫‰ªñ‰ª¨ÊòØÂê¶ÂèØ‰ª•Â∞Ü‰ªñ‰ª¨ÁöÑÊêúÁ¥¢‰∏ìÈïøÂ∫îÁî®‰∫éÊàë‰ª¨ÁöÑÈóÆÈ¢ò„ÄÇ ‰∏§‰∏™ÊúàÂâçÔºåÊàë‰ª¨ÂèëÂ∏É‰∫ÜÁªìÊûú - Áî±BingÊèê‰æõÊîØÊåÅÁöÑÊô∫ËÉΩËÆæÁΩÆÊêúÁ¥¢‰ΩìÈ™å„ÄÇ How it worksÁªèËøá‰∏ÄÊÆµÊó∂Èó¥ÁöÑËÆ®ËÆ∫ÂíåÂéüÂûãËÆæËÆ°‰πãÂêéÔºåÊàë‰ª¨ÂÜ≥ÂÆöÂÆâÊéíBingÂõ¢ÈòüËøêË°åËÆæÁΩÆÊêúÁ¥¢ÊúçÂä°Ôºå‰ª•‰æø‰∏∫Áî®Êà∑Âú®VS CodeËÆæÁΩÆÁºñËæëÂô®‰∏≠ÊêúÁ¥¢ÁöÑÊü•ËØ¢Êèê‰æõÊô∫ËÉΩÊ®°Á≥äËÆæÁΩÆÂåπÈÖç„ÄÇ Â∞ÜBingÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÊêúÁ¥¢ÂäüËÉΩÊï¥ÂêàÂà∞VS Code‰∏≠ËØÅÊòéÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇ ‰∏∫‰∫ÜÂú®ÁΩëÁªú‰∏äÊêúÁ¥¢ÊñáÊ°£ÔºåBingËÄÉËôë‰∫ÜÊï∞ÂçÉ‰∏™‰∏éÈ°µÈù¢Áõ∏‰ººÂ∫¶ÔºåÁÇπÂáªÊï∞ÊçÆÔºåÁî®Êà∑Ë°å‰∏∫Êï∞ÊçÆÁ≠âÊúâÂÖ≥ÁöÑ‰ø°Âè∑„ÄÇ‰ΩÜÊòØÊàë‰ª¨Ê≤°ÊúâËøôÁßç‰∏∞ÂØåÁöÑÂÖÉÊï∞ÊçÆÂèØÁî®‰∫éÊàë‰ª¨ÁöÑËÆæÁΩÆ - Âè™ÊòØÊØè‰∏™ÁÆÄË¶ÅÁöÑÂêçÁß∞ÂíåËØ¥Êòé ‰∏Ä„ÄÇ Âõ†Ê≠§ÔºåBingÂõ¢ÈòüÂ∞ÜÂÆöÂà∂ÊúçÂä°ÂíåBingÁöÑÂü∫Êú¨ÊêúÁ¥¢ÂäüËÉΩÁªìÂêàÂú®‰∏ÄËµ∑ÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÈÄÇÂêàÊàë‰ª¨ÊêúÁ¥¢Âú∫ÊôØÁöÑÁ≥ªÁªü„ÄÇ ‰ª•‰∏ãÊòØËØ•Á≥ªÁªüÁöÑÈ´òÁ∫ßÊ¶ÇËø∞ÔºöÊàë‰ª¨Êù•ÁúãÁúãÊØè‰∏™ÈÉ®ÂàÜ„ÄÇ Ëøô‰∏™Á≥ªÁªüÂü∫Êú¨‰∏äÊúâ‰∏§‰∏™ÊñπÈù¢ - Êî∂ÈõÜËÆæÁΩÆÁªÜËäÇÂπ∂Â∞ÜÂÖ∂ÁºñÂÖ•Á¥¢ÂºïÔºåÂπ∂Âú®Á∫øÊèê‰æõÁªìÊûú„ÄÇ Á¨¨‰∏ÄÈÉ®ÂàÜÁî±ÊúçÂä°ÊúçÂä°ÂÆûÊñΩ„ÄÇ ÂÆÉË¥üË¥£ÂàõÂª∫ÂåÖÂê´VS‰ª£Á†ÅÊú¨Ë∫´ÂíåÊâ©Â±ïÁöÑ‰∏∞ÂØåÁ¥¢Âºï„ÄÇ Áî±‰∫éÊàë‰ª¨Â∏åÊúõÊü•ËØ¢ÂìçÂ∫îÊó∂Èó¥Â∞ΩÂèØËÉΩÁü≠ÔºåÂõ†Ê≠§Êàë‰ª¨Âú®ËøõË°åËÆæÁΩÆÊó∂Â∞ΩÂèØËÉΩÂ§öÂú∞ÂÆåÊàêÂ∑•‰ΩúÔºå‰ª•ÂáèÂ∞ëÂ§ÑÁêÜÊü•ËØ¢Êó∂ÊâÄË¶ÅÂÅöÁöÑÂ∑•‰Ωú„ÄÇ Bing Ingestion ServiceÊî∂ÈõÜVS‰ª£Á†ÅÂíåÊâ©Â±ïËÆæÁΩÆÊï∞ÊçÆ Âú®ÊØèÊ¨°ÊûÑÂª∫ÊúüÈó¥ÔºåVS‰ª£Á†ÅÂ∞Ü‰ª•ÊâÄÊúâÈÖçÁΩÆÂÜôÂÖ•JSONÊñá‰ª∂ÁöÑÊ®°ÂºèÂêØÂä®„ÄÇ Êàë‰ª¨ÂøÖÈ°ªÂÆûÈôÖÂêØÂä®VS CodeÔºåÂõ†‰∏∫Êàë‰ª¨Êó†Ê≥ïÈùôÊÄÅÁ°ÆÂÆöÊâÄÊúâÈÖçÁΩÆÂÖÉÊï∞ÊçÆ„ÄÇ ËØ•Êñá‰ª∂ÂåÖÂê´ÊØè‰∏™ËÆæÁΩÆÁöÑÂá†Êù°‰ø°ÊÅØ - ÂêçÁß∞ÔºåËØ¥ÊòéÔºåÁ±ªÂûãÔºåÈªòËÆ§ÂÄº‰ª•Âèä‚ÄúÊûö‰∏æ‚ÄùÁ±ªÂûãËÆæÁΩÆÔºåÊúâÊïàÂÄºÂàóË°®ÂèäÂÖ∂ËØ¥Êòé„ÄÇ ÁÑ∂ÂêéÔºåÊàë‰ª¨Â∞ÜÊñá‰ª∂‰∏ä‰º†Âà∞AzureÂ≠òÂÇ®„ÄÇ Â¶ÇÊûú‰Ω†Â•ΩÂ•áÔºå‰Ω†ÂèØ‰ª•Âú®ËøôÈáåÁúãÂà∞‰∏Ä‰∏™ÊúÄËøëÁöÑ‰æãÂ≠êÔºöhttps://ticino.blob.core.windows.net/configuration/123000832/c1cd4378‚Ä¶/configuration.json 123000832ÊòØ‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑÂÜÖÈÉ®ÁâàÊú¨Âè∑ÔºåÊ†πÊçÆ‰∫ßÂìÅÁâàÊú¨Âä†‰∏äËá™‰∏ä‰∏ÄÁâàÊú¨‰ª•Êù•ÁöÑGitÊèê‰∫§Êï∞ÈáèËÆ°ÁÆóÂæóÂá∫„ÄÇ c1cd4378 ‚Ä¶ÊòØÊûÑÂª∫ÁâàÊú¨ÁöÑGit commit id„ÄÇÊèêÂ•ëËØ∫Ôºå‰Ω†‰ª¨ÁöÑ‰∏Ä‰∫õÊ≠ªÂø†Á≤â‰∏ùÂèØËÉΩËÆ∞ÂæóÔºåÊòØÊàë‰ª¨ÊúÄÂàùÁöÑÁü≠ÂëΩ‰ª£Âè∑„ÄÇ ÂøÖÂ∫îÁöÑËΩÆËØ¢ÊúçÂä°ÁõëËßÜAzureÂ≠òÂÇ®ÂÆπÂô®ÔºåÊ≥®ÊÑèÂà∞‰∏Ä‰∏™Êñ∞ÁöÑÁâàÊú¨ÔºåÂπ∂ÈÄöÁü•ÊëÑÂèñÊúçÂä°„ÄÇÂêåÊó∂ÔºåBing‰∏çÊñ≠Âú∞ÊäìÂèñVS CodeÊâ©Â±ïÂ∏ÇÂú∫ÔºåÁ≠âÂæÖÊâ©Â±ïÊõ¥Êñ∞ÂíåÊñ∞Êâ©Â±ï„ÄÇÂΩìÊâæÂà∞ÂÆÉÊó∂ÔºåÂÆÉ‰∏ãËΩΩÂÆÉÁöÑpackage.jsonÊñá‰ª∂ÔºàÂØπ‰∫éÊâ©Â±ïÔºåÊâÄÊúâÈÖçÁΩÆÂÖÉÊï∞ÊçÆÈÉΩÂåÖÂê´Âú®package.json‰∏≠Ôºå‰∏çÈúÄË¶ÅÂêØÂä®ÂÆÉÔºâÔºåÂπ∂Â∞ÜËøô‰∫õËÆæÁΩÆ‰º†ÈÄíÁªôIngestion Service„ÄÇ Êï¥‰∏™ËøáÁ®ãÈÉΩÊòØÂÆåÂÖ®Ëá™Âä®ÂåñÁöÑÔºåÂπ∂‰∏î‰∏çÊñ≠Êõ¥Êñ∞Êàë‰ª¨ÊØè‰∏™Á®≥ÂÆöÁâàÊú¨ÊûÑÂª∫ÂíåÊàë‰ª¨ÊØèÊó•ÂÜÖÂπï‰∫∫ÂëòÊûÑÂª∫ÁöÑÂÆûÊó∂Á¥¢ÂºïËÆæÁΩÆ„ÄÇÂú®ÊûÑÂª∫ÂÆåÊàêÁöÑÂá†ÂàÜÈíüÂÜÖÔºåBingÁöÑÁ¥¢ÂºïÂ∑≤Êõ¥Êñ∞‰∏∫ÂåÖÂê´‰ªª‰ΩïÊñ∞Ê∑ªÂä†ÁöÑËÆæÁΩÆ„ÄÇ1. Alternative Words PipelineÁî®Êà∑ÊúâÊó∂‰ºöÊêúÁ¥¢‰∏éÊàë‰ª¨Âú®ËÆæÁΩÆÂêçÁß∞ÂíåËØ¥Êòé‰∏≠‰ΩøÁî®ÁöÑËØçËØ≠‰∏çÂêå‰ΩÜÁõ∏ÂêåÁöÑËØçËØ≠„ÄÇ ‰∏∫Á°Æ‰øùÊàë‰ª¨ËÉΩÂ§üÂ§ÑÁêÜËøô‰∫õÊÉÖÂÜµÔºåÊàë‰ª¨Êï¥Âêà‰∫ÜBingÁöÑ‚ÄúAlternative Word‚ÄùÁîüÊàêÁÆ°ÈÅì„ÄÇ ËØ•ÁÆ°ÈÅì‰ΩøÁî®Áî®Êà∑Ë°å‰∏∫ÔºåÁÇπÂáªÊ¨°Êï∞ÔºåÂú®Á∫øÊéíÂêçÂíåÈ°µÈù¢Áõ∏‰ººÂ∫¶Á≠â‰ø°Âè∑Ôºå‰ªéBingÁöÑÊêúÁ¥¢Êï∞ÊçÆ‰∏≠Êî∂ÈõÜÂÖ∑ÊúâÁ±ª‰ººÂê´‰πâÁöÑÂçïËØç„ÄÇ ‰æãÂ¶ÇÔºå‚ÄúÊõ¥Êñ∞‚ÄùÂíå‚ÄúÂçáÁ∫ß‚ÄùË¢´ËÆæÁΩÆ‰∏∫‚ÄúÊõø‰ª£ËØç‚ÄùÔºåÂπ∂‰∏îÊêúÁ¥¢‰∏Ä‰∏™Â∞ÜËøîÂõûÂåÖÂê´Âè¶‰∏Ä‰∏™ÁöÑËÆæÁΩÆ„ÄÇ2. Stemmer and Speller PipelineÊàë‰ª¨‰∏çÊÉ≥ÊÉ©ÁΩöÁî®Êà∑ÊãºÂÜôÈîôËØØÁöÑËÆæÁΩÆÂêçÁß∞Ôºå‰ΩÜÊàë‰ª¨ÂæàÊó©Â∞±ÂèëÁé∞ÔºåÁÆÄÂçïÁöÑÊ®°Á≥äÂåπÈÖçÂèØËÉΩ‰ºöÈîôËøá‰∫∫Á±ªÂèØ‰ª•ÁêÜËß£ÁöÑËã±ËØ≠ÂçïËØçÁöÑÂèò‰ΩìÔºåÊàñËÄÖ‰ºöÂåÖÂê´Â§™Â§öÁöÑÈîôËØØËÇØÂÆöÂåπÈÖç„ÄÇ Âõ†Ê≠§ÔºåÊàë‰ª¨ËøòÊèê‰æõ‰∫ÜSpellerÂíåStemmerÊúçÂä°ÔºåËØ•ÊúçÂä°ÊòØ‰ªéBing.com‰∏ä‰ΩøÁî®ÁöÑÂÖ®Â•óÊúçÂä°‰∏≠ÊèêÂèñÁöÑÔºåËøô‰∫õÊúçÂä°‰ΩøÁî®Áõ∏ÂêåËØçÂπ≤ÁöÑÂ∏∏ËßÅÊãºÂÜôÈîôËØØÂíåÊõø‰ª£ÂΩ¢Âºè‰∏∞ÂØåÁ¥¢Âºï„ÄÇ ‰æãÂ¶ÇÔºå‚ÄúÊ†ºÂºèÂåñ‚ÄùÔºå‚ÄúÊ†ºÂºèÂåñÁ®ãÂ∫è‚ÄùÔºå‚ÄúÊ†ºÂºèÂåñ‚Äù - Â∞Ü‰ΩøÁî®‚ÄúÊ†ºÂºèÂåñ‚Äù‰∏ÄËØçËøõË°åÁ¥¢Âºï„ÄÇ3. Natural Language Processing (NLP) PipelineÊàë‰ª¨ËøòÂ∏åÊúõÁî®Êà∑ËÉΩÂ§ü‰ª•Ëá™Â∑±ÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞‰ªñ‰ª¨ÁöÑÊü•ËØ¢ÔºåÂõ†Ê≠§Êàë‰ª¨Ê∑ªÂä†‰∫ÜBingÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁÆ°ÈÅì„ÄÇ ÁÆ°ÈÅìÊî∂ÈõÜÂ∏∏Áî®ÁöÑËØ≠Èü≥ÂíåÊñáÊú¨Ê®°ÂºèÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨Ê∑ªÂä†Âà∞Á¥¢Âºï‰∏≠„ÄÇ ‰æãÂ¶ÇÔºåÂÆÉ‰ΩøÁ≥ªÁªüËÉΩÂ§üËØÜÂà´‚ÄúÂ¶Ç‰ΩïÁ¶ÅÁî®cssÈ™åËØÅ‚Äù‰∏≠ÁöÑÈáçË¶ÅËØçËØ≠‰ª•ÊâæÂà∞‚Äúcss.validate‚Äù„ÄÇ4. Feedback/Ranking PipelineÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂèçÈ¶àÊú∫Âà∂ÔºåÂèØ‰ª•ËÆ©Êàë‰ª¨‰ªéÁî®Êà∑ÂèçÈ¶à‰∏≠Â≠¶‰π†ÂíåÊîπËøõ„ÄÇ ÂÆÉÂÖÅËÆ∏Êàë‰ª¨ÊâãÂä®ÊåáÂÆöÊñ∞ÁöÑÂçïËØçÂØπÊàñÊèêÈ´òÊüê‰∫õÊü•ËØ¢ÁöÑÈ¢ÑÊúüÁªìÊûú„ÄÇ ÂèçÈ¶àË¢´‰∏ä‰º†Âà∞ÊúçÂä°‰∏≠ÔºåÂπ∂Âá†‰πéÁ´ãÂç≥ÂèçÊò†Âú®ÊêúÁ¥¢ÁªìÊûú‰∏≠„ÄÇGating ModuleÊØè‰∏ÄÊ¨°ËøõÂÖ•Á¥¢ÂºïÈÉΩ‰ºöÈÄöËøáÈó®ÊéßÊ®°ÂùóÔºåÂÆÉÂè™ÊòØÁ°Æ‰øùÁ¥¢ÂºïÁî±‰∫éÊüê‰∫õÁºñÁ®ãÈîôËØØËÄå‰∏ç‰ºöË¢´Á†¥Âùè„ÄÇ Êàë‰ª¨ÁºñÂÜô‰∫ÜÊµãËØïÁî®‰æãÊù•È™åËØÅ‰ª•‰∏ãÂÜÖÂÆπÔºö Êñ∞ÁöÑÁ¥¢ÂºïÊòØÂêëÂêéÂÖºÂÆπÁöÑÔºåÂèØÁî®‰∫éÊâÄÊúâVS‰ª£Á†ÅÊûÑÂª∫ Êàë‰ª¨ÁöÑGoldenÊü•ËØ¢ÈõÜËøîÂõûÈ¢ÑÊúüÁªìÊûú Èó®ÊéßÊ®°ÂùóÂ§±Ë¥•Â∞ÜÈòªÊ≠¢Á¥¢ÂºïÊëÑÂÖ•Âπ∂Á´ãÂç≥ÈÄöÁü•Âõ¢Èòü„ÄÇ ËøòÂàõÂª∫‰∫Ü‰ª™Ë°®ÊùøÊúçÂä°Ôºå‰ΩøÊàë‰ª¨ËÉΩÂ§üÁõëÊµãÁÆ°ÈÅìÂêÑ‰∏™Èò∂ÊÆµÁöÑÂÅ•Â∫∑ÊÉÖÂÜµ„ÄÇ ÂÆÉÂÖ∑ÊúâË≠¶Êä•Êú∫Âà∂Âπ∂ËÉΩÂ§üÂõûÊªöÂà∞ÊúÄÂêéÂ∑≤Áü•ÁöÑËâØÂ•ΩÁä∂ÊÄÅÔºå‰ª•Á°Æ‰øùËÉΩÂ§ü‰ª•ÊúÄÁü≠ÁöÑÂÅúÊú∫Êó∂Èó¥Âø´ÈÄüËß£ÂÜ≥‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ Search ServiceÊúÄÂêéÔºåÂú®ËøêË°åÊó∂ÔºåÊù•Ëá™Áî®Êà∑ÁöÑÊü•ËØ¢Â∞ÜËÆøÈóÆAzureË¥üËΩΩÂùáË°°Âô®ÊúçÂä°ÔºåËØ•ÊúçÂä°Ê†πÊçÆÂÖ∂Áâ©ÁêÜÊé•ËøëÂ∫¶ÊàñÂΩìÂâçË¥üËΩΩÈÄâÊã©Êàë‰ª¨ÁöÑÊüê‰∏™Âú∞ÁêÜÂ§çÂà∂ÊúçÂä°Âô®Êù•Â§ÑÁêÜÊü•ËØ¢„ÄÇ ÊâòÁÆ°Âú®ËØ•‰ΩçÁΩÆÁöÑÊêúÁ¥¢ÊúçÂä°ÈÄöËøáÁ¥¢Âºï‰∏≠ÁöÑÊü•ÊâæÊù•Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÁªìÊûúÔºåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÂ∫îÁî®ÊâãÂä®ÊéíÂ∫èË¶ÜÁõñÂπ∂Â∞ÜÂÖ∂ËøîÂõûÁªôVS‰ª£Á†ÅÂÆ¢Êà∑Á´Ø„ÄÇ Putting it all togetherÊàë‰ª¨Áé∞Âú®Êúâ‰∏Ä‰∏™Á≥ªÁªüÔºåÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£ËÆæÁΩÆÊü•ËØ¢ÔºåÂπ∂‰∏∫ËÆ∏Â§ö‰ª•ÂâçÊ≤°Êúâ‰ªª‰ΩïËøîÂõûÁöÑÊü•ËØ¢Êèê‰æõÁªìÊûú„ÄÇËøôÈáåÊúâ‰∫õ‰æãÂ≠êÔºö Â¶ÇÊûúÊÇ®ÊúâÁ±ª‰ººÁöÑÈóÆÈ¢òÔºåÂπ∂‰∏îÊ≤°ÊúâÊêúÁ¥¢Âõ¢ÈòüÂÉèBingÂõ¢ÈòüÈÇ£Ê†∑‰∏∫ÊÇ®Âª∫Á´ãÂÆöÂà∂ÊúçÂä°ÔºåÊàë‰ª¨‰ªçÁÑ∂Êúâ‰∏Ä‰∫õÂ•ΩÊ∂àÊÅØ„ÄÇ ÊÇ®ÂèØ‰ª•ÂºÄÂßã‰ΩøÁî®BingÁöÑËÆ§Áü•ÊúçÂä°ÔºåÂÆÉÂèØ‰ª•Â∏ÆÂä©ÊÇ®Âú®Ëá™Â∑±ÁöÑÂ∫îÁî®‰∏≠Ê∑ªÂä†‰∏Ä‰∫õÊô∫ËÉΩ„ÄÇ ‰æãÂ¶ÇÔºö Bing Spell Check API Language Understanding (LUIS) Bing Web Search API Bing Custom Search API A note about testingÂú®ÂºÄÂèëËøô‰∏™Á≥ªÁªüÊó∂ÔºåÊàë‰ª¨ÈúÄË¶Å‰∏ÄÁßçÂÆöÈáèËØÑ‰º∞ÁªìÊûúÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨ÂÜ≥ÂÆöÂª∫Á´ã‰∏Ä‰∏™Âü∫‰∫éÊ†áÂáÜÊäòÊâ£Á¥ØÁßØÊî∂ÁõäÔºàNDCGÔºâÊ¶ÇÂøµÁöÑÊµãËØïÊ°ÜÊû∂„ÄÇÊ≤°ÊúâÂ§™Â§öÁöÑÊùÇËçâÔºåËøôÊòØ‰∏ÄÁßçËØÑÂàÜÊêúÁ¥¢ÂºïÊìéÁªìÊûúÁöÑÊñπÊ≥ïÔºåÁªôÂá∫Êü•ËØ¢Ôºå‰∏ÄÁªÑÁªìÊûúÂíåËøô‰∫õÁªìÊûúÁöÑÂàÜÊï∞„ÄÇÊàë‰ª¨ÊâãÂ∑•ÁºñÂÜô‰∫ÜÂæàÂ§öÊµãËØïÁî®‰æãÔºå‰ΩÜÊÑèËØÜÂà∞Êàë‰ª¨ÈúÄË¶Å‰∏ÄÁßçËá™Âä®ÂåñÁöÑÊñπÂºèÊù•‰∏∫ÊâÄÊúâËÆæÁΩÆÁîüÊàêÊµãËØïÁî®‰æãÔºåÂåÖÊã¨Ê∑ªÂä†ÁöÑÊñ∞ËÆæÁΩÆÂíåÊâ©Â±ï‰∏≠ÁöÑËÆæÁΩÆ„ÄÇÊâÄ‰ª•Êàë‰ª¨ÁºñÂÜô‰∫Ü‰∏Ä‰∏™Â∑•ÂÖ∑ÔºåÂèØ‰ª•‰∏∫‰ªª‰ΩïËÆæÁΩÆËá™Âä®ÁîüÊàêÊµãËØïÁî®‰æã„ÄÇÂÆÉ‰ΩøÁî®ËÆæÁΩÆÂêçÁß∞ÂíåÊèèËø∞‰∏≠ÁöÑÂçïËØçÔºåÂπ∂ÈÄöËøá‰∏çÂêåÁöÑÂèòÊç¢Âô®ËøêË°åÂÆÉ‰ª¨Ôºå‰ª•Ê®°ÊãüÁî®Êà∑ÈÄâÊã©Êõø‰ª£ÂçïËØçÔºåÂà∂‰ΩúÊãºÂÜôÈîôËØØÊàñ‰ΩøÁî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊ®°ÂºèËøõË°åÊêúÁ¥¢„ÄÇÊàë‰ª¨Ëøò‰∏∫‰∏Ä‰∫õÂ∏∏Áî®Êâ©Â±ïÁöÑËÆæÁΩÆÁîüÊàê‰∫ÜÊµãËØïÁî®‰æã„ÄÇ Êàë‰ª¨ÊØè6Â∞èÊó∂ËøêË°å‰∏ÄÊ¨°ÂÆåÊï¥ÁöÑÊµãËØïÂ•ó‰ª∂ÔºåÂπ∂‰∏îÂèØ‰ª•Ëá™Âä®ËøõË°åËá™ÊàëÊõ¥Êñ∞Ôºå‰ª•‰æøÂßãÁªàÊµãËØïÊúÄÊñ∞ÁâàÊú¨ÁöÑËÆæÁΩÆ„ÄÇÊµãËØïÂ•ó‰ª∂ÂêëÊàë‰ª¨‰øùËØÅÁ≥ªÁªüËøêË°åÊ≠£Â∏∏ÔºåÂπ∂‰ΩøÊàë‰ª¨Áõ∏‰ø°ÔºåÂΩìÊàë‰ª¨Âú®ÂêéÁ´ØËøõË°åÊõ¥ÊîπÊó∂ÔºåÊàë‰ª¨‰∏ç‰ºöÊçüÂÆ≥ÁªìÊûúË¥®Èáè„ÄÇ What‚Äôs nextÊúâÂá†ÁßçÊñπÊ≥ïÂèØ‰ª•ÁªßÁª≠ÊîπËøõÁ≥ªÁªü„ÄÇ‰æãÂ¶ÇÔºåÊàë‰ª¨ËøòÂª∫Á´ã‰∫Ü‰∏Ä‰∏™Âü∫‰∫éÁî®Êà∑Ë°å‰∏∫ÁöÑËá™Âä®ÂèçÈ¶àÂæ™ÁéØ„ÄÇÂ¶ÇÊûúÂæàÂ§ö‰∫∫ÊêúÁ¥¢Á±ª‰ººÁöÑÊü•ËØ¢ÔºåÁÑ∂ÂêéÈÄâÊã©Áõ∏ÂêåÁöÑÁªìÊûúÔºåÊàë‰ª¨Áü•ÈÅìÁªìÊûúÂèØËÉΩÊòØÂ•ΩÁöÑÔºåÂ∫îËØ•ÊéíÂú®Êõ¥È´òÁöÑ‰ΩçÁΩÆ„ÄÇ ÁõÆÂâçËØ•ÊúçÂä°‰ªÖ‰ª•Ëã±ÊñáÁºñÂà∂Á¥¢ÂºïÔºå‰ΩÜÊàë‰ª¨Â∏åÊúõ‰∏∫ÁøªËØëÁöÑËÆæÁΩÆËØ¥ÊòéÁºñÂà∂Á¥¢ÂºïÂπ∂ÊîØÊåÅ‰ª•ÈùûËã±ÊñáËØ≠Ë®ÄËøõË°åÊêúÁ¥¢„ÄÇËøòÊúâ‰∏Ä‰∫õÈÖçÁΩÆÂÖÉÊï∞ÊçÆÁõÆÂâçÊ≤°ÊúâÁ¥¢ÂºïÔºå‰æãÂ¶Ç‚Äúworkbench.colorCustomizations‚ÄùËÆæÁΩÆÁöÑÂèØËÉΩÂÄº„ÄÇÂπ∂Ëøõ‰∏ÄÊ≠•ÊêúÁ¥¢ÔºåÊàë‰ª¨ÊÉ≥ÊòæÁ§∫ÁõÆÂâçÊú™ÂÆâË£ÖÁöÑÊâ©Â±ïÁöÑÁªìÊûú„ÄÇÂ¶ÇÊûúÊÇ®ÊêúÁ¥¢‚Äúdebug python‚ÄùÔºåÂπ∂‰∏îÊ≤°Êúâ‰∏éÊú¨Âú∞ËÆæÁΩÆÁõ∏ÂåπÈÖçÁöÑÂº∫ÂåπÈÖçÈ°πÔºåÈÇ£‰πàÊàë‰ª¨Â∏åÊúõÂºïÂØºÊÇ®‰ΩøÁî®ÂèØ‰ª•Â∏ÆÂä©ÊÇ®Ë∞ÉËØïPython‰ª£Á†ÅÁöÑÊâ©Â±ï„ÄÇÊàë‰ª¨‰πüÂú®VS Code‰∏≠ËÄÉËôëËøáËøôÁßçÊäÄÊúØÁöÑÂÖ∂‰ªñÂ∫îÁî®„ÄÇ‰πüËÆ∏ÂëΩ‰ª§Èù¢ÊùøÂèØ‰ª•‰ªéÁ±ª‰ººÁöÑÊúçÂä°‰∏≠ÂèóÁõä„ÄÇ We need your feedbackÁé∞Âú®Êõ¥ÂÆπÊòìÊâæÂà∞ËÆæÁΩÆÔºåËøôË¶ÅÊÑüË∞¢BingÂõ¢Èòü‰∏≠ÁöÑÊúãÂèãÔºÅ Áî®Êà∑ÂèçÈ¶àÊòØÊàë‰ª¨ÊîπËøõÁªìÊûúÁöÑÊúÄ‰Ω≥ÊñπÂºèÔºåÂõ†Ê≠§ÔºåÂ¶ÇÊûúÊÇ®Ê≤°ÊúâÁúãÂà∞ÊÇ®ÊúüÊúõÁöÑÁªìÊûúÔºåÈÇ£‰πàÂú®ÊÇ®ÊêúÁ¥¢ËÆæÁΩÆÊó∂ÔºåËØ∑Âú®GitHub‰∏äÊèê‰∫§ÈóÆÈ¢ò„ÄÇ ‰∫ãÂÆû‰∏äÔºåÂ¶ÇÊûú‰Ω†‰ΩøÁî®VS Code InsidersÔºå‰Ω†ÁîöËá≥‰ºöÁúãÂà∞‰∏Ä‰∏™ÊåâÈíÆÔºåÂÆÉ‰ºöË∞ÉÁî®Êàë‰ª¨ÁöÑÊñ∞ÈóÆÈ¢òËÆ∞ËÄÖÔºå‰Ωø‰Ω†Êõ¥ÂÆπÊòìÊèê‰∫§ÂåÖÂê´Êàë‰ª¨ÈúÄË¶ÅÁöÑÊâÄÊúâÁªÜËäÇÁöÑÈóÆÈ¢ò„ÄÇ ÁºñÁ†ÅÂø´‰πêÔºÅ]]></content>
      <tags>
        <tag>ÁøªËØë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂÖ≥‰∫éNlogÂú®dotNet Core2.0ÊéßÂà∂Âè∞Á®ãÂ∫è‰∏≠ÁöÑ‰ΩøÁî®]]></title>
    <url>%2F2018%2F05%2F03%2F%E5%85%B3%E4%BA%8ENlog%E5%9C%A8dotNet-Core2.0%E6%8E%A7%E5%88%B6%E5%8F%B0%E7%A8%8B%E5%BA%8F%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ÊëòË¶ÅÔºö ËøôÂè™ÊòØNlogÁöÑÂ≠¶‰π†Á¨îËÆ∞„ÄÇÊïôÁ®ãÁõ∏ÂÖ≥ÊïôÁ®ãÈÉΩÂæàËØ¶ÁªÜ‰∫ÜÔºåÂú®Ê≠§‰∏çËµòËø∞„ÄÇ ÂºÄÂßãÂÖàÂç†Âùë Ë∏©Âà∞ÁöÑÈõ∑Nlog‰∏çÂ∑•‰ΩúÂâçÊèêÔºöÈÖçÁΩÆÊñá‰ª∂ÂÜÖÂÆπÊ≤°ÊúâÈîôÔºå‰ª£Á†ÅÊ≤°ÊúâÈîôÂéüÂõ†ÔºöÈÖçÁΩÆÊñá‰ª∂È´òÁ∫ßÂ±ûÊÄß‰∏≠ÔºåÂ§çÂà∂Âà∞ËæìÂá∫ÁõÆÂΩïÈÖçÁΩÆÈîô„ÄÇËß£ÂÜ≥ÂäûÊ≥ïÔºöÂ∞ÜÂ§çÂà∂Âà∞ËæìÂá∫ÁõÆÂΩï Â±ûÊÄßÊîπ‰∏∫]]></content>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>ÊïôÁ®ã</tag>
        <tag>Nlog</tag>
        <tag>.Net Core</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo‚Äî‚ÄîYilia‰∏ªÈ¢òÊ∑ªÂä†ÊñáÁ´†ÁΩÆÈ°∂]]></title>
    <url>%2F2018%2F04%2F29%2FHexo%E2%80%94%E2%80%94Yilia%E4%B8%BB%E9%A2%98%E7%9A%84%E5%BD%92%E6%A1%A3%E9%A1%B5%E7%BD%AE%E9%A1%B6%2F</url>
    <content type="text"><![CDATA[ÊëòË¶ÅÔºö ÂÖ∂ÂÆûÂ∞±ÊòØËÆ∞ÂΩï‰∏Ä‰∏ãYilia‰∏ªÈ¢òÁöÑÊñáÁ´†ÁΩÆÈ°∂ÊÄé‰πàÂä†„ÄÇÁôæÂ∫¶ÊêúÁ¥¢Èù†ÂâçÁöÑÂÖ≥‰∫éÊñáÁ´†ÁΩÆÈ°∂ÁöÑÂçöÂÆ¢ÈÉΩÊòØÂÖ≥‰∫éNexT‰∏ªÈ¢òÁöÑ„ÄÇ‰∏çÂæó‰∏çÊâøËÆ§ËøòÊòØNexT‰∏ªÈ¢òÊØîËæÉÁÅ´‰∏Ä‰∫õ„ÄÇÂèØÊòØÈÇ£‰∏™‰∏ªÈ¢òÂØπÊàëÊù•ËØ¥Â§™ÂçïË∞É‰∫ÜÔºåËôΩÁÑ∂ÂæàÁÆÄÊ¥ÅÂ§ßÊ∞îÔºåÂäüËÉΩ‰πüÂ§ö„ÄÇ‰ΩÜÊòØYiliaÁöÑÁïåÈù¢È£éÊ†ºÁúüÊòØGetÂà∞ÊàëÁöÑÁÇπ‰∫ÜÔºå‰∏çÊÉ≥Êç¢„ÄÇËØùËØ¥Ëøô‰∏™‰∏ªÈ¢òÂ•Ω‰πÖÊ≤°Êõ¥Êñ∞‰∫ÜÔºå‰πü‰∏çÁü•ÈÅì‰ΩúËÄÖÊòØ‰∏çÊòØ‰∏çÊÉ≥Êêû‰∫ÜÔºåËøôÂèØÊòØGayHub‰∏äÈù¢ÊéíÂêçÁ¨¨‰∫åÁöÑ‰∏ªÈ¢òÂïä„ÄÇ‰ΩúËÄÖÂä†Ê≤πÂïäü§£ ÂºÄÂßãblogÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§„ÄÇËá≥‰∫éÂéüÂõ†ÂòõÔºåÊàë‰πü‰∏çÁü•ÈÅìüôÉ 12npm uninstall hexo-generator-index --savenpm install hexo-generator-index-pin-top --save ÁÑ∂ÂêéÂú®ÊñáÁ´†Â§¥Ê∑ªÂä† top:true Â¶ÇÔºö12345678---title: Áªà‰∫éË£ÖÂ•ΩHexoÂï¶date: 2018-04-19 00:04:46tags: ÂøÉÊÉÖtop: truecategories:- ‰Ω†Â•ΩÔºå‰∏ñÁïå--- ÊúÄÁªàÊïàÊûú]]></content>
      <categories>
        <category>HexoÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>ÊïôÁ®ã</tag>
        <tag>hexo‰∏™ÊÄßÂåñ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÊàëÂØπÊØîÁâπÂ∏Å‰∏éÂå∫ÂùóÈìæÁöÑÁêÜËß£]]></title>
    <url>%2F2018%2F04%2F22%2F%E6%88%91%E5%AF%B9%E6%AF%94%E7%89%B9%E5%B8%81%E4%B8%8E%E5%8C%BA%E5%9D%97%E9%93%BE%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[ËÆ∞ÂΩï‰∏Ä‰∏ãÂ≠¶‰π†Âå∫ÂùóÈìæÁöÑÁü•ËØÜÁÇπ Âå∫ÂùóÂõ†‰Ωï‰∫ßÁîüÂå∫ÂùóÊòØÊåñÁüøÁ®ãÂ∫èËá™Âä®‰∫ßÁîüÁöÑ„ÄÇ‰ª•ÊØîÁâπÂ∏ÅÊù•ËØ¥Ôºå‰∏ÄÁ¨î‰∫§ÊòìÂè™Êúâ500ByteÂ∑¶Âè≥ÔºåËÄå‰∏Ä‰∏™Âå∫ÂùóÁöÑÂ§ßÂ∞è‰∏∫1MBÂõ†Ê≠§ÂèØ‰ª•Â≠òÊîæ2000Â§öÁ¨î‰∫§Êòì„ÄÇÂÖ®ÁΩëÊâÄÊúâ‰∫ßÁîüÁöÑ‰∫§ÊòìÊï∞ÊçÆÈÉΩ‰ºöÂπøÊí≠Âà∞ÊâÄÊúâÁüøÂ∑•ËäÇÁÇπ„ÄÇÁî±ÊåñÁüøÁ®ãÂ∫èËá™Âä®ÂÜôÂÖ•Âå∫Âùó‰πã‰∏≠„ÄÇÂå∫Âùó‰∏çÊòØÁ´ãÂç≥Â∞±ÊúâÁöÑÔºåËÄåÊòØÊØèÂçÅÂàÜÈíüÁîüÊàê‰∏Ä‰∏™„ÄÇ ‰∫ßÁîüÊó∂Èó¥‰∏∫‰ªÄ‰πàÊòØ10ÂàÜÈíüÂå∫ÂùóÁöÑÁîüÊàêÈó¥ÈöîÊòØ‰ª£Á†Å‰∏≠ÂÆö‰πâÂ•ΩÁöÑÔºåÁüøÂ∑•ÊòØ‰∏çËÉΩÊîπÁöÑ„ÄÇÂØπÁüøÂ∑•‰∏™‰∫∫Êù•ËØ¥Ôºå‰πüÂ∞±ÊòØËØ¥ÂçÅÂàÜÈíü‰πãÂÜÖÔºåÊåñÁüøÁ®ãÂ∫èÈúÄË¶ÅÊâæÂà∞‰∏Ä‰∏™È≠îÊï∞ÔºåÂä†Âà∞Ëá™Â∑±ÁîüÊàêÁöÑÂå∫ÂùóÈìæÂ§¥Èáå„ÄÇÂπ∂ÂπøÊí≠Âá∫Âéª„ÄÇËøôÊ†∑ÂØπËøô‰∏™ÁüøÂ∑•Êù•ËØ¥ÊâçÊòØÁ°ÆÁ°ÆÂÆûÂÆûËÆ∞Êàê‰∫Ü‰∏ÄÁ¨îË¥¶ÔºåÂÖ∂ÂÆû‰∏çÂè™ÊòØ1Á¨îËÄåÊòØÂæàÂ§öÁ¨îË¥¶„ÄÇÂèñÂÜ≥‰∫éËøô10ÂàÜÈíü‰πãÂÜÖÔºåÊúâÂ§öÂ∞ë‰∫§Êòì‰ø°ÊÅØÂà∞ËææÁüøÂ∑•Â§Ñ„ÄÇÁî±‰∫é‰∏Ä‰∏™Âå∫ÂùóÂè™ËÉΩÂ≠òÊîæ2000Á¨î‰∫§ÊòìÔºåÂ¶ÇÊûúÊ≠§Êó∂ ‰Ω†ÁöÑ‰∫§ÊòìÊÅ∞Â•ΩÊòØÁ¨¨2001‰∏™ ÈÇ£‰πàÂØπ‰Ω†Êù•ËØ¥‰Ω†ÈúÄË¶ÅÁ≠â20ÂàÜÈíü„ÄÇÂú®Á≠âÂæÖÊúüÈó¥Ôºå‰Ω†ÁöÑ‰ø°ÊÅØ‰Ωç‰∫éÂÜÖÂ≠òÊ±†‰∏≠Ôºå‰πüÂ∞±ÊòØÂú®ÂÜÖÂ≠òÈáåÁ≠âÂæÖÂ§ÑÁêÜ„ÄÇÁ≠âÂà∞Á¨¨1‰∏™ÂçÅÂàÜÈíüËøáÂéª‰πãÂêéÔºåÊñ∞ÂùóÂá∫Áé∞„ÄÇÂ•ΩÔºå‰Ω†ÊòØ‰∏çÊòØÂú®ÊÉ≥ÔºöÁ¨¨‰∏Ä‰∏™ËøõÁ≠âÂæÖÊ±†ÁöÑÊòØ‰∏çÊòØ‰πü‰ºöÁ¨¨‰∏Ä‰∏™ËøõÂùóÔºüÁÑ∂ËÄåÂπ∂‰∏çÊòØÔºåÊåñÁüøÁ®ãÂ∫è‰ºöÈÄâÊã©ÊâãÁª≠Ë¥πËæÉÂ§öÁöÑÈÇ£‰∏™ÂÜôËøõÂùóÈáå„ÄÇÊéíÂ∫èËßÑÂàô‰∏∫ÔºàÊâãÁª≠Ë¥π/‰∫§ÊòìKBÂ§ßÂ∞èÔºâÂ¶ÇÊûúÁΩëÁªú‰∫§ÊòìÈáèÂ∑®Â§öÔºåËÄå‰Ω†ÁöÑÊâãÁª≠Ë¥πÂèàÂ∑®Â∞ëÔºå‰Ω†Âú®ÊâÄÊúâÁüøÂ∑•ÈÇ£ÈáåÈÉΩÊòØË¶ÅË¢´Â´åÂºÉÁöÑÔºåÂõ†Ê≠§‰Ω†ÁöÑ‰∫§Êòì‰ºöÂæà‰πÖÂæà‰πÖÂæó‰∏çÂà∞Á°ÆËÆ§Ôºå‰Ω†Â∞±ÂæóÁ≠âÂæà‰πÖÂæà‰πÖÊâçË°å„ÄÇ Âå∫ÂùóÊòØÂ¶Ç‰ΩïËøûÊé•Ëµ∑Êù•ÁöÑÂå∫ÂùóÈìæÁöÑÁªìÊûÑÂèØ‰ª•ËßÜ‰∏∫‰∏Ä‰∏™ÂûÇÁõ¥ÁöÑÊ†àÔºåÈ¶ñÂå∫ÂùóÂú®Ê†àÂ∫ï„ÄÇËøôÊ†∑Â∞±ÂèØ‰ª•‰ΩøÁî® È´òÂ∫¶ Êù•ÊèèËø∞ Âå∫Âùó‰∏éÈ¶ñÂå∫Âùó‰πãÈó¥ÁöÑË∑ùÁ¶ª„ÄÇ ÂØπÊØè‰∏™Âå∫ÂùóÂ§¥ËøõË°åSHA256Âä†ÂØÜÂìàÂ∏åÔºåÂèØÁîüÊàê‰∏Ä‰∏™ÂìàÂ∏åÂÄº„ÄÇÈÄöËøáËøô‰∏™ÂìàÂ∏åÂÄºÔºåÂèØ‰ª•ËØÜÂà´Âá∫Âå∫ÂùóÈìæ‰∏≠ÁöÑÂØπÂ∫îÂå∫Âùó„ÄÇÂêåÊó∂ÔºåÊØè‰∏Ä‰∏™Âå∫ÂùóÈÉΩÂèØ‰ª•ÈÄöËøáÂÖ∂Âå∫ÂùóÂ§¥ÁöÑ‚ÄúÁà∂Âå∫ÂùóÂìàÂ∏åÂÄº‚ÄùÂ≠óÊÆµÂºïÁî®Ââç‰∏ÄÂå∫ÂùóÔºàÁà∂Âå∫ÂùóÔºâ„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÊØè‰∏™Âå∫ÂùóÂ§¥ÈÉΩÂåÖÂê´ÂÆÉÁöÑÁà∂Âå∫ÂùóÂìàÂ∏åÂÄº„ÄÇËøôÊ†∑ÊääÊØè‰∏™Âå∫ÂùóÈìæÊé•Âà∞ÂêÑËá™Áà∂Âå∫ÂùóÁöÑÂìàÂ∏åÂÄºÂ∫èÂàóÂ∞±ÂàõÂª∫‰∫Ü‰∏ÄÊù°‰∏ÄÁõ¥ÂèØ‰ª•ËøΩÊ∫ØÂà∞Á¨¨‰∏Ä‰∏™Âå∫ÂùóÔºàÂàõ‰∏ñÂå∫ÂùóÔºâÁöÑÈìæÊù°„ÄÇÂºïÁî® Âå∫ÂùóÂ§¥hashÂÄºÊòØÁî±Êú∫Âô®ÁÆóÂá∫Êù•ÁöÑÔºåÂπ∂ÈùûÂåÖÂê´Âú®Âå∫ÂùóÂ§¥ÂÜÖ„ÄÇÂå∫ÂùóÂ§¥ÂÜÖÂè™‰øùÂ≠ò‰∫Ü‰∏§‰∏™hashÂÄº: Áà∂Âå∫ÂùóhashÂÄº‰∏é MerkleÊ†ëÊ†πÁà∂Âå∫ÂùóhashÂÄºÁî®‰∫éÊåáÂêëÁà∂Âå∫Âùó„ÄÇMerkleÊ†ëÊ†πÁî®Êà∑ÊåáÂêëÂå∫Âùó‰ΩìÊï∞ÊçÆ MerkleÊ†ëÁÆÄÂçïÊù•ËÆ≤ÔºåMerkleÊ†ëÊ†πÁöÑ‰ΩúÁî®Â∞±ÊòØ‰øùËØÅÂå∫Âùó‰∏≠ÁöÑËÆ∞ÂΩï‰∏çË¢´ÁØ°Êîπ„ÄÇ‰∏ÄÊó¶Âå∫Âùó‰∏≠ÁöÑÊüê‰∏Ä‰∏™ËÆ∞ÂΩïË¢´ÁØ°ÊîπÔºåÈÇ£‰πàÈÄöËøáÊØîËæÉMerkleÊ†ëÊ†πÂ∞±ÂèØ‰ª•Ê£ÄÊµãÂà∞„ÄÇ ËØ∑Ê≥®ÊÑèÔºåÂå∫ÂùóÂìàÂ∏åÂÄºÂÆûÈôÖ‰∏äÂπ∂‰∏çÂåÖÂê´Âú®Âå∫ÂùóÁöÑÊï∞ÊçÆÁªìÊûÑÈáåÔºå‰∏çÁÆ°ÊòØËØ•Âå∫ÂùóÂú®ÁΩëÁªú‰∏ä‰º†ËæìÊó∂ÔºåÊäëÊàñÊòØÂÆÉ‰Ωú‰∏∫Âå∫ÂùóÈìæÁöÑ‰∏ÄÈÉ®ÂàÜË¢´Â≠òÂÇ®Âú®ÊüêËäÇÁÇπÁöÑÊ∞∏‰πÖÊÄßÂ≠òÂÇ®ËÆæÂ§á‰∏äÊó∂„ÄÇÁõ∏ÂèçÔºåÂå∫ÂùóÂìàÂ∏åÂÄºÊòØÂΩìËØ•Âå∫Âùó‰ªéÁΩëÁªúË¢´Êé•Êî∂Êó∂Áî±ÊØè‰∏™ËäÇÁÇπËÆ°ÁÆóÂá∫Êù•ÁöÑ Âå∫ÂùóÈìæÂ¶Ç‰ΩïÈÅçÂéÜÊàë‰ª¨Áü•ÈÅì ÊØè‰∏Ä‰∏™Âå∫ÂùóÁöÑÁöÑÂ§¥ÈÉ®ÈÉΩÂ≠òÊîæ‰∫Ü‰∏ä‰∏ÄÂùóÁöÑhashÂÄº„ÄÇÈÇ£‰πàÊàë‰ª¨‰∏ãËΩΩ‰∏ãÊù•ÂΩìÂâçÊâÄÊúâÊï∞ÊçÆ‰πãÂêéËØ•ÊÄé‰πàÈÅçÂéÜÂë¢Ôºü‰∏çËøáÊàë‰ª¨Ë¶ÅÊ∏ÖÊ•ö‰∏ÄÁÇπÔºåÈÅçÂéÜÊìç‰ΩúÊú¨Ë∫´ÂèñÂÜ≥‰∫é‰Ω†Êú¨Âú∞Â≠òÂÇ®Âå∫ÂùóÊâÄÈááÁî®ÁöÑÊï∞ÊçÆÁªìÊûÑ„ÄÇÂ¶ÇÊûúÊòØÊï∞ÁªÑÂ≠òÊîæÔºåÈÇ£‰πàÈÅçÂéÜÂ∞±ÊòØÁÆÄÂçïÊù•ËØ¥forÂæ™ÁéØ‰∏ÄÈÅç„ÄÇÈÇ£‰πàÂå∫ÂùóÂ§¥ÈÉ®ÁöÑhashÂÄºÊúâ‰ªÄ‰πàÁî®Âë¢ÔºüÈÇ£Â∞±ÊòØÂΩì‰Ω†Âèñ‰∏ã‰∏ÄÂùóÊï∞ÊçÆÁöÑÊó∂ÂÄôÂèØ‰ª•È™åËØÅ‰∏ã‰∏ÄÂùóÁ©∂Á´üÊòØ‰∏çÊòØ‰∏ä‰∏ÄÂùóÁöÑÂêéÁªßÂå∫Âùó„ÄÇÂ∞ÜÂΩìÂâçÂå∫ÂùóËá™Ë∫´Â§¥ÈÉ®ÁöÑHash‰∏é‰∏ã‰∏Ä‰∏™Âå∫ÂùóÂ≠òÊîæÁöÑÂâçÈ©±Âå∫ÂùóÁöÑhashÂÅöÊØîËæÉÔºåÂ∞±ËÉΩÁü•ÈÅìËøô‰∏§‰∏™Âå∫ÂùóÊòØ‰∏çÊòØ‰∏Ä‰∏≤„ÄÇ Âå∫ÂùóÁöÑhashÊúâ‰ªÄ‰πàÁî®Âå∫ÂùóÁöÑhashÁöÑÁîüÊàêÊòØ ÂèÇËÄÉËµÑÊñôÁ≤æÈÄöÊØîÁâπÂ∏Å]]></content>
      <categories>
        <category>Â≠¶‰π†Á¨îËÆ∞</category>
        <category>Âå∫ÂùóÈìæ</category>
      </categories>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>ÊØîÁâπÂ∏Å</tag>
        <tag>Âå∫ÂùóÈìæ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰ΩøÁî®RSS]]></title>
    <url>%2F2018%2F04%2F22%2F%E4%BD%BF%E7%94%A8RSS%2F</url>
    <content type="text"><![CDATA[Âç†Âùë]]></content>
      <categories>
        <category>HexoÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>ÊïôÁ®ã</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰ΩøÁî®HexoEditorÂÜôÂçöÂÆ¢]]></title>
    <url>%2F2018%2F04%2F21%2FhexoEditorTest%2F</url>
    <content type="text"><![CDATA[ÊëòË¶ÅÔºö ÂìéÊÄé‰πàËØ¥Âë¢ÔºåHexoEditor Â§™Â•ΩÁî®‰∫ÜÔºåÂ§ßÂ§ßÊñπ‰æø‰∫ÜÂÜôÂçöÂÆ¢ÁöÑÊïàÁéá„ÄÇÊØîÂ¶Ç‰∏ÄÈîÆ‰∏ä‰º†ÂçöÂÆ¢ÂõæÁâáÔºåËÆ©ÊàëÁúÅÁöÑÊâìÂºÄËÖæËÆØ‰∫ëÂÆ¢Êà∑Á´ØÊâãÂä®‰∏ä‰º†ÂíåÂª∫Á´ãÊñá‰ª∂Â§π‰∫Ü„ÄÇÁé∞Âú®ÊàëË¶ÅËÆ∞ÂΩï‰ª•‰∏ãÊó•Â∏∏‰ΩøÁî®‰∏≠‰∏Ä‰∫õÁªèÂ∏∏Áî®Âà∞ÁöÑÊìç‰Ωú„ÄÇ ÂºÄÂßãÂú®windows‰∏ã ‰ª•ÁÆ°ÁêÜÂëòË∫´‰ªΩËøêË°å‰ª•‰∏ãÂëΩ‰ª§123456789npm config delete proxynpm config set prefix "C:/Program Files/nodejs/npm_global"npm config set cache "C:/Program Files/nodejs/npm_cache" npm config set registry "https://registry.npm.taobao.org/"npm config set electron_mirror "https://npm.taobao.org/mirrors/electron/"git clone https://github.com/zhuzhuyule/HexoEditor.gitnpm install -g electron@1.8.1npm installnpm start ‰ΩøÁî®ÂØºÂÖ•ÈÖçÁΩÆÁªëÂÆöÂõæÂ∫ä]]></content>
      <categories>
        <category>HexoÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>ÊïôÁ®ã</tag>
        <tag>HexoEditor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+yilia‰∏™ÊÄßÂåñ‰πã-Ê∑ªÂä†ËÉåÊôØÈü≥‰πê]]></title>
    <url>%2F2018%2F04%2F21%2Fhexo-yilia%E4%B8%AA%E6%80%A7%E5%8C%96%E4%B9%8B-%E6%B7%BB%E5%8A%A0%E8%83%8C%E6%99%AF%E9%9F%B3%E4%B9%90%2F</url>
    <content type="text"><![CDATA[Ê∑ªÂä†ËÉåÊôØÈü≥‰πêËé∑ÂèñÂ§ñÈìæÊâìÂºÄÁΩëÊòì‰∫ëÈü≥‰πê ÁÇπÂáªÂõæ‰∏≠ÊåâÈíÆË∑≥ËΩ¨Âà∞Â§ñÈìæÂàõÂª∫È°µÈù¢„ÄÇ ÂæàÂ§öÂ•ΩÂê¨ÁöÑÈü≥‰πêÈÉΩÂõ†‰∏∫ÁâàÊùÉÂéüÂõ†Êó†Ê≥ïÂàõÂª∫Â§ñÈìæÔºåÊâÄ‰ª•Âè™ËÉΩÂ§öÊç¢Âá†‰∏™ËØïËØï„ÄÇ ËØ¥ÊòéÔºö ÂèØ‰ª•Ë∞ÉÊï¥Êéß‰ª∂ÁöÑÈïøÂ∫¶„ÄÇ‰∏çËøáÂàùÂßãÈïøÂ∫¶Áõ∏ÂØπËæÉÈïøÔºåÁõ¥Êé•Ê∑ªÂä†‰ºöÁúãËµ∑Êù•‰∏çÂ±Ö‰∏≠ÂΩ±ÂìçÁæéËßÇÔºåÂêéÈù¢ÂÜçË∞É Ëá™Âä®Êí≠ÊîæÂ±ûÊÄßÊ†πÊçÆËá™Â∑±ÁöÑ‰π†ÊÉØËÆæÁΩÆ„ÄÇ ‰πãÂêé‰πüÂèØ‰ª•ÈÄöËøáÁõ¥Êé•‰øÆÊîπÁÆ≠Â§¥ÊâÄÊåáÁöÑÊï∞ÂÄºÊù•ÊîπÂèòÂ§ßÂ∞è ‰πãÂêé‰πüÂèØ‰ª•ÈÄöËøá‰øÆÊîπ 0Êàñ1 Êù•ÂÖ≥Èó≠ÊàñÊâìÂºÄËá™Âä®Êí≠Êîæ Á≤òË¥¥‰ª£Á†ÅÊâìÂºÄË∑ØÂæÑ themes/yilia/layout/_partial/left-col.ejs ÊâæÂà∞Â¶ÇÂõæÊâÄÁ§∫‰ΩçÁΩÆÔºåÂ∞ÜËé∑ÂèñÁöÑÂ§ñÈìæÁ≤òË¥¥‰∏äÂéª„ÄÇ ÂΩìÁÑ∂‰∏ç‰∏ÄÂÆöÈùûÂæóÊòØËøô‰∏™Âú∞ÊñπÔºåÊ†πÊçÆÈúÄË¶ÅÂèØ‰ª•ÈÄâÊã©‰∏çÂêåÁöÑÂÆâÊîæ‰ΩçÁΩÆ Ë∞ÉÊï¥Â§ßÂ∞èÁ©∂Á´üË¶ÅË∞ÉÊï¥Âà∞Â§öÈïøÊâçËÉΩÁúãËµ∑Êù•Â±Ö‰∏≠?ÊàëÂ∞ùËØï‰∫ÜÂæàÂ§öÊ¨°ÈïøÂ∫¶ÔºåÈÉΩ‰∏çÂ§™Êª°ÊÑè„ÄÇÂêéÊù•ÊàëÂ∞±ÊÉ≥Âà∞‰∏Ä‰∏™ÂèñÂ∑ßÁöÑÂäûÊ≥ï„ÄÇÂ¶ÇÂõæÊâÄÁ§∫Ôºö Áî±Ê≠§Á°ÆÂÆöÊàë‰ª¨ÊúÄÂ•ΩÂ∞ÜÊéß‰ª∂ÈïøÂ∫¶‰øÆÊîπ‰∏∫ 228px ÂÆåÊàêÂ±Ö‰∏≠ÊïàÊûúÁúãËµ∑Êù•ËøòÊòØ‰∏çÈîôÁöÑ„ÄÇ Âè¶Ôºö ÂΩìÁî®ÊâãÊú∫Á´ØÊü•ÁúãÁöÑÊó∂ÂÄôÊéß‰ª∂‰ºöÊ∂àÂ§±ÔºåÂõ†Ê≠§‰∏çËÉΩÊéßÂà∂ÊöÇÂÅú„ÄÇËØ∑Ê≥®ÊÑèËøô‰∏ÄÁÇπ„ÄÇ ipadÂ∏ÉÂ±ÄÊ≤°ÊúâÊµãËØïÔºåÂõ†‰∏∫ÊàëÊ≤°Êúâipad‚Ä¶. ( Ôø£„Å∏Ôø£) ÊØèËøõÂÖ•‰∏Ä‰∏™Êñ∞ÁöÑÈ°µÈù¢Èü≥‰πêÈÉΩ‰ºöÈáçÊñ∞ÂºÄÂßãÊí≠ÊîæÔºåÊàëËøò‰∏çÁü•ÈÅìÊÄé‰πàÂäû„ÄÇ]]></content>
      <categories>
        <category>HexoÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>ÊïôÁ®ã</tag>
        <tag>hexo‰∏™ÊÄßÂåñ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[httpsÂ≠¶‰π†Á¨îËÆ∞]]></title>
    <url>%2F2018%2F04%2F21%2Fhttps%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Èùû‰∏ì‰∏öËß£ÈáäÔºå‰∏ªË¶Å‰æø‰∫éÁêÜËß£ HTTPSÂçèËÆÆÁöÑÁÆÄÂçïÁêÜËß£ÊµÅÁ®ãÊèèËø∞HTTPSÁöÑÈÄö‰ø°ÊµÅÁ®ãÂèØ‰ª•ÁÆÄÂçïÊèèËø∞‰∏Ä‰∏ã ÂèØ‰ª•ËÆ§‰∏∫ÊúçÂä°Âô®Êúâ‰∏§ÊääÈí•ÂåôÔºå‰∏ÄÊääÂè´ÂÖ¨Èí• ‰∏ÄÊääÂè´ÁßÅÈí•ÔºàÊúçÂä°Âô®Ôºâ„ÄÇ‚ÄúÂÖ¨Èí•‚ÄùÁöÑ‰ΩúÁî®Âè™Êúâ‰∏Ä‰∏™ÔºöËÆ©ÂÆ¢Êà∑Á´ØÂØπËá™Â∑±ÁöÑ‰ø°ÊÅØÂä†ÂØÜ„ÄÇ ÂÆ¢Êà∑Á´ØÂèëÈÄÅËØ∑Ê±Ç„ÄÇ ÊúçÂä°Âô®ËøîÂõûËØÅ‰π¶„ÄÇ ÂÆ¢Êà∑Á´ØÂØπËØÅ‰π¶ËøõË°åÈ™åËØÅ„ÄÇÔºàÈò≤Ê≠¢ÂÜíÂÖÖÊúçÂä°Âô®Ôºâ ÂÆ¢Êà∑Á´ØÈ™åËØÅÊàêÂäüÂàôËøõË°åÁ¨¨5Ê≠•Âê¶ÂàôË≠¶ÂëäÁî®Êà∑ ÂÆ¢Êà∑Á´Ø‰ºö‰∫ßÁîü‰∏Ä‰∏™ÈöèÊú∫Êï∞Âπ∂ÁªìÂêàÊüêÁßçÁÆóÊ≥ïÁîüÊàê‰∏Ä‰∏™‰∏¥Êó∂ÁßÅÈí• ÂÆ¢Êà∑Á´Ø‰ΩøÁî®ËØÅ‰π¶ÈáåÈù¢ÁöÑÂÖ¨Èí•Êää‰∏¥Êó∂ÁßÅÈí•Âä†ÂØÜÂπ∂‰º†ËæìÁªôÊúçÂä°Âô® ÊúçÂä°Âô®‰ΩøÁî®ÁßÅÈí•Ëß£ÂØÜÂÆ¢Êà∑Á´ØÊ∂àÊÅØËé∑Âèñ‰∏¥Êó∂ÁßÅÈí• ÂÆ¢Êà∑Á´Ø‰ΩøÁî®‰∏¥Êó∂ÁßÅÈí•Âä†ÂØÜÂπ∂ÂèëÈÄÅÊïèÊÑü‰ø°ÊÅØÔºàÊØîÂ¶ÇÂØÜÁ†ÅÔºâ ÊúçÂä°Âô®‰ΩøÁî®‰∏¥Êó∂ÁßÅÈí•Ëß£ÂØÜÂπ∂Ëé∑ÂèñÊïèÊÑü‰ø°ÊÅØ Âú®‰∏äËø∞ËøáÁ®ã‰∏≠ÔºåÊâÄÊúâÁöÑ‰ø°ÊÅØÈÉΩÊòØÂä†ÂØÜ‰º†ËæìÁöÑ„ÄÇÂç≥‰ΩøÊäìÂåÖ‰πüÂè™ÊòØ‰∏ÄÂ†ÜÁúã‰∏çÊáÇÁöÑÂØÜÊñáÔºöÊ≤°ÊúâÊãøÂà∞ÊúçÂä°Âô®ÁöÑÁßÅÈí•ÈÇ£Â∞±Ê≤°ÂäûÊ≥ïÊü•ÁúãÂÆ¢Êà∑Á´Ø‰∏éÊúçÂä°Âô®ÁöÑ‰∏¥Êó∂ÁßÅÈí•ÔºåÂõ†‰∏∫‰∏¥Êó∂ÁßÅÈí•ÊòØÁî®ÂÖ¨Èí•Âä†ÂØÜËøáÁöÑÔºåÂè™ËÉΩÁî®ÊúçÂä°Âô®ÁöÑÁßÅÈí•Ëß£ÂØÜ„ÄÇÂõ†Ê≠§‰∏≠Èó¥ËøáÁ®ã‰∏çÂèØËÉΩ‰∏ãÂæó‰∫ÜÊâã„ÄÇ ÂÆ¢Êà∑Á´ØÂ∞ÜËøô‰∏™ÈöèÊú∫ÁîüÊàêÁöÑ‰∏¥Êó∂ÁßÅÈí•Áî®ÊúçÂä°Âô®ÁöÑÂÖ¨Èí•Âä†ÂØÜÂèëÁªôÊúçÂä°Âô®ÔºåÊúçÂä°Âô®Áî®Ëá™Â∑±ÁöÑÁßÅÈí•Ëß£ÂØÜÔºåËøôÊó∂ÂÄôÊúçÂä°Âô®‰πüÂ∞±Ëé∑Âèñ‰∫Ü‰∏¥Êó∂ÁßÅÈí•„ÄÇ Êé•‰∏ãÊù•ÁöÑÂØπËØùÂ∞±‰∏çÊòØÈùûÂØπÁß∞Âä†ÂØÜÁöÑÊñπÂºè‰∫ÜÔºåËÄåÊòØ‰ΩøÁî®ÁöÑÂØπÁß∞Âä†ÂØÜÔºåÊúçÂä°Âô®Áî®‰∏¥Êó∂ÁßÅÈí•Âä†ÂØÜ‰ø°ÊÅØÂèëÁªôÂÆ¢Êà∑Á´ØÔºåÂÆ¢Êà∑Á´Ø‰πüÁî®‰∏¥Êó∂ÁßÅÈí•Êää‰ø°ÊÅØËß£ÂØÜ„ÄÇ ‰∏Ä‰∫õHTTPsÁöÑÁü•ËØÜÁÇπ SSL/TLSÂçèËÆÆ ÈùûÂØπÁß∞Âä†ÂØÜ ÂØπÁß∞Âä†ÂØÜ ÂçïÂêëÊï£ÂàóÂáΩÊï∞ ‰∏≠Èó¥‰∫∫ÊîªÂáª ÂüüÂêçÂä´ÊåÅ Á≠æÂêç ËØÅ‰π¶ ÂÖ≥‰∫éHTTPsÁöÑÂ•ΩÂ§ÑÁî®‰∫ÜÂÆÉÂêéÂè∞Â∞±ÂèØ‰ª•‰∏çÂøÖÁÖûË¥πËã¶ÂøÉËøõË°å‰º†Ëæì‰ø°ÊÅØÁöÑÂä†ÂØÜ‰∫ÜÔºåÁúÅÂä≤ÂÑø„ÄÇ ÂÖ≥‰∫éÊ†πËØÅ‰π¶È¶ñÂÖàÔºå‰∏ÄËà¨Á≥ªÁªü‰ºöÈ¢ÑÁΩÆÂæàÂ§öÊ†πËØÅ‰π¶ÔºåÂÆÉ‰ª¨ÈÉΩÊòØÊù•Ëá™Âèó‰ø°‰ªªÁöÑÊ†πËØÅ‰π¶È¢ÅÂèëÊú∫ÊûÑÔºåÂπ∂‰∏îÂÖ®ÁêÉ‰πüÂ∞±ÈÇ£‰πàÂá†ÊâÄÊú∫ÊûÑËÄåÂ∑≤„ÄÇÂÖ∂‰ªñÁΩëÁ´ôÂ¶ÇÊûúÈúÄË¶Å‰ΩøÁî®HTTPsÔºåÈÇ£‰πàÂè™ËÉΩÂêëËøô‰∫õÊú∫ÊûÑË¥≠‰π∞ËØÅ‰π¶ÔºåËØÅ‰π¶ÈÉΩÊúâ‰∏Ä‰∏™ÁîüÊïàÊúü‰ª•ÂèäÂ§±ÊïàÊúü„ÄÇ ÊúâÊó∂ÂÄô‰ΩøÁî®Êú∫ÊàøÁöÑÁîµËÑë‰∏äÁΩëÊÄª‰ºöÊèêÁ§∫ËØÅ‰π¶Ë≠¶ÂëäÔºåÂéüÂõ†ÂèØËÉΩÂ∞±ÊòØÊú∫ÊàøÁîµËÑëÁöÑÁ≥ªÁªüÊó∂Èó¥‰∏çÂØπÔºåË¶Å‰πàËøòÊ≤°Âà∞ÁîüÊïàÊúüË¶Å‰πàÂ∞±ÊòØËøá‰∫ÜÂ§±ÊïàÊúü„ÄÇÊ≤°ÊúâËØ•CAÊú∫ÊûÑÁöÑÊ†πËØÅ‰π¶ÔºåÈÇ£‰πà‰ªª‰ΩïÁî±Ëøô‰∏™Êú∫ÊûÑÈ¢ÅÂèëÁöÑËØÅ‰π¶ÈÉΩ‰ºöÊó†Ê≥ïÈÄöËøáËØÅ‰π¶ÊòØÂèØ‰ª•Ëá™Â∑±ÁîüÊàêÔºå‰ΩÜÊòØÂá†‰πéÊâÄÊúâÁöÑÂÆ¢Êà∑Á´ØÈÉΩ‰∏ç‰ø°‰ªª‰Ω†ÔºåÊÉ≥ËÆ©ÂÆ¢Êà∑Á´Ø‰ø°‰ªªÁöÑËØùÂè™ËÉΩÊâãÂä®ÂÆâË£ÖÊ†πËØÅ‰π¶ÊâçË°å ËØÅ‰π¶ËÉΩ‰∏çËÉΩ‰º™ÈÄ†ÂÖ≥‰∫éËØÅ‰π¶ËÉΩ‰∏çËÉΩ‰º™ÈÄ†ÁöÑÈóÆÈ¢òÂ∑≤ÁªèÁî±‰∏Ä‰∏™ÂæàÂêàÁêÜÁöÑÊèèËø∞: Êï∞Â≠óËØÅ‰π¶ÈáåÊúâCAÁöÑÊï∞Â≠óÁ≠æÂêçÔºåÁ≠æÂêçÊòØÁî±ËØÅ‰π¶ÂÜÖÂÆπÁöÑÂìàÂ∏åÊëòË¶ÅÁî®CAÁöÑÁßÅÈí•Âä†ÂØÜÁöÑ„ÄÇÁî®CAÁöÑÂÖ¨Èí•È™åËØÅÁ≠æÂêçÁöÑÂêàÊ≥ïÊÄßÂ∞±ÂèØ‰ª•È™åËØÅËØÅ‰π¶ÁöÑÁúüÂÅá ÂèÇËÄÉËµÑÊñô 12306.cn Ë¥≠Á•®‰∏∫‰ªÄ‰πàË¶ÅÂÆâË£ÖÊ†πËØÅ‰π¶Ôºü RSAÁöÑÂÖ¨Èí•ÂíåÁßÅÈí•Âà∞Â∫ïÂì™‰∏™ÊâçÊòØÁî®Êù•Âä†ÂØÜÂíåÂì™‰∏™Áî®Êù•Ëß£ÂØÜÔºü ÁßÅÈí•„ÄÅÂÖ¨Èí•„ÄÅÊï∞Â≠óÁ≠æÂêç„ÄÅÊï∞Â≠óËØÅ‰π¶„ÄÅHTTPS HTTPSÂøÖÈ°ªÂú®ÊØèÊ¨°ËØ∑Ê±Ç‰∏≠ÈÉΩË¶ÅÂÖàÂú®SSLÂ±ÇËøõË°åÊè°Êâã‰º†ÈÄíÁßòÈí•ÂêóÔºü ËÅäËÅäHTTPSÂíåSSL/TLSÂçèËÆÆ HTTPS ÂéüÁêÜËØ¶Ëß£ ÊµèËßàÂô®Â¶Ç‰ΩïÈ™åËØÅHTTPSËØÅ‰π¶ÁöÑÂêàÊ≥ïÊÄßÔºü HTTPS ÊúçÂä°Âô®ÂíåÂÆ¢Êà∑Á´ØÂ¶Ç‰ΩïËøõË°åÂä†ÂØÜËß£ÂØÜÁöÑ? ÂÖ¨Èí•‰∏éÁßÅÈí•ÔºåHTTPSËØ¶Ëß£ (Êé®ËçêÁúãÁúã) ÂõΩÂÜÖCAÊú∫ÊûÑÊ≤ÉÈÄöÈîôËØØÈ¢ÅÂèëGitHubÂüüÂêçSSLËØÅ‰π¶]]></content>
      <categories>
        <category>Â≠¶‰π†Á¨îËÆ∞</category>
      </categories>
      <tags>
        <tag>HTTPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JWTÂ≠¶‰π†Á¨îËÆ∞]]></title>
    <url>%2F2018%2F04%2F21%2FJWT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[‰∏∫‰∏Ä‰∏™Ëá™Â®±Ëá™‰πêÁöÑÈ°πÁõÆÂºÄÂèëWebAPIÁöÑÊó∂ÂÄôÔºåÊàëÂ∞ùËØï‰ΩøÁî®‰∫ÜTokenÈ™åËØÅÊú∫Âà∂„ÄÇÂØπ‰∫éÊàëËøôÊ†∑‰∏Ä‰∏™ËèúÈ∏üÊù•ËØ¥ÔºåTokenÁöÑÈ™åËØÅÊú∫Âà∂Âú®ÊàëÁêÜËß£‰∏≠ÂçÅÂàÜÁÆÄÂçïÔºöÂÖà‰ΩøÁî®Áî®Êà∑ÂêçÂíåÂØÜÁ†ÅÁôªÂΩïÁ≥ªÁªüÔºåÊúçÂä°Âô®‰ºöËøîÂõû‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÔºåÁ∫¶ÂÆöÂ•ΩËøô‰∏™Â≠óÁ¨¶‰∏≤Â∞±ÊòØÁ≥ªÁªüÂá∫ÂÖ•ÁöÑÈÄöË°å‰ª§ÁâåÂÆ¢Êà∑Á´Ø‰πãÂêéÈô§‰∫ÜÁôªÂΩïÊìç‰ΩúÁöÑÊâÄÊúâËØ∑Ê±ÇÂè™Ë¶ÅÂ∏¶‰∏äËøô‰∏™Â≠óÁ¨¶‰∏≤ÔºåÁ≥ªÁªüÂ∞±‰ºöÊîæË°å„ÄÇÂ•ΩÔºåÂà∞Áé∞Âú®Â∞±ÊòéÁôΩ‰∫ÜÔºå‚ÄúToken‚ÄùÂú®ÂΩìÊó∂ÁöÑÊàëÁúãÊù•Âè™ÊòØ‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÔºåÊàëÂè™ÈúÄË¶Å‰øùËØÅËøô‰∏™Â≠óÁ¨¶‰∏≤‰∏Ä‰∫∫‰∏Ä‰∏™‰∏çÈáçÊ†∑Âç≥ÂèØ„ÄÇ‰∫éÊòØÊàëÂ∞±‰ΩøÁî®‰∫ÜGUIDÔºàÂÖ®Â±ÄÂèòÈáèÊ†áËØÜÁ¨¶ÔºâÊù•ÂÖÖÂΩìToken„ÄÇ‰ΩÜÊòØ‰ΩøÁî®ËøáÁ®ã‰∏≠Â∞±Êúâ‰∫ÜÈóÆÈ¢ò„ÄÇËøô‰∏™TokenÊòØÁîüÊàê‰∫ÜËÄå‰∏î‰∏çÈáçÊ†∑ÔºåÂèØÊàëÊÄé‰πàÁ°ÆÁ´ãÂÆÉÂíåÁî®Êà∑‰πãÈó¥ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÔºü‰∫éÊòØÊàëÊÉ≥Âà∞ÊääTokenÂ≠òÂà∞Áî®Êà∑Ë°®‰∏≠ÔºåÂÅáËÆæÁî®Êà∑Â∏¶ÁùÄTokenËÆøÈóÆÊõ¥Êñ∞Áî®Êà∑‰ø°ÊÅØÁöÑAPIÔºåÊúçÂä°Âô®ÊãøÂà∞TokenÂéªÊü•ËØ¢Áî®Êà∑Ë°®ÔºåËøõËÄåÊâæÂà∞ÂØπÂ∫îÁöÑÁî®Êà∑Âêç„ÄÇËá≥Ê≠§ÊàëÂ∞±Áü•ÈÅì‰∫ÜËøô‰∏™TokenÊòØÂì™‰ΩçÁî®Êà∑ÁöÑ‰∫Ü„ÄÇÊãøÁùÄËøô‰∏™Áî®Êà∑ÂêçÔºåÂ∞±ÂèØ‰ª•Êõ¥Êñ∞ËØ•Áî®Êà∑ÁöÑË°®È°π‰∫Ü„ÄÇÈÇ£‰πàÂÖ∂ÂÆÉÁöÑAPI‰πüÊòØËøô‰∏™ËøôÊ†∑ÔºåÈ¶ñÂÖàÁ¨¨‰∏Ä‰ª∂‰∫ãË¶ÅÂÅöÁöÑÂ∞±ÊòØÂéªÊü•ËØ¢Êï∞ÊçÆÂ∫ìÊâæÂà∞ÂØπÂ∫îÁöÑÁî®Êà∑ÂêçÔºåÊâçËÉΩËøõË°åÂÖ∂‰ªñÁöÑÊìç‰Ωú„ÄÇÊåâÁÖßÊàëÁöÑÊÄùË∑ØÁöÑËØùÔºåËøô‰∏™ÊñπÂºèËôΩÁÑ∂È∫ªÁÉ¶‰∏ÄÁÇπÔºå‰ΩÜÊòØËøòÊòØËÉΩÂÆûÁé∞ÂäüËÉΩÁöÑ„ÄÇÂè¶‰∏Ä‰∏™ËßÇ‰∫éTokenÁöÑÂ∏∏ËßÅË¶ÅÊ±ÇÊòØÔºåÂÆÉÂæóËÉΩËÆæÁΩÆÊúâÊïàÊúüÔºåÊÄª‰∏çËÉΩ‰∏Ä‰∏™TokenÁî®‰∏ÄËæàÂ≠êÂêß„ÄÇ‰∫éÊòØÊàëÂèàÂæÄÁî®Êà∑Ë°®Âä†‰∫Ü‰∏ÄÂàóËøáÊúüÊó∂Èó¥ÔºåÊØèÊ¨°È™åËØÅTokenÈÉΩË¶Å‰ªéÊï∞ÊçÆÂ∫ì‰∏≠ÊãøÂá∫Âà∞ÊúüÊó∂Èó¥‰∏éÂΩìÂâçÊó∂Èó¥ÂÅöÊØîËæÉÊù•Âà§Êñ≠ÊòØ‰∏çÊòØËøáÊúü‰∫Ü„ÄÇ JWTÁöÑÂü∫Êú¨Ê¶ÇÂøµJWTÔºàJson Web TokenÔºâÊú¨Ë¥®‰∏äÊòØ‰∏ÄÁßçTokenÁöÑËÆæËÆ°ËßÑËåÉ„ÄÇÁî®‰∫éÂÆûÁé∞TokenÊú∫Âà∂ÁöÑTokenËØ¥Âà∞Â∫ï‰πüÂ∞±ÊòØ‰∏™Â≠óÁ¨¶‰∏≤ÔºåÈáçÁÇπÊòØËøô‰∏™Â≠óÁ¨¶‰∏≤ËØ•ÊÄé‰πàÂÜôÊâç‰ºöÊØîËæÉÂêàÁêÜ„ÄÇÊàëËØïËøáÁî®GUIDÊù•ÂÖÖÂΩìËøô‰∏™TokenÂÆûÁé∞ÁÆÄÂçïÁöÑTokenË∫´‰ªΩÈ™åËØÅÊú∫Âà∂ÔºåÂÆåÂÖ®Ê≤°ÈóÆÈ¢ò„ÄÇÂèØÂ∞±ÊòØÁî®Ëµ∑Êù•ÊúâÁÇπÂà´Êâ≠„ÄÇËÄåJWTÂ∞±ÊòØ‰∏ÄÁßçÊõ¥Âä†ÂêàÁêÜÁöÑÁªÑÁªáTokenÂ≠óÁ¨¶‰∏≤ÁöÑÊñπÂºè„ÄÇÂÆÉ‰ΩøÁî®JSONÊ†ºÂºèÔºåÊÄª‰ΩìÂàÜ‰∏∫‰∏â‰∏™Â§ßÂùóÔºöHeader .Payload. Signature,ÂàÜÂà´ÊâøÊãÖÂêÑËá™ÁöÑË¥£‰ªª„ÄÇ Header 1234&#123; "alg": "HS256", "typ": "JWT"&#125; Payload 12345&#123; "sub": "1234567890", "name": "John Doe", "admin": true&#125; Signature 1234HMACSHA256( base64UrlEncode(header) + "." + base64UrlEncode(payload), secret) Êï¥‰ΩìÁªìÊûÑ‰∏∫Ôºöheader (base64)+payload (base64)+SignatureÊúâÂÖ≥ÈÇ£‰∫õClaimÁöÑËØ¥ÊòéÂÆòÁΩë‰∏äÊúâËØ¶ÁªÜ‰ªãÁªç„ÄÇ Êé•‰∏ãÊù•‰ªéSignatureÂá∫ÂèëÁúãÁúãÂèØ‰ª•ÊåñÂà∞‰ªÄ‰πàÁü•ËØÜ„ÄÇ‚Üì ÂÖ≥‰∫éSignatureÂâç‰∏§ÈÉ®ÂàÜÁöÑÂ§ÑÁêÜÈÉΩÂçÅÂàÜÁÆÄÂçïÔºåÂÖ≥‰∫éSignatureÁöÑÂ§ÑÁêÜÊúâÁÇπËÆ©‰∫∫ÁúºËä±Áº≠‰π±ÔºåÁâπÂà´ÊòØÊàëËøôÁßçÂØπÂØÜÁ†ÅÊäÄÊúØ‰∏ÄÊó†ÊâÄÁü•ÁöÑÂ∞èÁôΩ„ÄÇÂÖ≥‰∫éËøô‰∏™ÊúÄÂêé‰∏ÄÈÉ®ÂàÜÊàëÊúâËøá‰∏ÄÁÇπËØØËß£Ôºö‰ª•‰∏∫‰ΩøÁî®SHA-256ËøõË°åÂ§ÑÁêÜ‰ª•ÂêéÂ∞±ÂèØ‰ª•ÁúãÂÅöÊòØÂØπJWTÂä†ÂØÜ‰∫ÜÂà´‰∫∫Â∞±Áúã‰∏çÂà∞‰∫Ü„ÄÇÂÖ∂ÂÆûÂπ∂‰∏çÊòØËøôÊ†∑ÁöÑ„ÄÇÈ¶ñÂÖàË¶ÅÂ£∞ÊòéÔºåÊúÄÂêé‰∏ÄÈÉ®ÂàÜ‚ÄúÁ≠æÂêç‚ÄùÂπ∂‰∏çÊòØÂØπTokenËøõË°åÂä†ÂØÜÂ§ÑÁêÜ„ÄÇËÄåHS256‰πü‰∏çÊòØ‰∏ÄËà¨ÊÑè‰πâ‰∏äÁöÑÂä†ÂØÜÂáΩÊï∞ÔºåÂÆÉÁöÑÂÖ®Áß∞ÊòØHMAC using SHA-256ÔºåËøôÁâµÊâØÂà∞ÁöÑÂ∞±Â§ö‰∫Ü„ÄÇ Ë¶ÅÁêÜËß£ÂÆÉÈ¶ñÂÖàË¶ÅÁü•ÈÅì‰∏Ä‰∏ãÂá†‰∏™Ê¶ÇÂøµÔºö 1. ÂçïÂêëÊï£ÂàóÂáΩÊï∞ÔºöÂçïÂêëÊï£ÂàóÂáΩÊï∞‰πüÁß∞‰ø°ÊÅØÊëòË¶ÅÂáΩÊï∞ÔºàMessage Digest FunctionÔºâ,ÂìàÂ∏åÔºàHashÔºâÂáΩÊï∞ÊàñËÄÖÊùÇÂáëÂáΩÊï∞„ÄÇ ÂÆÉÊòØÂÖ∂‰Ωô‰∏§‰∏™Ê¶ÇÂøµÁöÑÂü∫Á°Ä„ÄÇÁé∞Âú®‰∏çËÄÉËôëÂÖ∑‰ΩìÂÆûÁé∞ÔºåÊääËøô‰∏™Ê¶ÇÂøµÂΩìÊàê‰∏Ä‰∏™ÈªëÁÆ±„ÄÇ ËæìÂÖ•Ôºö ‰ªªÊÑèÈïøÂ∫¶Ê∂àÊÅØÔºàÂèØ‰ª•ÊòØ1bit „ÄÅ1K„ÄÅ1MÔºåÁîöËá≥ÂèØ‰ª•100GÔºâËæìÂá∫Ôºö ‰∏Ä‰∏≤Âõ∫ÂÆöÈïøÂ∫¶ÁöÑÊï∞ÊçÆÔºàÊï£ÂàóÂÄºÔºå‰πüÁß∞Ê∂àÊÅØÊëòË¶ÅÔºåÊåáÁ∫πÔºâ ÂçïÈ°πÊï£ÂàóÂáΩÊï∞ÊúâÂ¶Ç‰∏ãÂá†ÁßçÁâπÊÄßÔºö Ê†πÊçÆ‰ªªÊÑèÈïøÂ∫¶Ê∂àÊÅØÂæóÂá∫ÁöÑÊï£ÂàóÂÄºÈïøÂ∫¶ÊòØÂõ∫ÂÆöÁöÑ„ÄÇ Êï£ÂàóÂÄºËÆ°ÁÆóÊó∂Èó¥Áü≠ ‰∏çÂêåÁöÑÊ∂àÊÅØÊúâ‰∏çÂêåÁöÑÊï£ÂàóÂÄºÔºàÂ¶ÇÊûú‰∏§‰∏™‰∏çÂêåÊ∂àÊÅØÁöÑÊï£ÂàóÂÄºÁõ∏ÂêåÔºåÈÇ£Â∞±Áß∞‰∏∫ÂèëÁîüÁ¢∞ÊíûÔºâ Ê†πÊçÆÊï£ÂàóÂÄºÊó†Ê≥ïËøòÂéüÊ∂àÊÅØÔºàÂçïÂêëÊÄßÔºåÂè™ËÉΩ‰ªéÊ∂àÊÅØÂà∞Êï£ÂàóÂÄºÔºåÂèç‰πã‰∏çÊàêÁ´ãÔºâ ÂÖ≥‰∫éÂçïÂêëÊï£ÂàóÂáΩÊï∞ÁöÑÂ∫îÁî®ÔºåÂæàÂ∏∏ËßÅÁöÑ‰∏Ä‰∏™Â∞±ÊòØÊñá‰ª∂ÁöÑÊ†°È™å ÂõæÁâá‰∏≠ÊâÄ‰ΩøÁî®ÁöÑÂìàÂ∏åÂáΩÊï∞ÊòØ SHA-1Ôºà‰πüÊúâ‰ΩøÁî®MD5ÁöÑÔºâ„ÄÇÊúâ‰∫ÜÂÆÉÂ∞±ÁúãÂèØ‰ª•Á°ÆÂÆö‰∏ãËΩΩÁöÑÊñá‰ª∂ÊòØ‰∏çÊòØË¢´Âä®ËøáÊâãËÑö„ÄÇÂõ†‰∏∫ËΩØ‰ª∂Âú®ÂèëÂ∏ÉÁöÑÊó∂ÂÄô‰ºöÂêåÊó∂ÂÖ¨Â∏ÉÂÆÉÊï£ÂàóÂÄºÔºåÊ≠£Âú®‰∏ãËΩΩÁöÑËΩØ‰ª∂‰πüÂèØ‰ª•ËÆ°ÁÆóÊï£ÂàóÂÄºÔºå‰∏§‰∏™Êï£ÂàóÂÄºÁõ∏ÂêåÂ∞±ËØ¥ÊòéÊòØÂêå‰∏Ä‰∏™ËΩØ‰ª∂„ÄÇ Â∏∏ËßÅÁöÑÂçïÈ°πÊï£ÂàóÂáΩÊï∞Ôºö MD4(Message Digest 4) MD5(Message Digest 5) SHA-1 SHA-256 Êï∞Â≠ó‰ª£Ë°®Êï£ÂàóÂÄºÈïøÂ∫¶‰∏∫256bit SHA-384 SHA-512 2. MAC Ê∂àÊÅØËÆ§ËØÅÁ†ÅMAC:Message Authentication Code Âç≥Ê∂àÊÅØËÆ§ËØÅÁ†Å ÂÆÉÂèØ‰ª•Á°ÆËÆ§Ê∂àÊÅØÂÆåÊï¥ÊÄßÂπ∂ËøõË°åËÆ§ËØÅ„ÄÇ ËæìÂÖ•Ôºö Ê∂àÊÅØ+ÂèëÈÄÅËÄÖ‰ª•ÂèäÊé•Êî∂ËÄÖ‰πãÈó¥ÂÖ±‰∫´ÁöÑÂØÜÈí• Ê≥®Ôºö‰∏éÂçïÈ°πÊï£ÂàóÂáΩÊï∞‰∏çÂêå‰πãÂ§ÑÂ∞±ÊòØÂÆÉÂ§ö‰∫Ü‰∏Ä‰∏™ÂØÜÈí•ÁöÑÂèÇ‰∏é ËæìÂá∫Ôºö Âõ∫ÂÆöÈïøÂ∫¶ÁöÑÊï∞ÊçÆÔºåÁß∞‰∏∫MACÂÄº Ê≥®ÔºöËøôÂ∞±Ë∑üÂçïÂêëÊï£ÂàóÂáΩÊï∞‰∏ÄÊ†∑‰∫Ü Á°ÆÂàáÊù•ËØ¥ÂÆÉÊåáÁöÑÊòØ‰∏ÄÁßçËÆ§ËØÅÊú∫Âà∂„ÄÇËøôÁßçÊú∫Âà∂ÊúâÂ§öÁßçÂÆûÁé∞ÊñπÊ≥ïÔºåÂçïÈ°πÊï£ÂàóÂáΩÊï∞Â∞±ÊòØÂÖ∂‰∏≠‰πã‰∏Ä„ÄÇ‰ΩøÁî®ÂçïÂêëÊï£ÂàóÂáΩÊï∞Ôºà‰πüÁß∞HashÂáΩÊï∞ÔºâÂÆûÁé∞ÁöÑÊ∂àÊÅØËÆ§ËØÅÁ†ÅÂ∞±Áß∞‰∏∫ HMACÔºåÂÖ∂‰∏≠HÂ∞±ÊòØHashÁöÑÊÑèÊÄù„ÄÇ‰∏ÄÊ¨°Ëß£ÂÜ≥‰∫ÜÂçïÈ°πÊï£ÂàóÂáΩÊï∞ËôΩÁÑ∂ÂèØ‰ª•Ê£ÄÊµãÂà∞ÁØ°ÊîπÔºàÂÆåÊï¥ÊÄßÔºâÔºå‰ΩÜÊòØÂç¥Ê≤°ÊúâÂäûÊ≥ïËØÜÂà´‰º™Ë£ÖÁöÑÈóÆÈ¢ò„ÄÇ Ê≥®ÔºöÊó†Ê≥ïËØÜÂà´‰º™Ë£ÖÊòØÂõ†‰∏∫Â¶ÇÊûúÊúâÁ¨¨‰∏âËÄÖÂÅáË£ÖÂÖ∂‰∏≠‰∏ÄÊñπÂèëÊ∂àÊÅØÔºåÂè¶‰∏ÄÊñπÊ†πÊú¨Êó†‰ªéÁü•ÊôìËøô‰∏™Ê∂àÊÅØÊòØ‰∏çÊòØÂØπÊñπÂèëÊù•ÁöÑ„ÄÇ 3.HMAC ÂìàÂ∏åÊ∂àÊÅØËÆ§ËØÅÁ†Å‰∏äÈù¢‰πüÊèêÂà∞‰∫ÜÔºåHMACÂ∞±ÊòØ‰ΩøÁî®‰∫ÜÂçïÈ°πÊï£ÂàóÂáΩÊï∞Êù•ÊûÑÈÄ†Ê∂àÊÅØËÆ§ËØÅÁ†ÅÁöÑ‰∏ÄÁßçÊñπÊ≥ïÔºàRFC2104Ôºâ„ÄÇÊ†πÊçÆÂÆÉÊâÄ‰ΩøÁî®ÁöÑÊï£ÂàóÂáΩÊï∞‰∏çÂêåÔºåÂ∞±Âá∫Áé∞‰∫ÜÂ¶Ç‰∏ãËøô‰πàÂ§öÁßç HMAC HMAC ÁÆóÊ≥ï Â§áÊ≥® HS256 HMAC using SHA-256 HS384 HMAC using SHA-384 HS512 HMAC using SHA-512 Ê≥®Ôºö‰ΩøÁî®Ê∂àÊÅØËÆ§ËØÅÁ†ÅÊòØÊó†Ê≥ï‰øùËØÅÊ∂àÊÅØÁöÑÊú∫ÂØÜÊÄßÁöÑÔºåÂÆÉÂè™ËÉΩ‰øùËØÅÊ∂àÊÅØË¢´Ê≠£Á°ÆÁöÑ‰º†ÈÄÅ‰∫Ü„ÄÇ‰æãÂ¶Ç ‰º†ÈÄÅÁöÑÂÆåÊï¥Ê∂àÊÅØÊ†ºÂºè‰∏∫ ‚Äú123456‚Äù+‚ÄúÊ∂àÊÅØÈ™åËØÅÁ†Å‚ÄùÔºåÊ∂àÊÅØÈ™åËØÅÁ†ÅÁöÑ‰ΩúÁî®Â∞±ÊòØÂú®ÂØπÊñπÊî∂Âà∞Ê∂àÊÅØ‰πãÂêéÂèØ‰ª•Ê†πÊçÆÈ™åËØÅÁ†ÅÊù•È™åËØÅ ‚Äú123456‚ÄùÊòØÊ≤°ÊúâË¢´‰øÆËøáÁöÑ„ÄÇËá≥‰∫éÊú∫ÂØÜÊÄßÔºåÈÇ£ÈúÄË¶ÅÂØπ‚Äú123456‚ÄùËøõË°åÂä†ÂØÜÔºåËÄåËøô‰∏çÂÖ≥Ê∂àÊÅØÈ™åËØÅÁ†Å‰ªÄ‰πà‰∫ã„ÄÇ Ê∂àÊÅØËÆ§ËØÅÁ†ÅÊúâ‰∏§‰∏™Êó†Ê≥ïËß£ÂÜ≥ÁöÑÈóÆÈ¢òÔºö ÂØπÁ¨¨‰∏âÊñπËØÅÊòé Èò≤Ê≠¢Âê¶ËÆ§ Êï∞Â≠óÁ≠æÂêçÂèØ‰ª•Ëß£ÂÜ≥‰∏äËø∞ÈóÆÈ¢òÔºå‰ΩÜÊòØËøô‰∏éJWT ÂÖ≥Á≥ª‰∏çÂ§ß„ÄÇÁé∞Âú®ÊàëÊòéÁôΩ‰∫ÜÔºåË¢´Âú®JWT‰∏≠Ë¢´Áß∞‰∏∫ ‚ÄúSignature Á≠æÂêç‚ÄùÁöÑÈÉ®ÂàÜÔºåÂÖ∂ÂÆûÊòØÂâç‰∏§ÈÉ®ÂàÜÁöÑÊ∂àÊÅØËÆ§ËØÅÁ†ÅÔºàMACÔºâÔºåÂÆòÊñπÁß∞‰πã‰∏∫Êï∞Â≠óÁ≠æÂêçÊàëËßâÁöÑÂÖ∂ÂÆûÂπ∂‰∏çÊÄé‰πàÂáÜÁ°Æ„ÄÇÂú®JWT‰∏≠ÂØÜÈí•Âπ∂‰∏ç‰∏éÂÆ¢Êà∑Á´ØÂÖ±‰∫´ÔºåÂÖ∂‰∏∫ÊúçÂä°Âô®Áã¨Êúâ„ÄÇËøôÊ†∑‰∏ÄÊù•Âè™ÊúâÊúçÂä°Âô®ÂèØ‰ª•ÂèëTokenÔºåËÄåÂÆ¢Êà∑Á´ØÂõ†‰∏∫Áº∫Â∞ëÂØÜÈí•ËÄåÊó†Ê≥ï‰º™ÈÄ† Token„ÄÇÊúçÂä°Âô®‰ºöÂØπÊØè‰∏™ËØ∑Ê±ÇÈáåÈù¢ÁöÑTokenÁî®ÂØÜÈí•Êù•ÁÆóMACÂÄºÊù•È™åËØÅËøô‰∏™TokenÊòØ‰∏çÊòØËá™Â∑±ÂèëÂá∫ÂéªÁöÑ„ÄÇ ËÄå‰∏î‰∏∫‰∫ÜÁ°Æ‰øùToken‰∏çÊòØ‰∏ÄÁõ¥ÊúâÊïàÔºåËøòË¶ÅÂä†‰∏Ä‰∫õÊó∂Èó¥Êà≥Êù•ÈôêÂà∂ Reserverd claims ËØ¥Êòé exp(Expiration time) ÊòØ‰∏Ä‰∏™Êó∂Èó¥Êà≥Ôºå‰ª£Ë°®Ëøô‰∏™JWTÁöÑËøáÊúüÊó∂Èó¥ nbf(Not Before) ÊòØ‰∏Ä‰∏™Êó∂Èó¥Êà≥Ôºå‰ª£Ë°®Ëøô‰∏™JWTÁîüÊïàÁöÑÂºÄÂßãÊó∂Èó¥ÔºåÊÑèÂë≥ÁùÄÂú®Ëøô‰∏™Êó∂Èó¥‰πãÂâçÈ™åËØÅJWTÊòØ‰ºöÂ§±Ë¥•ÁöÑ iat(Issued at) ÊòØ‰∏Ä‰∏™Êó∂Èó¥Êà≥Ôºå‰ª£Ë°®Ëøô‰∏™JWTÁöÑÁ≠æÂèëÊó∂Èó¥ jti(JWT ID) ÊòØJWTÁöÑÂîØ‰∏ÄÊ†áËØÜ Ëøô‰∫õÊó∂Èó¥Êà≥ÂèØ‰ª•ËÆ©ÊúçÂä°Âô®Áü•ÈÅìËøô‰∏™TokenÊúâÊ≤°ÊúâËøáÊúü„ÄÇ ÂÖ≥‰∫éÊâÄË∞ìJWTÁöÑÂÆâÂÖ®ÊÄßÂØπ‰∫éTokenÁöÑÂÆâÂÖ®ÊÄßÔºåÂÖ∂ÂÆûÊàëËßâÂæóÊ≤°‰ªÄ‰πàÂ•ΩËØ¥ÁöÑÔºåTokenÂÆûË¥®‰∏äÂ∞±ÊòØ‰∏Ä‰∏™Áî±ÊúçÂä°Âô®Á≠æÂèëÁöÑÊó†Ê≥ï‰º™ÈÄ†ÁöÑ‰ª§Áâå„ÄÇJWTËÉΩÂ§ü‰øùËØÅ‰ª§ÁâåÊó†Ê≥ï‰º™ÈÄ†Â∞±Â∑≤ÁªèË∂≥Â§üÂÆâÂÖ®‰∫Ü„ÄÇËá≥‰∫é‰ªÄ‰πà‚ÄúÈªëÂÆ¢ÊãøÂà∞‰∫Ü‰ª§ÁâåÊÄé‰πàÂäûÔºåË¶ÅÊòØÊúâ‰∏≠Èó¥‰∫∫Âä´ÊåÅÔºåÊã¶Êà™‰∫ÜÊÄé‰πàÂäû‚Äù‰πãÁ±ªÁöÑÂÆâÂÖ®ÈóÆÈ¢òÔºåÂπ∂‰∏çÊòØJWTÁöÑÈîÖÔºåËÄåÊòØ‰º†ËæìÂçèËÆÆÁöÑÈîÖ„ÄÇHTTPÂçèËÆÆ ÊòéÊñá‰º†Ëæì‰ø°ÊÅØËÄåÈÄ†ÊàêÁöÑÂÆâÂÖ®ÈóÆÈ¢òÔºåJWTËÉΩÊúâ‰ªÄ‰πàË¥£‰ªªÔºüÊúÄÁÆÄÂçïÁöÑÊñπÊ≥ïÂ∞±ÊòØÊç¢HTTPSÂçèËÆÆÂñΩ„ÄÇ ‰ΩøÁî®JWTÊù•ËøõË°åÁî®Êà∑Ë∫´‰ªΩËÆ§ËØÅ C#‰∏≠ÊÄé‰πàÁîüÊàêJWTÊ†ºÂºèÁöÑTokenJWTÂú®ÂêÑ‰∏™Âπ≥Âè∞ÈÉΩÊúâÂ∑≤ÁªèÂ∞ÅË£ÖÂ•ΩÁöÑÂ∫ìÂèØ‰ª•‰ΩøÁî®Âú®.NETÂπ≥Âè∞‰∏ãÊúâÂ¶Ç‰∏ãÂá†‰∏™ÂëΩÂêçÁ©∫Èó¥ÂèØ‰ª•ÂºïÁî®Ôºö sharp123using System.Security.Claims;using System.IdentityModel.Tokens.Jwt;using Microsoft.IdentityModel.Tokens; ÂæÆËΩØÊääËøô‰∫õÁ±ªÂÖ®ÈÉΩÂºÄÊ∫êÊîæÂú®Âú®Github‰∏äÈù¢ÂèØ‰ª•‰æõÁ†îÁ©∂ÂÖ∂ÂÆûÁé∞ËøáÁ®ã ÂÖ≥‰∫éJWTÁöÑClaim Reserverd claims ËØ¥Êòé iss(Issuser) ‰ª£Ë°®Ëøô‰∏™JWTÁöÑÁ≠æÂèë‰∏ª‰Ωì sub(Subject) ‰ª£Ë°®Ëøô‰∏™JWTÁöÑ‰∏ª‰ΩìÔºåÂç≥ÂÆÉÁöÑÊâÄÊúâ‰∫∫ aud(Audience) ‰ª£Ë°®Ëøô‰∏™JWTÁöÑÊé•Êî∂ÂØπË±° exp(Expiration time) ÊòØ‰∏Ä‰∏™Êó∂Èó¥Êà≥Ôºå‰ª£Ë°®Ëøô‰∏™JWTÁöÑËøáÊúüÊó∂Èó¥ nbf(Not Before) ÊòØ‰∏Ä‰∏™Êó∂Èó¥Êà≥Ôºå‰ª£Ë°®Ëøô‰∏™JWTÁîüÊïàÁöÑÂºÄÂßãÊó∂Èó¥ÔºåÊÑèÂë≥ÁùÄÂú®Ëøô‰∏™Êó∂Èó¥‰πãÂâçÈ™åËØÅJWTÊòØ‰ºöÂ§±Ë¥•ÁöÑ iat(Issued at) ÊòØ‰∏Ä‰∏™Êó∂Èó¥Êà≥Ôºå‰ª£Ë°®Ëøô‰∏™JWTÁöÑÁ≠æÂèëÊó∂Èó¥ jti(JWT ID) ÊòØJWTÁöÑÂîØ‰∏ÄÊ†áËØÜ ÂìàÂ∏åÁÆóÊ≥ï alg ÂèÇÊï∞ Digital Signature or MAC ÁÆóÊ≥ï HS256 HMAC using SHA-256 hash algorithm HS384 HMAC using SHA-384 hash algorithm HS512 HMAC using SHA- 5 12 hash algorithm RS256 RSASSA using SHA-256 hash algorithm RS384 RSASSA using SHA- 384 hash algorithm RS512 RSASSA using SHA-512 hash algorithm ES256 ECDSA using P-256 curve and SHA-256 hash algorithm ES384 ECDSA using P- 384 curve and SHA-384 hash algorithm ES512 ECDSA using P-521curve and SHA-512 hash algorithm none No digital signature or MAC value included ÂèÇËÄÉËµÑÊñôJWTÂÆûÁé∞token-based‰ºöËØùÁÆ°ÁêÜMACÁÆóÊ≥ïÂªñÈõ™Â≥∞Python]]></content>
      <categories>
        <category>Â≠¶‰π†Á¨îËÆ∞</category>
      </categories>
      <tags>
        <tag>JWT</tag>
        <tag>ÂØÜÁ†ÅÂ≠¶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰ΩøÁî®ËÖæËÆØ‰∫ëÂØπË±°Â≠òÂÇ®‰Ωú‰∏∫ÂõæÂ∫ä]]></title>
    <url>%2F2018%2F04%2F20%2F%E4%BD%BF%E7%94%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E4%BD%9C%E4%B8%BA%E5%9B%BE%E5%BA%8A%2F</url>
    <content type="text"><![CDATA[ÂºÄÂßãËÖæËÆØ‰∫ëÁöÑÂØπË±°Â≠òÂÇ®ÊòØÊúâÂÖçË¥πÈ¢ùÂ∫¶ÁöÑÔºåËÄå‰∏îËøò‰∏çÁÆóÂ∞ë ‰∏ÉÁâõ‰∫ëÁöÑÂÖçË¥πÈ¢ùÂ∫¶Áï•Êúâ‰∏çÂêåÔºöÂÜôËØ∑Ê±ÇÊï∞‰∏∫50‰∏áÊ¨°ÊØèÊúà ÊéßÂà∂Âè∞ÈÖçÁΩÆÊ≥®ÊÑèÊúâ‰∏§‰∏™ÂÖ≥ÈîÆÁöÑÈÖçÁΩÆ‰∏çËÉΩÂøΩÁï• Â≠òÂÇ®Ê°∂ÔºàbucketÔºâËÆøÈóÆÊùÉÈôê Èò≤ÁõóÈìæËÆæÁΩÆ ËÆøÈóÆÊùÉÈôêËÆøÈóÆÊùÉÈôêÂ∫îËÆæÁΩÆ‰∏∫ÂÖ¨ÊúâËØªÁßÅÊúâÂÜô„ÄÇÂæàÂ•ΩÁêÜËß£ÔºåÊàë‰ª¨ÊòØË¶ÅÁî®COSÊù•ÂÅöÂõæÂ∫äÁöÑÔºåÂøÖÁÑ∂ÊòØÈúÄË¶ÅËé∑ÂèñÊñá‰ª∂ÁöÑÁõ¥Êé•ÈìæÊé•ÊâçË°åÔºåÊâÄ‰ª•‰∏çËÉΩÂú®ËØªÂèñÂõæÁâáÁöÑÊó∂ÂÄôËÆæÁΩÆÊùÉÈôê„ÄÇ ÊùÉÈôêËÆæÁΩÆÊñáÊ°£Âú®Ê≠§ Èò≤ÁõóÈìæÂ¶ÇÊûúË¢´ÁõóÈìæ‰ºöÂØºËá¥‰Ω†ÁöÑÂÖçË¥πÈ¢ùÂ∫¶ÂèØËÉΩ‰ºöÂø´Áî®Â∞ΩÔºåÂõ†Ê≠§Èò≤ÁõóÈìæ‰πüÊòØÈúÄË¶ÅËÄÉËôëÁöÑ„ÄÇ‰∏ãÂõæ‰ª•ÊàëÁöÑÂçöÂÆ¢Âú∞ÂùÄ‰∏∫‰æã ÂºÄÂêØ‰πãÂêéÂç≥‰ΩøÂÖ∂‰ªñ‰∫∫Ëé∑ÂèñÂà∞ÈìæÊé•‰πüÊó†Ê≥ïËÆøÈóÆÁõ∏Â∫îÂõæÁâá ‰∏ä‰º†ÂèñÈìæ‰∏ä‰º†ÂõæÁâáÂÆòÊñπ‰πüÊúâÊñá‰ª∂ÁÆ°ÁêÜËΩØ‰ª∂COS Browser Ôºå‰∏ä‰º†‰∏ãËΩΩÂõæÁâáËøòÊòØÊå∫Êñπ‰æøÁöÑ„ÄÇ ÂΩìÁÑ∂Â¶ÇÊûúËßâÂæóËøôÊ†∑Â≠êËøòÊòØ‰∏çÂ§üÊñπ‰æøÁöÑËØùÔºåÂèØ‰ª•Âü∫‰∫éËÖæËÆØ‰∫ëAPIËá™Â∑±ÂºÄÂèëÂ∫îÁî® Ëé∑ÂèñÈìæÊé•Ë¶ÅËé∑ÂèñÂõæÁâáÈìæÊé•ÁöÑËØùÂè≥ÈîÆÊñá‰ª∂ËØ¶ÊÉÖÂç≥ÂèØ„ÄÇÂ∞ÜÂÖ∂Â§çÂà∂‰∏ãÊù•Âç≥ÂèØ‰ΩøÁî®„ÄÇ Ê≥®ÔºöÂõ†‰∏∫ÂºÄÂêØ‰∫ÜÈò≤ÁõóÈìæËøôËÆæÁΩÆÔºåÂõ†Ê≠§ÂΩì‰Ω†Âú®Êú¨Âú∞ÂÜôÂçöÂÆ¢ÁöÑÊó∂ÂÄô‰ºöÂá∫Áé∞ÂõæÁâáÊó†Ê≥ïÈ¢ÑËßàÁöÑÊÉÖÂÜµ„ÄÇÂõ†‰∏∫Èò≤ÁõóÈìæÊú∫Âà∂Âè™ÂÖÅËÆ∏ÁôΩÂêçÂçï‰∏≠ÊåáÂÆöÂüüÂêçÊàñIPËÆøÈóÆ„ÄÇ‰πüÂ∞±ÊòØËØ¥Âè™ËÉΩÂú®ÂçöÂÆ¢‰∏≠Áúã ÂêéËÆ∞ÂÖ≥‰∫éÂõæÂ∫äÔºåÂõΩÂÜÖÂèØ‰ª•ÈÄâÊã©ÁöÑÊúâÂæàÂ§ö„ÄÇ ÁΩë‰∏äÊúâÂæàÂ§ö‰ΩøÁî®ÂæÆ‰ø°‰ª•ÂèäÂæÆÂçöÊù•ÂÖÖÂΩìÂÖçË¥πÂõæÂ∫äÁöÑ„ÄÇ‰ΩÜÊòØÂ¶Ç‰Ω†ÊâÄËßÅÔºåÂæÆÂçöÂæÆ‰ø°ÊÉ≥Âä†‰∏Ä‰∏™Èò≤ÁõóÈìæÊù•ÈòªÊ≠¢Â§ñÈìæËÆøÈóÆÊòØÂæàÂÆπÊòìÁöÑÔºåÊääÂõæÊîæÂú®ÈÇ£‰∏äÈù¢‰∏ùÊØ´Ê≤°ÊúâÂÆâÂÖ®ÊÑü„ÄÇ Âõ†Ê≠§ÂèØ‰ª•ËÄÉËôëËÖæËÆØ‰∫ëÊàñËÄÖÈòøÈáå‰∫ëÁöÑÂØπË±°Â≠òÂÇ®ÊúçÂä°Ôºå‰∏çËøáÈòøÈáå‰∫ëÂ•ΩÂÉèÊ≤°ÊúâÂÖçË¥πÈ¢ùÂ∫¶ÔºåÂõ†Ê≠§Ê≤°ÊúâÈÄâÁî®„ÄÇ ‰∏ÉÁâõ‰∫ë‰∏éËÖæËÆØ‰∫ëÁ±ª‰ººÔºåÈÉΩÂèØ‰ª•‰ΩøÁî®ÂÖçË¥πÁöÑÂØπË±°Â≠òÂÇ®ÊúçÂä°Êù•ÂÖÖÂΩìËá™Â∑±ÁöÑÂõæÂ∫ä„ÄÇ ÂõΩÂ§ñÁöÑ‰∏Ä‰∏™Áü•ÂêçÂõæÂ∫ä imgur ÂÆûÂú®ÊòØÂèØÊÉúË¢´Â¢ô‰∫Ü,‰∏çÁÑ∂Áî®Ëµ∑Êù•ËøòÊòØÂçÅÂàÜÊñπ‰æøÁöÑ„ÄÇ]]></content>
      <categories>
        <category>HexoÁõ∏ÂÖ≥</category>
      </categories>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>ÊïôÁ®ã</tag>
        <tag>ÂõæÂ∫ä</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Áªà‰∫éË£ÖÂ•ΩHexoÂï¶]]></title>
    <url>%2F2018%2F04%2F19%2F%E7%BB%88%E4%BA%8E%E8%A3%85%E5%A5%BDHexo%E5%95%A6%2F</url>
    <content type="text"><![CDATA[Â§ßÂäüÂëäÊàêÔºÅÊäòËÖæ‰∫Ü‰∏ÄÂ§©Áªà‰∫éÊääÁõ∏ÂÜåÂäüËÉΩÂä†‰∏äÂï¶„ÄÇ ÂæÖÊàëËÄÉ‰∏äÁ†îÔºå‰∏ÄÂÆöÂ•ΩÂ•ΩÁª¥Êä§Ëøô‰∏™ÂçöÂÆ¢ÔºÅ 2018Âπ¥4Êúà19Êó•Êôö12ÁÇπ Áïô Âª∫Á´ôÂèÇËÄÉ ‰ª•‰∏ãÊòØÊàëÂú®ÊäòËÖæÂçöÂÆ¢ÁöÑËøáÁ®ã‰∏≠ÂØπÊàëÊúâÂæàÂ§ßÂ∏ÆÂä©ÁöÑÂçöÊñáÈìæÊé•ÔºåÂú®Ê≠§ÂçÅÂàÜÊÑüË∞¢‰ΩúËÄÖ‰ª¨ÁöÑÂàÜ‰∫´„ÄÇ Ê∑ªÂä†ÊñáÁ´†ÁΩÆÈ°∂2018Âπ¥4Êúà29Êó• ÂÆåÊàêhexoÂçöÂÆ¢‰ºòÂåñ‰πãÊñáÁ´†ÁΩÆÈ°∂+ÁΩÆÈ°∂Ê†áÁ≠æ ‰ΩúËÄÖ: wangwlj HexoÈ´òÁ∫ßÊïôÁ®ãHexoÈ´òÁ∫ßÊïôÁ®ã‰πã‰∏ªÈ¢òÂºÄÂèë‰ΩúËÄÖÔºöJamling IssueÂäüËÉΩÁöÑ‰ΩøÁî®github issue ÁöÑÁî®Ê≥ï‰ΩúËÄÖÔºöXiChen Â¢ûÂä†GitmentËØÑËÆ∫Á≥ªÁªüHexo-Yilia‰∏≠Ê∑ªÂä†gitmentËØÑËÆ∫ÂäüËÉΩ ‰ΩúËÄÖÔºöÂ•ΩÂ§ß‰∏ÄÊ£µÊ†ë Â¢ûÂä†ÊñáÁ´†ËÆøÈóÆÁªüËÆ°hexo yilia ‰∏ªÈ¢òÊ∑ªÂä†ÊñáÁ´†ËÆøÈóÆÁªüËÆ° ‰ΩúËÄÖ Êû´‰πãÊú®ËêΩ‰ΩúËÄÖÔºöÊû´‰πãÊú®ËêΩ ÂéüÈìæÊé•ÔºöÁªôHexoÂçöÂÆ¢Ê∑ªÂä†ËÆøÈóÆÁªüËÆ°Âéü‰ΩúËÄÖÔºöÂøÉÂΩª Â¢ûÂä†ÊñáÁ´†ÁâàÊùÉÂ£∞Êòé Âú®Hexo‰∏≠Ëá™Âä®‰∏∫Yilia‰∏ªÈ¢òÂ¢ûÂä†ÁâàÊùÉÂ£∞Êòé ‰ΩúËÄÖÔºöF!redent Â¢ûÂä†insÁõ∏ÂÜåÊ®°ÂùóHexoÂçöÂÆ¢ÂàõÂª∫insÁõ∏ÂÜåÊ®°Âùó ‰ΩúËÄÖÔºöLuojinghui Â¢ûÂä†‰∏ªÈ¢ò(Yilia)‰∏Ä‰∏™ÁÆÄÊ¥Å‰ºòÈõÖÁöÑhexo‰∏ªÈ¢ò ‰ΩúËÄÖÔºöLitten Bug‰øÆÂ§çÔºàÂùëÔºâ‰øÆÂ§çÈáçÊñ∞ÊâìÂåÖ‰ª•ÂêéÂ≠êÊ†áÈ¢ò‰∏çÂ±Ö‰∏≠ÈóÆÈ¢ò„ÄÇÂéüÂõ†Ôºö CSS‰∏≠Ëé´ÂêçÂ•áÂ¶ôÁº∫Â§±‰∏Ä‰∏™Â±ûÊÄß -webkit-box-orient: vertical;Ëß£ÂÜ≥ÂäûÊ≥ïÔºö Âú®ÁîüÊàêÁöÑCSSÊñá‰ª∂‰∏≠ÊâæÂà∞ Áõ∏Â∫î‰ΩçÁΩÆÁ≤ò‰∏äÂéªÂç≥ÂèØ„ÄÇ 2018Âπ¥4Êúà29Êó• 22ÁÇπ24ÂàÜ Ë°•ÂÖÖÔºöCSSÊñá‰ª∂ÁöÑ‰ΩçÁΩÆÂú®1\themes\yilia\source CSSÊñá‰ª∂ÂêçÁß∞ÊØèÊ¨°ÁîüÊàêÈÉΩ‰∏ç‰∏ÄÊ†∑Ôºå‰∏ÄËà¨ÊòØ main.%%%%%%.css„ÄÇÂ¶ÇÊûúÊñá‰ª∂Â§π‰∏ãÊúâÂæàÂ§ömainÂºÄÂ§¥ÁöÑCSSÊñá‰ª∂ÔºåÈÇ£‰πàÂèØ‰ª•ÂÖ®Âà†ÊéâÈáçÊñ∞ÁîüÊàê„ÄÇËøôÊ†∑Â∞±‰ºöÂè™Êúâ‰∏Ä‰∏™main.css‰∫Ü„ÄÇ ÊâìÂºÄ‰πãÂêéÂèØËÉΩ‰ºöÁúãÂà∞ÂØÜÂØÜÈ∫ªÈ∫ªÁöÑÂ≠óÁ¨¶ÔºåËøôÂõ†‰∏∫ÁîüÊàêÁöÑÊó∂ÂÄôÂ¶ÇÊûúÊòØÊâßË°åÁöÑ 1npm run dist ÈÇ£‰πàÁîüÊàêÁöÑcssÊñá‰ª∂ÂíåjsÊñá‰ª∂ÂÖ®ÈÉ®ÈÉΩÊòØÂéãÁº©‰ª•ÂêéÁöÑÔºåÂÆÉ‰ª¨ÁöÑÊ†ºÂºèÂÖ®ÈÉΩÊ≤°‰∫ÜÔºåÊâÄ‰ª•‰ªéÂ§¥ËøûÂà∞Â∞æ„ÄÇËøôÊó∂ÂÄô‰Ω†ÈúÄË¶ÅÁî®‰ΩøÁî®ÊñáÊú¨ÁºñËæëÂô®ÁöÑÊêúÁ¥¢ÂäüËÉΩÔºåÊêúÁ¥¢ÂÖ≥ÈîÆÂ≠ó header-subtitle123456789101112.header-subtitle&#123; text-align:center; color:#999; font-size:14px; line-height:25px; overflow:hidden; text-overflow:ellipsis; display:-webkit-box; -webkit-line-clamp:2;/*Âú®Ê≠§Ë°åÂêéÈù¢Ê∑ªÂä†*/ -webkit-box-orient: vertical; padding:0 24px &#125; Êï¥‰∏™CSSÊñá‰ª∂‰∏≠Êúâ‰∏§Â§ÑÈúÄË¶ÅËøôÊ†∑ÊîπÂä®ÔºåÂõ†‰∏∫‰∏ÄÂ§ÑÂØπÂ∫îÁöÑÊòØ Ê°åÈù¢Á´ØÂ∏ÉÂ±ÄÁîüÊïà„ÄÇÂè¶‰∏ÄÂ§ÑÂàôÊòØÁßªÂä®Á´ØÁîüÊïà ‰øÆÂ§çGitmentÂàùÂßãÂåñCommentÂ§±Ë¥•ÁöÑÈóÆÈ¢ò 2018Âπ¥4Êúà21Êó•20ÁÇπ23ÂàÜÂéüÂõ†Ôºö CommentsÁöÑlableÂ≠óÊï∞Ë∂ÖËøáÊúÄÂ§ßÈïøÂ∫¶Ôºà50‰∏™Â≠óÁ¨¶ÔºâËß£ÂÜ≥ÂäûÊ≥ïÔºö ‰ΩøÁî®ÊñáÁ´†ÂàõÂª∫Êó∂Èó¥ÂΩìlabelÂ∞±‰∏ç‰ºöË∂ÖÈïø‰∫ÜÊ∑ªÂä†GitmentËØÑËÆ∫Á≥ªÁªüË∏©ËøáÁöÑÂùë‰ΩúËÄÖÔºöXiChen Ê≥®ÔºöÂú®Yilia‰∏ªÈ¢ò‰∏≠ÔºåÊòØÈúÄË¶ÅËøõÂÖ•Ê≠§Ë∑ØÂæÑ‰∏ãËøõË°åÊõ¥ÊîπidÁöÑÔºö layout/_partial/post/gitment.ejs ÈÖçËâ≤ÊñπÊ°àËìùËâ≤‰ª£Á†ÅÔºö#00C3FF Ôºà0,195,255Ôºâ ÈªÑËâ≤‰ª£Á†ÅÔºö#FFE149Ôºà255,225,73Ôºâ]]></content>
      <categories>
        <category>‰Ω†Â•ΩÔºå‰∏ñÁïå</category>
      </categories>
      <tags>
        <tag>hexo‰∏™ÊÄßÂåñ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VScode+Sphinx+ReadTheDocs‰ªéÁéØÂ¢ÉÊê≠Âª∫Âà∞ÊîæÂºÉ]]></title>
    <url>%2F2017%2F12%2F21%2FVScode%2BSphinx%2BReadTheDocs%E4%BB%8E%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%88%B0%E6%94%BE%E5%BC%83%2F</url>
    <content type="text"><![CDATA[Ê¶ÇËø∞Ê≠§ÁØáÂçöÂÆ¢Áî®Êù•ËÆ∞ÂΩïÂú®windows10‰∏≠ÈÖçÁΩÆÁéØÂ¢ÉÁöÑËøáÁ®ãÔºåÊ≥®ÊÑèÊòØWindows‰∏ã ÔºåÊàëÊ≤°ÊúâÂú®Linux‰∏ãÈù¢Â∞ùËØïËøáÈÖçÁΩÆ„ÄÇ‰∏Ä‰∏ãÂèÇËÄÉ‰∫ÜÂêÑË∑ØÊïôÁ®ãÂä†‰∏äËá™Â∑±‰∫≤ÊµãÔºåÂ∫îËØ•ÊòØÊ≤°ÊúâÈóÆÈ¢òÁöÑ„ÄÇ ‚Ä¶‚Ä¶..Ë¢´ÊéèÁ©∫(‚óé_‚óé;) ÊùêÊñôÊ∏ÖÂçï Python 3.4 Visual Studio Code Git GitHub‰ªìÂ∫ì GitHub Desktop Â∑•ÂÖ∑ÂÆâË£Ö ÂÆâË£ÖPython3.4 ÂÆâË£ÖGit ÂÆâË£ÖVisual Studio Code ÂÆâË£ÖGitHubDesktop Ê≠£Á°ÆÂÆâË£ÖÂÆå‰∏äËø∞ËΩØ‰ª∂‰πãÂêéÊé•‰∏ãÊù•ËØ¥ÊòéÂÖ∑‰ΩìÊìç‰ΩúÔºö ÊâìÂºÄPowerShell ËæìÂÖ•ÂëΩ‰ª§Ôºö ÂÆâË£Ö sphinx1pip install sphinx sphinx-autobuild ÂÆâË£Ö restructuredtext-lint1pip install restructuredtext-lint Ê≥®Ôºö To PythonÂ∞èÁôΩÔºöÂ¶ÇÊûúÊ≤°ÊúâÊ≠£Á°ÆÁöÑÂ∞ÜPythonÊ∑ªÂä†Âà∞ÁéØÂ¢ÉÂèòÈáèÔºåÈÇ£‰πàpowerShellÊòØ‰∏ç‰ºöËØÜÂà´ pip ÂëΩ‰ª§ÁöÑ„ÄÇÊàë‰πüÊòØ‰∏™PythonÂ∞èÁôΩÂëµÂëµÂëµ VScodeÈÖçÁΩÆ ÂÆâË£ÖÊèí‰ª∂ reStructuredText ÂÆÉÊèê‰æõ‰∫Ü.rstÊñá‰ª∂ÁöÑÈ¢ÑËßàÂäüËÉΩÂçÅÂàÜ‰æøÊç∑„ÄÇ ÂÆâË£ÖÊèí‰ª∂ Table Formatter Áî±‰∫éreStructuredTextÂÅöË°®Ê†ºÁâπÂà´È∫ªÁÉ¶ÔºåËøô‰∏™Êèí‰ª∂ÂèØ‰ª•Â∏ÆÂä©‰Ω†ÂÅöË°®Ê†ºÔºåÂè™ÈúÄË¶ÅÊääÂÖ≥ÈîÆÁöÑÊ†áËÆ∞ÂÜôÂØπÔºåÂÖ∂‰ΩôÁöÑÈÉΩ‰ºöËá™Âä®Ë°•ÂÖ®ËÄå‰∏îÊïàÊûúÂçÅÂàÜÁæéËßÇ„ÄÇ Ê≥®ÊÑèÔºö Êèí‰ª∂ÂÆâË£ÖÂÆåÊàê‰πãÂêéÈúÄË¶ÅÁÇπÂáªÈáçÊñ∞Âä†ËΩΩÊñπËÉΩÁîüÊïà To ÔºöVSCodeÂ∞èÁôΩ Êñ∞Âª∫SphinxÈ°πÁõÆ Êñ∞Âª∫‰∏Ä‰∏™ÁõÆÂΩï ÊØîÂ¶ÇÂ∞±Âè´Ôºösphinxtest Âú®Ê≠§ÁõÆÂΩï‰∏ãÊâìÂºÄVSCode ÊâìÂºÄÈõÜÊàêÁªàÁ´ØÂπ∂ËæìÂÖ•ÂëΩ‰ª§Ôºö1sphinx-quickstart ËøáÁ®ãÂ¶Ç‰∏ãÔºö Âú®Ëøô‰πàÈïøÁöÑÊµÅÁ®ã‰∏≠Ôºå‰∏ÄËà¨Êù•ËØ¥Âè™Êúâ‰∏âÈ°πÊòØÈúÄË¶Å‰Ω†ÊâãÂä®ËæìÂÖ•ÁöÑÔºåÂÖ∂‰ΩôÁöÑÂùáÂèØ‰ª•Áõ¥Êé•Êï≤Enter Project name: sphinxtestProject ÔºàÈ°πÁõÆÂêçÁß∞Ôºâ Author name(s): Tom Ôºà‰ΩúËÄÖÔºâ Project version []: 1.0.1 ÔºàÈ°πÁõÆÁâàÊú¨Ôºâ Ê≥®ÈáäÔºö ÊñáÊ°£Ê†πÁõÆÂΩï(Root path for the documentation)ÔºåÈªòËÆ§‰∏∫ÂΩìÂâçÁõÆÂΩï(.) ÊòØÂê¶ÂàÜÁ¶ªÊñáÊ°£Ê∫ê‰ª£Á†Å‰∏éÁîüÊàêÂêéÁöÑÊñáÊ°£(Separate source and build directories): y Ê®°Êùø‰∏éÈùôÊÄÅÊñá‰ª∂Â≠òÊîæÁõÆÂΩïÂâçÁºÄ(Name prefix for templates and static dir):_ È°πÁõÆÂêçÁß∞(Project name) : EvaEngine ‰ΩúËÄÖÂêçÁß∞(Author name)ÔºöAlloVince È°πÁõÆÁâàÊú¨(Project version) : 1.0.1 ÊñáÊ°£ÈªòËÆ§Êâ©Â±ïÂêç(Source file suffix) : .rst ÈªòËÆ§È¶ñÈ°µÊñá‰ª∂Âêç(Name of your master document):index ÊòØÂê¶Ê∑ªÂä†epubÁõÆÂΩï(Do you want to use the epub builder):n ÂêØÁî®autodoc|doctest|intersphinx|todo|coverage|pngmath|ifconfig|viewcodeÔºön ÁîüÊàêMakefile (Create Makefile)Ôºöy ÁîüÊàêwindowsÁî®ÂëΩ‰ª§Ë°å(Create Windows command file):y ËøáÁ®ãÊâßË°åÂÆåÊàê ÈÄâÊã© index.rst ‰πãÂêéÈÄâÊã©È¢ÑËßà ÊïàÊûúÂ¶Ç‰∏ãÔºö ÁÑ∂ËÄåËøô‰∏™‰∏ªÈ¢òÊàë‰∏™‰∫∫ÊÑüËßâ‰∏ëÁöÑ‰∏çËÉΩÁõ¥ËßÜ ÈÇ£‰πàÊàë‰ª¨Â∞±Êç¢‰∏Ä‰∏™‰∏ªÈ¢òÂêßÔºö È¶ñÂÖàÊâìÂºÄ Conf.py Â∞ÜÂéüÊú¨ÁöÑ 1html_theme = &apos;alabaster&apos; Êç¢Êàê 1html_theme = &apos;sphinx_rtd_theme&apos; Â¶ÇÂõæ Êé•ÁùÄÊâìÂºÄÁªàÁ´Ø 1pip install sphinx_rtd_theme ËøôÊ†∑Â∞±Â•ΩÁúãÂ§ö‰∫Ü GitHubÈÖçÁΩÆÂ∞ÜÂàöÊâçÊñ∞Âª∫ÁöÑ Shinxtest Êñá‰ª∂Â§π PushÂà∞GitHub‰ªìÂ∫ì‰∏≠Ôºå‰Ω†ÂèØ‰ª•ÂçïÁã¨‰∏∫ÂÆÉÂª∫Á´ã‰∏Ä‰∏™‰ªìÂ∫ìÔºå‰πüÂèØ‰ª•Â∞ÜÂÆÉÊ∑ªÂä†Âà∞‰Ω†ÁöÑ‰∏Ä‰∏™È°πÁõÆ‰ªìÂ∫ì‰∏≠‰Ωú‰∏∫È°πÁõÆÊñáÊ°£„ÄÇÊØîÂ¶ÇÊàëÂ∞±ÊääÂÆÉ‰Ωú‰∏∫È°πÁõÆÊñáÊ°£ÊîæËøõ‰∫ÜÊàëÁöÑÈ°πÁõÆ‰ªìÂ∫ìÈáå„ÄÇ GitHub ÈáåÈÄâÊã©‰ªìÂ∫ìÔºåÁÑ∂Âêé‰æùÊ¨°ÁÇπÂáª Setting =&gt; Webhooks &amp; Service =&gt; Add service =&gt; ReadTheDocs,ÊøÄÊ¥ªËøô‰∏™ÈÄâÈ°π„ÄÇ Read The Docs ÈÖçÁΩÆRead The Docs ‰∏ªË¶ÅÊòØ‰∏Ä‰∏™ ËøõÂÖ•RTDÂÆòÁΩë Ê≥®ÂÜåÂπ∂ÁôªÈôÜ ËøûÊé•GitHub ÈÄâÊã©Import a project ÂØºÂÖ•ÊàêÂäüÂêéÔºåÁÇπÂáªÈòÖËØªÊñáÊ°£Ôºå‰æøÂèØÁúãÂà∞ Web ÊïàÊûú‰∫Ü„ÄÇ ÂÆåÊàêÂà∞ËøôÈáåÁÆóÊòØÊàêÂäüÂï¶Ôºå‰Ω†ÂèØ‰ª•ÊâìÂºÄVScode Âú®Êú¨Âú∞ÂØπ ÊñáÊ°£ËøõË°åÁºñËæë„ÄÇÁºñËæëÂÆå‰πãÂêéÔºåÁî±‰∫éVSCodeÂÆâË£Ö‰∫ÜGit ÊâÄ‰ª•ÊîØÊåÅÂ∞ÜÊú¨Âú∞ÂÅöÁöÑÊõ¥ÊîπÂêåÊ≠•Âà∞GitHub‰ªìÂ∫ì„ÄÇÂóØÔºå Âïä¬∑¬∑ÁªìÊùüÔºÅ . ÂèÇËÄÉËµÑÊñô[1] http://avnpc.com/pages/writing-best-documentation-by-sphinx-github-readthedocsÂÜôÊúÄÂ•ΩÁöÑÊñáÊ°£ÔºöSphinx + Read the Docs [2] https://www.jianshu.com/p/78e9e1b8553aÂ¶Ç‰ΩïÁî® ReadtheDocs„ÄÅSphinx Âø´ÈÄüÊê≠Âª∫ÂÜô‰π¶ÁéØÂ¢É[3] http://www.sphinx-doc.org/en/stable/config.htmlThe build configuration file[4] https://github.com/vscode-restructuredtext/vscode-restructuredtext/blob/master/docs/sphinx.mdvscode-restructuredtext/vscode-restructuredtext]]></content>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>ÊïôÁ®ã</tag>
        <tag>VScode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ESP32-DevKitC ÂÖ•Èó®ÊåáÂçóÔºàËØëÔºâ]]></title>
    <url>%2F2017%2F12%2F14%2FESP32-DevKitC-%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97%EF%BC%88%E8%AF%91%EF%BC%89%2F</url>
    <content type="text"><![CDATA[ESP32-DevKitC ÂÖ•Èó®ÊåáÂçóËøôÁØáÊåáÂçóÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂÖ•Èó®ESP32-DevKitC ÂºÄÂèëÊùø ÈúÄË¶Å 1 √ó ESP32-DevKitC ÂºÄÂèëÊùø 1 √ó micro USB Êï∞ÊçÆÁ∫ø 1 √ó Ë£ÖÊúâ Windows,LinuxÊàñ Mac OSÁöÑÁîµËÑë Ê¶ÇËø∞ESP32-DevKitC ÊòØ‰∏Ä‰∏™Áî±‰πêÈë´ÂÖ¨Âè∏ÔºàEspressifÔºâÁîü‰∫ßÁöÑÁöÑÂ∞èÂûãESP32ÂºÄÂèëÊùø„ÄÇÂ§ßÈÉ®ÂàÜI/OÂºïËÑöÁî±‰∏§‰æßÊéíÈíàÂºïÂá∫‰ª•‰æø‰∫éËøûÊé•„ÄÇÂºÄÂèëËÄÖÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅÔºàas neededÔºâÂ∞ÜËøô‰∫õÂºïËÑöËøûÊé•Âà∞Â§ñÂõ¥ËÆæÂ§á„ÄÇÂΩì‰ΩøÁî®Èù¢ÂåÖÊùøÊó∂ÔºåÊ†áÂáÜÂåñÁöÑÊéíÈíà‰πü‰ª§ÂºÄÂèëÂèòÂæóÂÆπÊòì‰∏îÊñπ‰æø„ÄÇ ÂäüËÉΩËØ¥Êòé‰ª•‰∏ãÂàóË°®ÂíåÂõæË°®‰ªãÁªç‰∫ÜESP32-DevKitCÊùøÁöÑÂÖ≥ÈîÆÁªÑ‰ª∂„ÄÅÊé•Âè£ÂíåÊéßÂà∂ ESP-WROOM-32Ê†áÂáÜESP-WROOM-32 Ê®°ÂùóÁÑäÊé•Âú®ESP32-DevKitC Êùø ENÈáçÁΩÆÊåâÈíÆÔºöÊåâ‰∏ãÊ≠§ÊåâÈíÆÂèØ‰ª•ÈáçÁΩÆÁ≥ªÁªü Boot‰∏ãËΩΩÊåâÈíÆÔºöÊåâ‰ΩèBootÊåâÈíÆÂπ∂Êåâ‰∏ãENÊåâÈíÆÂàùÂßãÂåñÂõ∫‰ª∂‰∏ãËΩΩÊ®°Âºè„ÄÇÁÑ∂ÂêéÁî®Êà∑ÂèØ‰ª•ÈÄöËøá‰∏≤Âè£‰∏ãËΩΩÂõ∫‰ª∂„ÄÇ USBUSBÊé•Âè£„ÄÇÊòØÁªôÊùøÂ≠ê‰æõÁîµ‰ª•ÂèäESP-WROOM-32‰∏éPCÈÄö‰ø°ÁöÑÊé•Âè£„ÄÇ I/OESP-WROOM-32ÁöÑÂ§ßÈÉ®ÂàÜI/OÂºïËÑöÂ∑≤Áî±Êùø‰∏äÁöÑ‰∏§‰æßÊéíÈíàÂºïÂá∫„ÄÇÁî®Êà∑ÂèØ‰ª•ÂØπESP32ËøõË°åÁºñÁ®ãÊù•ÂÆûÁé∞ÂêÑÁßçÂäüËÉΩÔºåÊØîÂ¶ÇPWM,ADC,DAC,I2C,I2S,SPI,etc„ÄÇ Esp32-DevKitC ÂºÄÂèëÊùøÂ∏ÉÂ±Ä ‰æõÁîµÈÖçÁΩÆ‰ª•‰∏ãÈÄâÈ°πÂèØ‰ª•ÁªôESP32-PICO-KIT V4 ‰æõÁîµÔºö Micro USB Âè£„ÄÇÊ≠§‰∏∫ÈªòËÆ§ÁöÑ‰æõÁîµËøûÊé• 5V/GND ÈíàËÑö 3V3/GND ÈíàËÑö Ë≠¶ÂëäÔºö‰ª•‰∏äÈÄâÈ°πÊòØ‰∫íÊñ•ÁöÑÔºå‰πüÂ∞±ÊòØËØ¥‰æõÁîµÊñπÂºèÂè™ËÉΩÊòØ‰ª•‰∏äÂÖ∂‰∏≠‰∏ÄÁßç„ÄÇÂ∞ùËØïÂêåÊó∂‰ΩøÁî®Â§öÁßçËøûÊé•ÊñπÂºèÁªôÊùøÂ≠ê‰æõÁîµÂèØËÉΩ‰ºöÊçüÂùèÂºÄÂèëÊùøÊàñËÄÖÁîµÊ∫ê„ÄÇ ÂºÄÂßãÁºñÁ®ãÂú®ÁªôESP32-DevKitC ‰∏äÁîµ‰πãÂâçÔºåËØ∑Á°Æ‰øùÊùøÂ≠êÂÆåÂ•ΩÂπ∂Ê≤°ÊúâÊòéÊòæÁöÑÊçüÂùèËøπË±°„ÄÇË¶ÅÂºÄÂßãËøõË°åÂ∫îÁî®ÂºÄÂèëÔºåË∑≥ËΩ¨Âà∞Get Started ËäÇÔºåËøôÂ∞ÜÊåáÂØº‰Ω†ËøõË°å‰∏Ä‰∏ãÂá†Ê≠•Ôºö Âú®ÁîµËÑë‰∏äÂÆâË£ÖËøûÊé•Â∑•ÂÖ∑Ôºå‰ΩøÁî®CËØ≠Ë®ÄÁªôESP32ÂºÄÂèëÂ∫îÁî® Â∞ÜÊ®°ÂùóËøûÊé•Âà∞ÁîµËÑëÂπ∂È™åËØÅÊòØÂê¶ÂèØÁî® ÁªôESP32Âà∑ÂÖ•Á§∫‰æãÁ®ãÂ∫è ÁõëËßÜÂ∫îÁî®Âú®ÂÅö‰ªÄ‰πà ÂÖàÂç†Âùë ÊúâÁ©∫ÁøªÂÖ∂‰ªñÁöÑ]]></content>
      <categories>
        <category>IoT</category>
      </categories>
      <tags>
        <tag>ÁøªËØë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂÖ≥‰∫éÂçéÁ°ïÁ¨îËÆ∞Êú¨A400UËøõPEÔºåÊîπÂºÄÊú∫ÂØÜÁ†ÅÔºåÁ°¨ÁõòÊó†Ê≥ïËØÜÂà´ÁöÑÁ¨îËÆ∞]]></title>
    <url>%2F2017%2F10%2F10%2F%E5%85%B3%E4%BA%8E%E5%8D%8E%E7%A1%95%E7%AC%94%E8%AE%B0%E6%9C%ACA400U%E8%BF%9BPE%EF%BC%8C%E6%94%B9%E5%BC%80%E6%9C%BA%E5%AF%86%E7%A0%81%EF%BC%8C%E7%A1%AC%E7%9B%98%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%AB%E7%9A%84%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ÊúãÂèãÁöÑÁ¨îËÆ∞Êú¨ÁîµËÑëÂøò‰∫ÜÂºÄÊú∫ÂØÜÁ†ÅÔºåËÆ©ÊàëÁªôÊÉ≥ÊÉ≥ÂäûÊ≥ï„ÄÇÊàëÊü•‰∫ÜÂ∑≤ÊúâÁöÑÁôæÂ∫¶ÁªèÈ™åÔºå‰ΩÜÊòØËøáÁ®ãÂç¥‰∏ÄÊ≥¢‰∏âÊäò„ÄÇ‰∏ªË¶ÅÈóÆÈ¢òÊúâÔºöÊó†Ê≥ïËØÜÂà´UÁõòÔºåPEÁ≥ªÁªüÔºàwin8peÔºâÊó†Ê≥ïËØÜÂà´Á°¨Áõò;ÊâÄ‰ª•ÊàëÊâìÁÆóËÆ∞ÂΩï‰∏ãÊù•„ÄÇÊâÄÈúÄÂ∑•ÂÖ∑Ôºö1.Â§ßÁôΩËèúUÁõòÂêØÂä®ÁõòÂà∂‰ΩúÂ∑•ÂÖ∑ÔºàÊàëÁî®ÁöÑÊòØË£ÖÊú∫ÁâàÔºâ2.UÁõò Á¨¨1Ê≠• Âà∂‰ΩúUÁõòÂêØÂä®ÁõòÁï•ÔºàÂèÇËßÅÂ§ßÁôΩËèúÂÆòÁΩëÊïôÁ®ãÔºâ Á¨¨2Ê≠• ÈáçÂêØÁîµËÑë ËøõÂÖ•BIOS ÈÄâÊã©Advanced modeËøõÂÖ•BiosÁöÑÊñπÊ≥ïÊòØÊåâ‰∏ãÁîµÊ∫êÈîÆ‰πãÂêéÁãÇÁÇπ ESCÈîÆ Áõ¥Âà∞Âá∫Áé∞BootÈÄâÈ°πÔºåÁÑ∂ÂêéÈÄâÊã©Enter SetupÂç≥ÂèØ Á¨¨3Ê≠• ÈÄâÊã©AdvanceÔºåÁÇπÂáªSATA configuration Á¨¨4Ê≠• SATA ModeÊîπ‰∏∫AHCI Á¨¨5Ê≠• Â¶ÇÂõæ ÈÄâÊã©Secure Boot Á¨¨6Ê≠• Â¶ÇÂõæ Â∞Ü secure boot controlÊîπ‰∏∫ Disable Á¨¨7Ê≠• Â¶ÇÂõæ Â∞ÜCSMÊîπ‰∏∫ Enable Á¨¨8Ê≠• Â¶ÇÂõæ ‰øùÂ≠òÂπ∂ÈÄÄÂá∫ Á¨¨9Ê≠• ÊèíÂÖ•UÁõò ÈáçÂêØÁîµËÑë‰ªéUÁõòÂêØÂä®Â¶ÇÂõæÔºåÈÄâÊã©ÁÆ≠Â§¥ÊâÄÊåáÁöÑÈÇ£‰∏ÄÈïø‰∏≤ÔºåÂÖ∂‰∏≠Á¨¨‰∏Ä‰∏™ÊòØÊ≠£Â∏∏‰ªéÁ°¨ÁõòÂêØÂä®„ÄÇÂ¶ÇÊûúÈÄâÊã©‰Ω†ÁöÑUÁõòÂêçÁöÑËØù‰ºö‰∏ÄÁõ¥ÈªëÂ±èÔºå‰∏çÁü•ÈÅì‰∏∫‰ªÄ‰πà Á¨¨10Ê≠• Â¶ÇÂõæ Â¶ÇÂõæÂèØ‰ª•ÈáçÁΩÆwindowsÂºÄÊú∫ÂØÜÁ†Å ÂºÇÂ∏∏ÊÉÖÂÜµ ÔºöÁ°¨ÁõòÊó†Ê≥ïËØÜÂà´ÂéüÂõ†Â∞±ÊòØÊ≤°ÊúâËøõË°åÁ¨¨4Ê≠•ÁöÑÊìç‰Ωú Âà∞Ê≠§ÁªìÊùü]]></content>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>‰øÆÁîµËÑë2333</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UWPÂ∫îÁî®Ëé∑ÂèñHTTPËØ∑Ê±ÇËøîÂõûÁöÑMP3Èü≥È¢ëÊñá‰ª∂Ôºà‰ª•ÁôæÂ∫¶ËØ≠Èü≥ÂêàÊàêAPI‰∏∫‰æãÔºâ]]></title>
    <url>%2F2017%2F09%2F03%2FUWP%E5%BA%94%E7%94%A8%E8%8E%B7%E5%8F%96HTTP%E8%AF%B7%E6%B1%82%E8%BF%94%E5%9B%9E%E7%9A%84MP3%E9%9F%B3%E9%A2%91%E6%96%87%E4%BB%B6%EF%BC%88%E4%BB%A5%E7%99%BE%E5%BA%A6%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90API%E4%B8%BA%E4%BE%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Ëé∑ÂèñHTTPËØ∑Ê±ÇËøîÂõûÁöÑMP3Èü≥È¢ëÊñá‰ª∂Ôºà‰ª•ÁôæÂ∫¶ËØ≠Èü≥ÂêàÊàêAPI‰∏∫‰æãÔºâ‰ΩøÁî®ÁôæÂ∫¶ËØ≠Èü≥ÂêàÊàêAPIÊó∂Ôºå‰∏é‰ΩøÁî®‰∫∫ËÑ∏ËØÜÂà´APIÁöÑÊúÄÂ§ß‰∏çÂêå‰πãÂ§ÑÂú®‰∫éÔºö‰∫∫ËÑ∏ËØÜÂà´APIËøîÂõûÁªìÊûúÂùá‰∏∫JSONÊ†ºÂºèÁöÑÂ≠óÁ¨¶‰∏≤ÔºåËÄåËØ≠Èü≥ÂêàÊàêAPIÂàôÊòØËøîÂõûÁöÑ‰∫åËøõÂà∂ËØ≠Èü≥Êñá‰ª∂„ÄÇÂõ†Ê≠§Âú®‰ª£Á†ÅÁöÑÁºñÂÜô‰∏äÊúâÂæàÂ§ßÁöÑ‰∏çÂêå„ÄÇÂú®Ê≠§‰πãÂâçÊàë‰πü‰∏äÁΩëÊü•‰∫ÜÂ•ΩÂ§öÔºå‰ΩÜÊòØÊ≤°ÊúâÈíàÂØπUWPÁöÑÁ§∫‰æãÔºåÁªèËøá‰∏ÄÁï™Êë∏Á¥¢ÁÆóÊòØÊâæÂà∞‰∫ÜÂèØ‰ª•Áî®ÁöÑÂäûÊ≥ï (ÔΩûÔø£‚ñΩÔø£)ÔΩû‰∏ãÈù¢Áõ¥Êé•ÂÜôÂá∫Ê†∏ÂøÉ‰ª£Á†Å‰ª•‰æõÊù•Êó•Â§ç‰π†ÂèÇËÄÉ„ÄÇÊ≥®ÔºöÁ§∫‰æã‰ª£Á†ÅÂùáÊòØ‰ªéuwpÈ°πÁõÆ‰∏≠Êà™ÂèñÁöÑÔºå‰øùÁïô‰∫ÜÂºÇÊ≠•Áî®Ê≥ïÔºå‰∏çÊ∏ÖÊ•öÁöÑËØùÂèØ‰ª•Â§ßËá¥ÂÖà‰∫ÜËß£‰∏Ä‰∏ãÂºÇÊ≠•ÁºñÁ®ã„ÄÇ‰ΩøÁî®Âà∞ÁöÑÂëΩÂêçÁ©∫Èó¥Ôºö12345using Windows.Web.Http;//HttpClientÊâÄÂ±ûusing System.Threading.Tasks;//TaskÊâÄÂ±ûusing Windows.Storage.Streams;//IBufferÊâÄÂ±ûusing Windows.Media.Playback;//MediaPlayerÊâÄÂ±ûusing Windows.Media.Core;//MediaSourceÊâÄÂ±û ËøôÊòØËØ∑Ê±ÇÁöÑÂèÇÊï∞ÂàóË°®Ôºö123456789101112List&lt;KeyValuePair&lt;String, String&gt;&gt; paralist = new List&lt;KeyValuePair&lt;string, string&gt;&gt; &#123; new KeyValuePair&lt;string, string&gt;("tex",tex_seting), new KeyValuePair&lt;string, string&gt;("lan",lan_setting), new KeyValuePair&lt;string, string&gt;("tok",access_Token), new KeyValuePair&lt;string, string&gt;("ctp",ctp_setting), new KeyValuePair&lt;string, string&gt;("cuid","cuid"), new KeyValuePair&lt;string, string&gt;("spd",spd_setting), new KeyValuePair&lt;string, string&gt;("pit",pit_setting), new KeyValuePair&lt;string, string&gt;("vol",vol_setting), new KeyValuePair&lt;string, string&gt;("per",per_setting) &#125;; Ëé∑Âèñcontent ÂÜÖÂÆπÔºö12345678910private async Task&lt;IBuffer&gt; GetTtsResultAsync(string url, List&lt;KeyValuePair&lt;String, String&gt;&gt; list)// Ëé∑ÂèñÊñá‰ª∂ &#123; HttpClient hc = new HttpClient(); using (var content = new HttpFormUrlEncodedContent(list)) &#123; var response = await hc.PostAsync(new Uri(url), content); IBuffer buffer = await response.Content.ReadAsBufferAsync(); return buffer; &#125; &#125; Ê≠§Â§ÑÁöÑÂÖ≥ÈîÆÁÇπÂú®‰∫é ‰ΩøÁî®1ReadAsBufferAsync() Êù•Â∞Ücontent‰∏≠ÁöÑÂÜÖÂÆπËØªÂèñ‰∏∫ IBuffer 12345678910111213private async void PlayAudio(IBuffer result)//ÊúÄÁªàÊí≠ÊîæÈü≥È¢ë &#123; folder = await KnownFolders.MusicLibrary.CreateFolderAsync("Greeting", CreationCollisionOption.ReplaceExisting);//ÂàõÂª∫Êñá‰ª∂Â§π StorageFile x = await folder.CreateFileAsync("ËØ≠Èü≥Êñá‰ª∂.mp3", CreationCollisionOption.ReplaceExisting);//ÂàõÂª∫Êñá‰ª∂ StorageFile storageFile = await folder.GetFileAsync("ËØ≠Èü≥Êñá‰ª∂.mp3"); await FileIO.WriteBufferAsync(storageFile, result);//‰ªéÁºìÂÜ≤ÂÜôÂÖ•Êñá‰ª∂ // storageFile = await folder.GetFileAsync("ËØ≠Èü≥Êñá‰ª∂.mp3"); MediaPlayer _mediaPlayer = new MediaPlayer(); _mediaPlayer.Source = MediaSource.CreateFromStorageFile(storageFile); _mediaPlayer.Play(); &#125; Â∞Ücontent‰∏≠ÁöÑÂÜÖÂÆπËØªÂèñ‰∏∫ÁºìÂÜ≤Á±ªÂûã‰πãÂêéÔºåÂú®ÂÜôÂÖ•Êñá‰ª∂Êó∂Â∞±ÂèØ‰ª•‰ΩøÁî®1FileIO.WriteBufferAsync() ÊñπÊ≥ïÊù•Â∞ÜÊï∞ÊçÆ‰ªéÁºìÂÜ≤Âå∫ÂÜôÂÖ•Êñá‰ª∂‰∫Ü„ÄÇÂõ†‰∏∫Ê≠§Â§Ñ‰∏∫mp3Ê†ºÂºèÔºåÊâÄ‰ª•Âú®ÂàõÂª∫Êñá‰ª∂Êó∂ÔºåÈúÄË¶ÅÂä†‰∏äÂêéÁºÄÂêç.mp3 1StorageFile x = await folder.CreateFileAsync("ËØ≠Èü≥Êñá‰ª∂.mp3", CreationCollisionOption.ReplaceExisting); ÂÖ∂‰ªñËé∑ÂèñÂÖ∂‰ªñÁ±ªÂûãÁöÑÊñá‰ª∂‰πüÂèØ‰ª•ÂêåÁêÜÂêß :-)]]></content>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>UWP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Â¶Ç‰ΩïÂÖçË¥πËé∑Âèñwindows10ÂºÄÂèëËÄÖË¥¶Êà∑Ôºà‰∏™‰∫∫Ôºâ]]></title>
    <url>%2F2017%2F08%2F31%2F%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E8%8E%B7%E5%8F%96windows10%E5%BC%80%E5%8F%91%E8%80%85%E8%B4%A6%E6%88%B7%EF%BC%88%E4%B8%AA%E4%BA%BA%EF%BC%89%2F</url>
    <content type="text"><![CDATA[ÂÖçË¥πËé∑Âèñ windows10 ÂºÄÂèëËÄÖË¥¶Êà∑Ôºà‰∏™‰∫∫ÔºâÂõ†‰∏∫Ë¶ÅÂ≠¶‰π†ÂºÄÂèëuwpÂ∫îÁî®ÁöÑÁºòÊïÖÔºåÊâÄ‰ª•‰∏çÂæó‰∏çÊ≥®ÂÜå‰∏Ä‰∏™windows10ÁöÑÂºÄÂèëËÄÖË¥¶Êà∑„ÄÇ‰ΩÜÊòØËÉñËôéÁúâÂ§¥‰∏ÄÁö±ÂèëÁé∞Ê≠§‰∫ãÂπ∂‰∏çÁÆÄÂçï„ÄÇÈ¶ñÂÖàÔºåÊúâÂæÆËΩØÁöÑË¥¶Êà∑‰∏ç‰ª£Ë°®Â∞±ÊúâÂºÄÂèëËÄÖË¥¶Êà∑Ôºå‰∏§ËÄÖÂπ∂‰∏çÊòØ‰∏ÄÂõû‰∫ãÂÑøÔºåÂè™ÊÄ™ÂΩìÂàùÂæàÂÇªÂæàÂ§©ÁúüÔºõÂÖ∂Ê¨°ÔºåÂºÄÂèëËÄÖË¥¶Êà∑Âπ∂‰∏çÊòØfreeÁöÑÔºå‰∏™‰∫∫ÁâàË¶ÅËä±116CNYÔºà‰∫∫Ê∞ëÂ∏ÅÔºüÔºâÔºå‰ºÅ‰∏öÁâàÈúÄË¶Å600CNYÔºõÂÜçËÄÖÔºåÂ∞±ÁÆóË¶ÅË¥≠‰π∞ÔºåÂ±ÖÁÑ∂‰πüÂè™ËÉΩ‰ΩøÁî®‰ø°Áî®Âç°‰ªòÊ¨æÔºå[ÂõæÁâá]ÔºåÁÑ∂ËÄåÊàëÂè™ÊúâAlipay„ÄÇÁΩë‰∏äÂ∑≤ÁªèÊúâÂæàÂ§öÂà©Áî®Â≠¶Ê†°ÈÇÆÁÆ±Ëé∑ÂèñÂÖçË¥πÊ≥®ÂÜåÁ†ÅÁöÑÊñπÊ≥ï‰∫ÜÔºåÂè™ÊòØÂÆÉ‰ª¨ÁöÑÂèëÂ∏ÉÊó∂Èó¥Â∑≤ÁªèÂæàÊó©‰∫ÜÔºå Dream Spark ÁΩëÁ´ô‰πüÂ∑≤ÁªèÊîπÁâà„ÄÇÊàëËøôÁØáÂçöÂÆ¢ÁÆóÊòØÂÜô‰∏Ä‰∏™Êñ∞ÁâàÁöÑÂêß„ÄÇ ÂºÄÂßãÂâçÁöÑÂáÜÂ§á ÂèØÁî®ÁöÑÂæÆËΩØË¥¶Êà∑ Â≠¶ÁîüÂºÄÂèë‰∫∫ÂëòÂ∑•ÂÖ∑,ËµÑÊ∫êÂíå‰ΩìÈ™å | Imagine Windows ÂºÄÂèë‰∫∫Âëò‰∏≠ÂøÉ ÂèØÁî®ÁöÑÊ†°Âõ≠ÈÇÆÁÆ± ÊØîÂ¶ÇÔºö abc@Â≠¶Ê†°Ê†°Âêç.edu.cn ÔºöËøô‰∏™‰∏çÂ§™Â•ΩÂºÑÔºåËßÜÂÖ∑‰ΩìÂ≠¶Ê†°ÊÉÖÂÜµËÄåÂÆöÔºåÂ¶ÇÊûúÊú¨Ê†°Âú®Ê†°Áîü‰∏çÂÖÅËÆ∏Ê≥®ÂÜåÊ†°Âõ≠ÈÇÆÁÆ±ÔºàÂú®Êàë‰ª¨Â≠¶Ê†°Ê†°Âõ≠ÈÇÆÁÆ±Âè™ËÉΩÊïôËÅåÂ∑•Ê≥®ÂÜåÔºåÂ≠¶ÁîüÊ†πÊú¨‰∏çÁªôÊ≥®ÂÜåÁöÑÔºâÔºåÂèØ‰ª•ÂÄüÁî®ÂÖ∂‰ªñÂ≠¶Ê†°ÂêåÂ≠¶ÁöÑÂ≠¶Ê†°ÈÇÆÁÆ±Êù•Áî®„ÄÇÂÖ∂ÂÆû‰∏çÁÆ°ÊòØ‰∏çÊòØÂ≠¶ÁîüÔºåÂè™Ë¶ÅÊúâÂ≠¶Ê†°ÈÇÆÁÆ±Â∞±ÂèØ‰ª•ËøõË°åÂ≠¶ÁîüËÆ§ËØÅÁöÑÔºå‰Ω†‰ΩøÁî®ÁöÑÈÇÆÁÆ±ÊâÄÂ±ûÁöÑÂ≠¶Ê†°‰∏é‰Ω†Â°´ÂÜôÁöÑÂ≠¶Ê†°‰ø°ÊÅØÁîöËá≥ÂèØ‰ª•‰∏çÂêåÔºåËøôÊòØ‰∫≤ÊµãÔºåÊàëÂ∞±ÊòØËøôÊ†∑Â≠ê„ÄÇ Ê≠•È™§1.Ëé∑ÂèñÊ≥®ÂÜå‰ª£Á†Å ‰ΩøÁî®ÂæÆËΩØË¥¶Êà∑ÁôªÂΩï Imagine Â°´ÂÜô‰∏™‰∫∫‰ª•ÂèäÂ≠¶Ê†°‰ø°ÊÅØ ÈÄâÊã©Â≠¶ÁîüÈ™åËØÅÊñπÂºè ÁôªÂΩïÂ≠¶Ê†°ÈÇÆÁÆ±Êé•Êî∂È™åËØÅÈÇÆ‰ª∂ ÂÆåÊàêÈ™åËØÅ ÂõûÂà∞‰∏ªÈ°µÁÇπÂáªÂØºËà™Ê†è‰∏≠ÁöÑ‰∏ãËΩΩ‚ÜíËΩØ‰ª∂ÁõÆÂΩï ÊâæÂà∞Â¶ÇÂõæÊ†èÁõÆÔºö ‰∏ãÈù¢Êúâ‰º†ÈÄÅÈó®Ëé∑ÂèñwindowsÂ∫îÁî®ÂïÜÂ∫ó‰ª£Á†Å Ê≠•È™§2.Ê≥®ÂÜåÂºÄÂèëËÄÖË¥¶Êà∑ ËøõÂÖ•ÂºÄÂèë‰∫∫Âëò‰∏≠ÂøÉ ÁôªÂΩïÂêéÁÇπÂáªÊ≥®ÂÜå‰º†ÈÄÅÈó®ÔºöÊ≥®ÂÜåÊàê‰∏∫Â∫îÁî®ÂºÄÂèë‰∫∫Âëò Â°´ÂÜôË°®Ê†º ÔºåË¥¶Êà∑Á±ªÂûãÂΩìÁÑ∂ÊòØÈÄâÊã©‰∏™‰∫∫ Á≤òË¥¥‰ª£Á†Å ÂÆåÊàêÂñΩ]]></content>
      <categories>
        <category>ÊùÇË¥ßÈì∫</category>
      </categories>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>ÊïôÁ®ã</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows10IoT+Ê†ëËéìÊ¥æÂÆòÊñπÊëÑÂÉèÂ§¥ÔºàPi CamaraÔºâ == ÊöÇÊó∂Êó†Ëß£]]></title>
    <url>%2F2017%2F06%2F29%2Fwindows10IoT%2B%E6%A0%91%E8%8E%93%E6%B4%BE%E5%AE%98%E6%96%B9%E6%91%84%E5%83%8F%E5%A4%B4%EF%BC%88Pi-Camara%EF%BC%89-%3D%3D-%E6%9A%82%E6%97%B6%E6%97%A0%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[ÊöÇ‰∏çË¶Å‰ΩøÁî®windows10IoT+Ê†ëËéìÊ¥æÂÆòÊñπÊëÑÂÉèÂ§¥ÂéüÂõ†Â¶ÇÂõæÔºö ‰∏ªË¶ÅÊÑèÊÄùÂ∞±ÊòØÔºåÁî±‰∫éPi CamaraÊòØÈÄöËøáGPUÂ≠êÁ≥ªÁªüËøûÊé•Âà∞Á≥ªÁªüÁöÑÔºå‰ΩÜÊòØÊìç‰ΩúÁ≥ªÁªüÂπ∂‰∏çÊîØÊåÅËøôÁßçËøûÊé•ÔºåÂÆòÊñπËøò‰∏çËÉΩÁªôÂá∫‰ªª‰ΩïËß£ÂÜ≥ÂäûÊ≥ï„ÄÇ‰ΩÜÊòØÂ∞ÜÊù•‰ºöÊîØÊåÅÁöÑ„ÄÇËøôÊòØwin10IoTÂÆòÊñπÂºÄÂèëÂõ¢ÈòüÂÅöÂá∫ÁöÑÂõûÂ§çÊâÄ‰ª•Âè™ËÉΩÈÄâÊã©ÂéüË∞Ö‰ªñ‰∫Ü¬∑¬∑¬∑¬∑Âìá¬∑¬∑¬∑ÁúüÂùë¬∑¬∑¬∑¬∑ ËØ¶ÊÉÖÁÇπÂáªÔºöMSDNÂéüÊñáÈìæÊé•]]></content>
      <categories>
        <category>Ê†ëËéìÊ¥æ</category>
        <category>IoT</category>
      </categories>
      <tags>
        <tag>ÂéüÂàõ</tag>
        <tag>Ê†ëËéìÊ¥æ</tag>
        <tag>windows10Iot</tag>
      </tags>
  </entry>
</search>
