<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-corner-indicator.min.css?v=1.0.2">

















  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5">







<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    fastclick: false,
    lazyload: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="多读书读好书">
<meta property="og:type" content="website">
<meta property="og:title" content="结果元素">
<meta property="og:url" content="http://cooguyan.github.io/index.html">
<meta property="og:site_name" content="结果元素">
<meta property="og:description" content="多读书读好书">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="结果元素">
<meta name="twitter:description" content="多读书读好书">






  <link rel="canonical" href="http://cooguyan.github.io/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>结果元素</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">结果元素</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">Resualt Element</h1>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">31</span></a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类<span class="badge">15</span></a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">45</span></a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2019/01/30/5_Parallel_Memory_System/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/30/5_Parallel_Memory_System/" class="post-title-link" itemprop="url">5_Parallel_Memory_System</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-30 10:38:48 / 修改时间：12:49:33" itemprop="dateCreated datePublished" datetime="2019-01-30T10:38:48+08:00">2019-01-30</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/GPU相关/" itemprop="url" rel="index"><span itemprop="name">GPU相关</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Parallel-Memory-System"><a href="#Parallel-Memory-System" class="headerlink" title="Parallel Memory System"></a>Parallel Memory System</h1><p>Outside of the GPU itself, the memory subsystem is the most important determiner of the performance of a graphics system. Graphics workloads demand very high transfer rates to and from memory. Pixel write and blend (read-modifywrite) operations, depth buffer reads and writes, and texture map reads, as well as command and object vertex and attribute data reads, comprise the majority of memory traffic.<br>在GPU本身之外，存储器子系统是图形系统性能的最重要的决定因素。 图形工作负载需要非常高的内存传输速率。 像素写入和混合（读取 - 修改写入）操作，深度缓冲区读取和写入以及纹理映射读取以及命令和对象顶点和属性数据读取构成了大部分内存流量。</p>
<p>Modern GPUs are highly parallel, as shown in Figure C.2.5. For example, the GeForce 8800 can process 32 pixels per clock, at 600 MHz. Each pixel typically requires a color read and write and a depth read and write of a 4-byte pixel. Usually an average of two or three texels of four bytes each are read to generate the pixel’s color. So for a typical case, there is a demand of 28 bytes times 32 pixels = 896 bytes per clock. Clearly the bandwidth demand on the memory system is enormous.<br>现代GPU高度并行，如图C.2.5所示。 例如，GeForce 8800每时钟可处理32个像素，频率为600 MHz。 每个像素通常需要颜色读取和写入以及4字节像素的深度读取和写入。 通常，读取平均每个四个字节的两个或三个纹素，以生成像素的颜色。 因此，对于典型情况，需要28个字节乘以32个像素=每个时钟896个字节。 显然，存储系统的带宽需求是巨大的。</p>
<p>To supply these requirements, GPU memory systems have the following characteristics:<br>为满足这些要求，GPU内存系统具有以下特征：</p>
<ul>
<li><p>They are wide, meaning there are a large number of pins to convey data between the GPU and its memory devices, and the memory array itself comprises many DRAM chips to provide the full total data bus width. 它们很宽，意味着有大量的引脚在GPU和它的存储器设备之间传送数据，并且存储器阵列本身包括许多DRAM芯片以提供完整的总数据总线宽度。</p>
</li>
<li><p>They are fast, meaning aggressive signaling techniques are used to maximize the data rate (bits/second) per pin. 它们很快，意味着积极的信令技术用于最大化每个引脚的数据速率（位/秒）。</p>
</li>
<li><p>GPUs seek to use every available cycle to transfer data to or from the memory array. To achieve this, GPUs specifcally do not aim to minimize latency to the memory system. High throughput (utilization efciency) and short latency are fundamentally in conﬂict. GPU寻求使用每个可用周期来将数据传输到存储器阵列或从存储器阵列传输数据。 为实现此目的，GPU特别不旨在最小化对存储器系统的延迟。 高吞吐量（利用效率）和短延迟基本上是冲突的。</p>
</li>
<li><p>Compression techniques are used, both lossy, of which the programmer must be aware, and lossless, which is invisible to the application and opportunistic. 使用压缩技术，既有损耗，程序员必须知道，无损，对应用程序和机会主义是不可见的。</p>
</li>
<li><p>Caches and work coalescing structures are used to reduce the amount of offchip traffic needed and to ensure that cycles spent moving data are used as fully as possible. 高速缓存和工作合并结构用于减少所需的片外流量，并确保尽可能充分地使用移动数据的周期。</p>
</li>
</ul>
<h2 id="DRAM-Considerations-DRAM注意事项"><a href="#DRAM-Considerations-DRAM注意事项" class="headerlink" title="DRAM Considerations DRAM注意事项"></a>DRAM Considerations DRAM注意事项</h2><p>GPUs must take into account the unique characteristics of DRAM. DRAM chips are internally arranged as multiple (typically four to eight) banks, where each bank includes a power-of-2 number of rows (typically around 16,384), and each row contains a power-of-2 number of bits (typically 8192). DRAMs impose a variety of timing requirements on their controlling processor. For example, dozens of cycles are required to activate one row, but once activated, the bits within that row are randomly accessible with a new column address every four clocks. Double-data rate (DDR) synchronous DRAMs transfer data on both rising and falling edges of the interface clock (see Chapter 5). So a 1 GHz clocked DDR DRAM transfers data at 2 gigabits per second per data pin. Graphics DDR DRAMs usually have 32 bidirectional data pins, so eight bytes can be read or written from the DRAM per clock.<br>GPU必须考虑DRAM的独特特性。 DRAM芯片内部排列为多个（通常为四到八个）存储体，其中每个存储体包括2个幂的行数（通常约为16,384个），每行包含2个幂的位数（通常为8192个））。 DRAM对其控制处理器施加了各种时序要求。 例如，激活一行需要几十个周期，但一旦激活，该行中的位可随机访问，每四个时钟使用一个新的列地址。 双倍数据速率（DDR）同步DRAM在接口时钟的上升沿和下降沿传输数据（见第5章）。 因此，1 GHz时钟DDR DRAM以每个数据引脚每秒2千兆位的速度传输数据。 图形DDR DRAM通常具有32个双向数据引脚，因此每个时钟可以从DRAM读取或写入8个字节。</p>
<p>GPUs internally have a large number of generators of memory traffic. Different stages of the logical graphics pipeline each have their own request streams: command and vertex attribute fetch, shader texture fetch and load/store, and pixel depth and color read-write. At each logical stage, there are ofen multiple independent units to deliver the parallel throughput. These are each independent memory requestors. When viewed at the memory system, there are an enormous number of uncorrelated requests in ﬂight. This is a natural mismatch to the reference pattern preferred by the DRAMs. A solution is for the GPU’s memory controller to maintain separate heaps of trafc bound for diﬀerent DRAM banks, and wait until enough traffic for a particular DRAM row is pending before activating that row and transferring all the trafc at once. Note that accumulating pending requests, while good for DRAM row locality and thus efcient use of the data bus, leads to longer average latency as seen by the requestors whose requests spend time waiting for others. The design must take care that no particular request waits too long, otherwise some processing units can starve waiting for data and ultimately cause neighboring processors to become idle.<br>GPU内部具有大量内存流量生成器。逻辑图形管道的不同阶段各自具有其自己的请求流：命令和顶点属性获取，着色器纹理获取和加载/存储，以及像素深度和颜色读写。在每个逻辑阶段，有多个独立单元来提供并行吞吐量。这些是每个独立的内存请求者。在内存系统中查看时，飞行中存在大量不相关的请求。这与DRAM优选的参考图案自然不匹配。一种解决方案是GPU的内存控制器维护绑定到不同DRAM库的单独的流量堆，并等待特定DRAM行的足够流量待激活，然后激活该行并立即传输所有流量。请注意，累积待处理请求虽然有利于DRAM行位置并因此有效地使用数据总线，但会导致请求者花费时间等待其他请求者的平均延迟时间更长。设计必须注意没有特定请求等待太长时间，否则一些处理单元可能会饿死等待数据并最终导致相邻处理器变为空闲。</p>
<p>GPU memory subsystems are arranged as multiple memory partitions, each of which comprises a fully independent memory controller and one or two DRAM devices that are fully and exclusively owned by that partition. To achieve the best load balance and therefore approach the theoretical performance of n partitions, addresses are fnely interleaved evenly across all memory partitions. The partition interleaving stride is typically a block of a few hundred bytes. The number of memory partitions is designed to balance the number of processors and other memory requesters.<br>GPU存储器子系统被布置为多个存储器分区，每个存储器分区包括完全独立的存储器控制器和由该分区完全和专有地拥有的一个或两个DRAM设备。 为了实现最佳负载平衡并因此接近n个分区的理论性能，地址在所有内存分区上均匀地交错。 分区交织步幅通常是几百字节的块。 内存分区的数量旨在平衡处理器和其他内存请求者的数量。</p>
<h2 id="Caches"><a href="#Caches" class="headerlink" title="Caches"></a>Caches</h2><p>GPU workloads typically have very large working sets—on the order of hundreds of megabytes to generate a single graphics frame. Unlike with CPUs, it is not practical to construct caches on chips large enough to hold anything close to the full working set of a graphics application. Whereas CPUs can assume very high cache hit rates (99.9% or more), GPUs experience hit rates closer to 90% and must therefore cope with many misses in ﬂight. While a CPU can reasonably be designed to halt while waiting for a rare cache miss, a GPU needs to proceed with misses and hits intermingled. We call this a streaming cache architecture.<br>GPU工作负载通常具有非常大的工作集 - 大约数百兆字节以生成单个图形帧。 与CPU不同，在足够大的芯片上构建缓存以保存靠近图形应用程序的完整工作集的任何东西都是不实际的。 虽然CPU可以承担非常高的缓存命中率（99.9％或更高），但GPU的命中率接近90％，因此必须应对飞行中的许多未命中。 虽然CPU可以合理地设计为在等待罕见的高速缓存未命中时停止，但是GPU需要继续进行未命中和命中混合。 我们称之为流缓存架构。</p>
<p>GPU caches must deliver very high-bandwidth to their clients. Consider the case of a texture cache. A typical texture unit may evaluate two bilinear interpolations for each of four pixels per clock cycle, and a GPU may have many such texture units all operating independently. Each bilinear interpolation requires four separate texels, and each texel might be a 64-bit value. Four 16-bit components are typical. Thus, total bandwidth is 2 x 4 x 4 x 64 = 2048 bits per clock. Each separate 64-bit texel is independently addressed, so the cache needs to handle 32 unique addresses per clock. This naturally favors a multibank and/or multiport arrangement of SRAM arrays.<br>GPU缓存必须为其客户提供非常高的带宽。 考虑纹理缓存的情况。 典型的纹理单元可以针对每个时钟周期的四个像素中的每一个评估两个双线性插值，并且GPU可以具有全部独立操作的许多这样的纹理单元。 每个双线性插值需要四个单独的纹素，每个纹素可能是64位值。 通常有四个16位组件。 因此，总带宽是每时钟2×4×4×64 = 2048比特。 每个独立的64位纹素都是独立寻址的，因此缓存需要每个时钟处理32个唯一地址。 这自然有利于SRAM阵列的多库和/或多端口布置。</p>
<h2 id="MMU"><a href="#MMU" class="headerlink" title="MMU"></a>MMU</h2><p>Modern GPUs are capable of translating virtual addresses to physical addresses. On the GeForce 8800, all processing units generate memory addresses in a 40-bit virtual address space. For computing, load and store thread instructions use 32-bit byte addresses, which are extended to a 40-bit virtual address by adding a 40-bit offset. A memory management unit performs virtual to physical address translation; hardware reads the page tables from local memory to respond to misses on behalf of a hierarchy of translation lookaside buffers spread out among the processors and rendering engines. In addition to physical page bits, GPU page table entries specify the compression algorithm for each page. Page sizes range from 4 to 128 kilobytes.<br>现代GPU能够将虚拟地址转换为物理地址。 在GeForce 8800上，所有处理单元都在40位虚拟地址空间中生成内存地址。 对于计算，加载和存储线程指令，使用32位字节地址，通过添加40位偏移量将其扩展为40位虚拟地址。 存储器管理单元执行虚拟到物理地址转换; 硬件从本地存储器读取页表以代表在处理器和呈现引擎之间展开的转换后备缓冲器的层次结构来响应未命中。 除了物理页面位之外，GPU页表条目还为每个页面指定压缩算法。 页面大小范围为4到128千字节。</p>
<h2 id="Memory-Spaces"><a href="#Memory-Spaces" class="headerlink" title="Memory Spaces"></a>Memory Spaces</h2><p>As introduced in Section C.3, CUDA exposes different memory spaces to allow the programmer to store data values in the most performance-optimal way. For the following discussion, NVIDIA Tesla architecture GPUs are assumed.<br>如C.3节所述，CUDA公开了不同的内存空间，以允许程序员以最佳性能最佳的方式存储数据值。 对于以下讨论，假设使用NVIDIA Tesla架构GPU。</p>
<h2 id="Global-memory"><a href="#Global-memory" class="headerlink" title="Global memory"></a>Global memory</h2><p>Global memory is stored in external DRAM; it is not local to any one physical streaming multiprocessor (SM) because it is meant for communication among different CTAs (thread blocks) in different grids. In fact, the many CTAs that reference a location in global memory may not be executing in the GPU at the same time; by design, in CUDA a programmer does not know the relative order in which CTAs are executed. Because the address space is evenly distributed among all memory partitions, there must be a read/write path from any streaming multiprocessor to any DRAM partition.</p>
<p>1399/5000<br>全局存储器存储在外部DRAM中;它不是任何一个物理流多处理器（SM）的本地，因为它用于不同网格中不同CTA（线程块）之间的通信。实际上，引用全局存储器中的位置的许多CTA可能不会同时在GPU中执行;按照设计，在CUDA中，程序员不知道执行CTA的相对顺序。由于地址空间均匀分布在所有内存分区中，因此必须存在从任何流式多处理器到任何DRAM分区的读/写路径。</p>
<p>Access to global memory by different threads (and diﬀerent processors) is not guaranteed to have sequential consistency. Thread programs see a relaxed memory ordering model. Within a thread, the order of memory reads and writes to the same address is preserved, but the order of accesses to different addresses may not be preserved. Memory reads and writes requested by different threads are unordered. Within a CTA, the barrier synchronization instruction bar.sync can be used to obtain strict memory ordering among the threads of the CTA. The membar thread instruction provides a memory barrier/fence operation that commits prior memory accesses and makes them visible to other threads before proceeding. Threads can also use the atomic memory operations described in Section C.4 to coordinate work on memory they share.<br>不同线程（和不同的处理器）对全局内存的访问不保证具有顺序一致性。 线程程序看到放松的内存排序模型。 在线程内，保留了对同一地址的内存读取和写入顺序，但可能无法保留对不同地址的访问顺序。 不同线程请求的内存读取和写入是无序的。 在CTA中，屏障同步指令bar.sync可用于在CTA的线程之间获得严格的内存排序。 membar线程指令提供了一个内存屏障/围栅操作，它提交先前的内存访问，并使其在继续之前对其他线程可见。 线程还可以使用第C.4节中描述的原子内存操作来协调它们共享的内存的工作。</p>
<h2 id="Shared-memory"><a href="#Shared-memory" class="headerlink" title="Shared memory"></a>Shared memory</h2><p>Per-CTA shared memory is only visible to the threads that belong to that CTA, and shared memory only occupies storage from the time a CTA is created to the time it terminates. Shared memory can therefore reside on-chip. This approach has many benefits. First, shared memory traffic does not need to compete with limited off-chip bandwidth needed for global memory references. Second, it is practical to build very high-bandwidth memory structures on-chip to support the read/write demands of each streaming multiprocessor. In fact, the shared memory is closely coupled to the streaming multiprocessor<br>Per-CTA共享内存仅对属于该CTA的线程可见，共享内存仅占用从创建CTA到终止时的存储。 因此，共享存储器可以驻留在芯片上。 这种方法有很多好处。 首先，共享内存流量不需要与全局内存引用所需的有限片外带宽竞争。 其次，在片上构建非常高带宽的存储器结构以支持每个流多处理器的读/写需求是实用的。 实际上，共享存储器与流式多处理器紧密耦合</p>
<p>Each streaming multiprocessor contains eight physical thread processors. During one shared memory clock cycle, each thread processor can process two threads’ worth of instructions, so 16 threads’ worth of shared memory requests must be handled in each clock. Because each thread can generate its own addresses, and the addresses are typically unique, the shared memory is built using 16 independently addressable SRAM banks. For common access patterns, 16 banks are sufcient to maintain throughput, but pathological cases are possible; for example, all 16 threads might happen to access a different address on one SRAM bank. It must be possible to route a request from any thread lane to any bank of SRAM, so a 16-by-16 interconnection network is required.<br>每个流式多处理器包含八个物理线程处理器。 在一个共享内存时钟周期内，每个线程处理器可以处理两个线程的指令，因此必须在每个时钟中处理16个线程的共享内存请求。 由于每个线程都可以生成自己的地址，并且地址通常是唯一的，因此共享内存使用16个可独立寻址的SRAM bank构建。 对于常见的访问模式，16个银行足以维持吞吐量，但病理情况是可能的; 例如，所有16个线程可能碰巧访问一个SRAM组上的不同地址。 必须可以将来自任何线程通道的请求路由到任何SRAM组，因此需要16×16的互连网络。</p>
<h2 id="Local-Memory"><a href="#Local-Memory" class="headerlink" title="Local Memory"></a>Local Memory</h2><p>Per-thread local memory is private memory visible only to a single thread. Local memory is architecturally larger than the thread’s register file, and a program can compute addresses into local memory. To support large allocations of local memory (recall the total allocation is the per-thread allocation times the number of active threads), local memory is allocated in external DRAM. Although global and per-thread local memory reside oﬀ-chip, they are wellsuited to being cached on-chip.<br>每线程本地内存是仅对单个线程可见的私有内存。 本地存储器在体系结构上比线程的寄存器文件大，并且程序可以将地址计算到本地存储器中。 为了支持本地内存的大量分配（调用总分配是每线程分配乘以活动线程数），本地内存分配在外部DRAM中。 虽然全局和每线程本地内存驻留在芯片上，但它们非常适合在片上缓存。</p>
<h2 id="Constant-Memory"><a href="#Constant-Memory" class="headerlink" title="Constant Memory"></a>Constant Memory</h2><p>Constant memory is read-only to a program running on the SM (it can be written via commands to the GPU). It is stored in external DRAM and cached in the SM. Because commonly most or all threads in a SIMT warp read from the same address in constant memory, a single address lookup per clock is sufcient. The constant cache is designed to broadcast scalar values to threads in each warp.<br>常量存储器对SM上运行的程序是只读的（可以通过命令写入GPU）。 它存储在外部DRAM中并缓存在SM中。 因为SIMT warp中的大多数或所有线程通常从常量存储器中的相同地址读取，所以每个时钟的单个地址查找是足够的。 常量缓存旨在向每个warp中的线程广播标量值。</p>
<h2 id="Texture-Memory"><a href="#Texture-Memory" class="headerlink" title="Texture Memory"></a>Texture Memory</h2><p>Texture memory holds large read-only arrays of data. Textures for computing have the same attributes and capabilities as textures used with 3D graphics. Although textures are commonly two-dimensional images (2D arrays of pixel values), 1D (linear) and 3D (volume) textures are also available.<br>纹理内存包含大量只读数据数组。 用于计算的纹理具有与用于3D图形的纹理相同的属性和能力。 虽然纹理通常是二维图像（像素值的2D阵列），但也可以使用1D（线性）和3D（体积）纹理。</p>
<p>A compute program references a texture using a tex instruction. Operands include an identifer to name the texture, and 1, 2, or 3 coordinates based on the texture dimensionality. Te ﬂoating-point coordinates include a fractional portion that specifes a sample location, ofen in between texel locations. Noninteger coordinates invoke a bilinear weighted interpolation of the four closest values (for a 2D texture) before the result is returned to the program.<br>计算程序使用tex指令引用纹理。 操作数包括用于命名纹理的标识符，以及基于纹理维度的1,2或3个坐标。 浮点坐标包括指定样本位置的小数部分，位于纹素位置之间。 在将结果返回到程序之前，非整数坐标调用四个最接近值（对于2D纹理）的双线性加权插值。</p>
<p>Texture fetches are cached in a streaming cache hierarchy designed to optimize throughput of texture fetches from thousands of concurrent threads. Some programs use texture fetches as a way to cache global memory.<br>纹理提取缓存在流缓存层次结构中，旨在优化来自数千个并发线程的纹理提取的吞吐量。 一些程序使用纹理提取作为缓存全局内存的方法。</p>
<h2 id="Surfaces"><a href="#Surfaces" class="headerlink" title="Surfaces"></a>Surfaces</h2><p>Surface is a generic term for a one-dimensional, two-dimensional, or threedimensional array of pixel values and an associated format. A variety of formats are defned; for example, a pixel may be defned as four 8-bit RGBA integer components, or four 16-bit ﬂoating-point components. A program kernel does not need to know the surface type. A tex instruction recasts its result values as ﬂoating-point, depending on the surface format.</p>
<h2 id="Load-Store-Access"><a href="#Load-Store-Access" class="headerlink" title="Load/Store Access"></a>Load/Store Access</h2><p>Load/store instructions with integer byte addressing enable the writing and compiling of programs in conventional languages like C and C++. CUDA programs use load/store instructions to access memory.<br>带有整数字节寻址的加载/存储指令可以用C和C ++等传统语言编写和编译程序。 CUDA程序使用加载/存储指令来访问内存。</p>
<p>To improve memory bandwidth and reduce overhead, the local and global load/store instructions coalesce individual parallel thread requests from the same warp together into a single memory block request when the addresses fall in the same block and meet alignment criteria. Coalescing individual small memory requests into large block requests provides a signifcant performance boost over separate requests. Te large thread count, together with support for many outstanding load requests, helps cover load-to-use latency for local and global memory implemented in external DRAM.<br>为了改善存储器带宽并减少开销，当地址落入同一块并满足对齐标准时，本地和全局加载/存储指令将来自相同warp的各个并行线程请求合并为单个存储器块请求。 将单个小内存请求合并到大块请求中可以显着提升单独请求的性能。 大线程数以及对许多未完成的负载请求的支持有助于覆盖外部DRAM中实现的本地和全局内存的负载使用延迟。</p>
<h2 id="ROP"><a href="#ROP" class="headerlink" title="ROP"></a>ROP</h2><p>As shown in Figure C.2.5, NVIDIA Tesla architecture GPUs comprise a scalable streaming processor array (SPA), which performs all of the GPU’s programmabl calculations, and a scalable memory system, which comprises external DRAM control and fxed function Raster Operation Processors (ROPs) that perform color and depth framebuffer operations directly on memory. Each ROP unit is paired with a specifc memory partition. ROP partitions are fed from the SMs via an interconnection network. Each ROP is responsible for depth and stencil tests and updates, as well as color blending. The ROP and memory controllers cooperate to implement lossless color and depth compression (up to 8:1) to reduce external bandwidth demand. ROP units also perform atomic operations on memory.<br>如图C.2.5所示，NVIDIA Tesla架构GPU包括一个可扩展的流处理器阵列（SPA），它执行所有GPU的programmabl计算，以及一个可扩展的存储器系统，包括外部DRAM控制和固定功能光栅操作处理器（ROP） ）直接在内存上执行颜色和深度帧缓冲操作。 每个ROP单元与特定的内存分区配对。 ROP分区通过互连网络从SM馈送。 每个ROP负责深度和模板测试和更新，以及颜色混合。 ROP和内存控制器协同工作，实现无损色彩和深度压缩（高达8：1），以减少外部带宽需求。 ROP单元还对内存执行原子操作。</p>

          
        
      
    </div>

    
      

  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\27\Multithreaded_Multiprocessor_Architecture\" rel="bookmark">4_Multithreaded_Multiprocessor_Architecture</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\24\Programming_GPUs\" rel="bookmark">3_Programming_GPUs</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\23\GPU_System_Architectures\" rel="bookmark">2_GPU_System_Architectures</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\11\Graphics_and_Computing_GPUs\" rel="bookmark">1_图形和计算GPU</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2018\05\05\VSCode插件入门\" rel="bookmark">VSCode插件开发</a></div>
      
    </li>
  
  </ul>


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2019/01/27/Multithreaded_Multiprocessor_Architecture/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/27/Multithreaded_Multiprocessor_Architecture/" class="post-title-link" itemprop="url">4_Multithreaded_Multiprocessor_Architecture</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-27 08:07:34" itemprop="dateCreated datePublished" datetime="2019-01-27T08:07:34+08:00">2019-01-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-30 10:46:42" itemprop="dateModified" datetime="2019-01-30T10:46:42+08:00">2019-01-30</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/GPU相关/" itemprop="url" rel="index"><span itemprop="name">GPU相关</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>机器翻译，未校对</p>
<h1 id="Multithreaded-Multiprocessor-Architecture-多线程多处理器体系结构"><a href="#Multithreaded-Multiprocessor-Architecture-多线程多处理器体系结构" class="headerlink" title="Multithreaded Multiprocessor Architecture 多线程多处理器体系结构"></a>Multithreaded Multiprocessor Architecture 多线程多处理器体系结构</h1><p>To address different market segments, GPUs implement scalable numbers of multiprocessors—in fact, GPUs are multiprocessors composed of multiprocessors. Furthermore, each multiprocessor is highly multithreaded to execute many fnegrained vertex and pixel shader threads efciently. A quality basic GPU has two to four multiprocessors, while a gaming enthusiast’s GPU or computing platform has dozens of them. This section looks at the architecture of one such multithreaded multiprocessor, a simplifed version of the NVIDIA Tesla streaming multiprocessor (SM) described in Section C.7.<br>为了解决不同的细分市场，GPU实现了可扩展数量的多处理器 - 实际上，GPU是由多处理器组成的多处理器。此外，每个多处理器都是高度多线程的，以便有效地执行许多fnegrained顶点和像素着色器线程。高质量的基本GPU有两到四个多处理器，而游戏爱好者的GPU或计算平台有几十个。本节介绍一个这样的多线程多处理器的体系结构，这是第C.7节中描述的NVIDIA Tesla流多处理器（SM）的简化版本。</p>
<p>Why use a multiprocessor, rather than several independent processors? The parallelism within each multiprocessor provides localized high performance and supports extensive multithreading for the fine-grained parallel programming models described in Section C.3. Te individual threads of a thread block execute together within a multiprocessor to share data. The multithreaded multiprocessor design we describe here has eight scalar processor cores in a tightly coupled architecture, and executes up to 512 threads (the SM described in Section C.7 executes up to 768 threads). For area and power efciency, the multiprocessor shares large complex units among the eight processor cores, including the instruction cache, the multithreaded instruction unit, and the shared memory RAM.<br>为什么要使用多处理器，而不是几个独立的处理器？每个多处理器内的并行性提供了本地化的高性能，并支持C.3节中描述的细粒度并行编程模型的广泛多线程。线程块的各个线程在多处理器内一起执行以共享数据。我们在这里描述的多线程多处理器设计在紧密耦合的架构中有八个标量处理器内核，并执行多达512个线程（C.7节中描述的SM执行多达768个线程）。对于面积和功率效率，多处理器在八个处理器内核之间共享大型复杂单元，包括指令高速缓存，多线程指令单元和共享内存RAM。</p>
<h2 id="Massive-Multithreading"><a href="#Massive-Multithreading" class="headerlink" title="Massive Multithreading"></a>Massive Multithreading</h2><p>GPU processors are highly multithreaded to achieve several goals:<br>GPU处理器是高度多线程的，以实现几个目标：</p>
<ul>
<li>Cover the latency of memory loads and texture fetches from DRAM 覆盖DRAM中存储器负载和纹理提取的延迟</li>
<li>Support fine-grained parallel graphics shader programming models 支持细粒度并行图形着色器编程模型</li>
<li>Support fine-grained parallel computing programming models 支持细粒度并行计算编程模型</li>
<li>Virtualize the physical processors as threads and thread blocks to provide transparent scalability 将物理处理器虚拟化为线程和线程块，以提供透明的可伸缩性</li>
<li>Simplify the parallel programming model to writing a serial program for one thread 简化并行编程模型，为一个线程编写串行程序</li>
</ul>
<p>Memory and texture fetch latency can require hundreds of processor clocks, because GPUs typically have small streaming caches rather than large working-set caches like CPUs. A fetch request generally requires a full DRAM access latency plus interconnect and buﬀering latency. Multithreading helps cover the latency with useful computing—while one thread is waiting for a load or texture fetch to complete, the processor can execute another thread. The fne-grained parallel programming models provide literally thousands of independent threads that can keep many processors busy despite the long memory latency seen by individual threads.<br>内存和纹理提取延迟可能需要数百个处理器时钟，因为GPU通常具有小型流缓存，而不是像CPU那样的大型工作集缓存。 获取请求通常需要完整的DRAM访问延迟以及互连和缓冲延迟。 多线程有助于通过有用的计算来弥补延迟 - 当一个线程正在等待加载或纹理提取完成时，处理器可以执行另一个线程。 这些细粒度的并行编程模型提供了数千个独立的线程，即使各个线程看到的内存延迟很长，也可以使许多处理器保持忙碌状态。</p>
<p>A graphics vertex or pixel shader program is a program for a single thread that processes a vertex or a pixel. Similarly, a CUDA program is a C program for a single thread that computes a result. Graphics and computing programs instantiate many parallel threads to render complex images and compute large result arrays. To dynamically balance shifing vertex and pixel shader thread workloads, each multiprocessor concurrently executes multiple different thread programs and different types of shader programs.<br>图形顶点或像素着色器程序是用于处理顶点或像素的单个线程的程序。 类似地，CUDA程序是用于计算结果的单个线程的C程序。 图形和计算程序实例化许多并行线程以渲染复杂图像并计算大型结果数组。 为了动态平衡shifing顶点和像素着色器线程工作负载，每个多处理器同时执行多个不同的线程程序和不同类型的着色器程序。</p>
<p>To support the independent vertex, primitive, and pixel programming model of graphics shading languages and the single-thread programming model of CUDA C/C++, each GPU thread has its own private registers, private per-thread memory, program counter, and thread execution state, and can execute an independent code path. To efciently execute hundreds of concurrent lightweight threads, the GPU multiprocessor is hardware multithreaded—it manages and executes hundreds of concurrent threads in hardware without scheduling overhead. Concurrent threads within thread blocks can synchronize at a barrier with a single instruction. Lightweight thread creation, zero-overhead thread scheduling, and fast barrier synchronization effciently support very fne-grained parallelism.<br>为了支持图形着色语言的独立顶点，原始和像素编程模型以及CUDA C / C ++的单线程编程模型，每个GPU线程都有自己的私有寄存器，私有每线程内存，程序计数器和线程执行状态，可以执行独立的代码路径。 为了有效地执行数百个并发轻量级线程，GPU多处理器是硬件多线程的 - 它在硬件中管理和执行数百个并发线程，而无需调度开销。 线程块内的并发线程可以使用单个指令在屏障上同步。 轻量级线程创建，零开销线程调度和快速屏障同步有效地支持非常细粒度的并行性。</p>
<h2 id="Multiprocessor-Architecture-多处理器架构"><a href="#Multiprocessor-Architecture-多处理器架构" class="headerlink" title="Multiprocessor Architecture 多处理器架构"></a>Multiprocessor Architecture 多处理器架构</h2><p>A unifed graphics and computing multiprocessor executes vertex, geometry, and pixel fragment shader programs, and parallel computing programs. As Figure C.4.1 shows, the example multiprocessor consists of eight scalar processor (SP) cores each with a large multithreaded register fle (RF), two special function units (SFUs), a multithreaded instruction unit, an instruction cache, a read-only constant cache,and a shared memory.<br>统一的图形和计算多处理器执行顶点，几何和像素片段着色器程序以及并行计算程序。 如图C.4.1所示，示例多处理器由8个标量处理器（SP）内核组成，每个内核具有一个大型多线程寄存器（RF），两个特殊功能单元（SFU），一个多线程指令单元，一个指令高速缓存，一个读取器。 只有常量缓存和共享内存。</p>
<p>The 16 KB shared memory holds graphics data buﬀers and shared computing data. CUDA variables declared as __shared__ reside in the shared memory. To map the logical graphics pipeline workload through the multiprocessor multiple times, as shown in Section C.2, vertex, geometry, and pixel threads have independent input and output buffers, and workloads arrive and depart independently of thread execution.<br>16 KB共享内存可存储图形数据缓冲区和共享计算数据。 声明为__shared__ 的CUDA变量驻留在共享内存中。 若要多次映射逻辑图形管道工作负载通过多处理器，如第C.2节所示，顶点，几何和像素线程具有独立的输入和输出缓冲区，并且工作负载独立于线程执行而到达和离开。</p>
<p>Each SP core contains scalar integer and ﬂoating-point arithmetic units that execute most instructions. The SP is hardware multithreaded, supporting up to 64 threads. Each pipelined SP core executes one scalar instruction per thread per clock, which ranges from 1.2 GHz to 1.6 GHz in diﬀerent GPU products. Each SP core has a large RF of 1024 general-purpose 32-bit registers, partitioned among its assigned threads. Programs declare their register demand, typically 16 to 64 scalar 32-bit registers per thread. The SP can concurrently run many threads that use a few registers or fewer threads that use more registers. The compiler optimizes register allocation to balance the cost of spilling registers versus the cost of fewer threads. Pixel shader programs ofen use 16 or fewer registers, enabling each SP to run up to 64 pixel shader threads to cover long-latency texture fetches. Compiled CUDA programs ofen need 32 registers per thread, limiting each SP to 32 threads, which limits such a kernel program to 256 threads per thread block on this example multiprocessor, rather than its maximum of 512 threads.<br>每个SP内核包含执行大多数指令的标量整数和浮点运算单元。 SP是硬件多线程，最多支持64个线程。每个流水线SP核心每个时钟每个线程执行一个标量指令，在不同的GPU产品中，范围从1.2 GHz到1.6 GHz。每个SP内核都有一个1024个通用32位寄存器的大RF，在其分配的线程之间进行分区。程序声明它们的寄存器需求，通常每个线程有16到64个标量32位寄存器。 SP可以同时运行许多线程，这些线程使用少量寄存器或更少使用更多寄存器的线程。编译器优化寄存器分配以平衡溢出寄存器的成本与更少线程的成本。像素着色器程序使用16个或更少的寄存器，使每个SP能够运行多达64个像素着色器线程，以覆盖长延迟纹理提取。编译的CUDA程序每个线程需要32个寄存器，将每个SP限制为32个线程，这在这个示例多处理器上将每个线程块的内核程序限制为256个线程，而不是最多512个线程。</p>
<p><img src="https://cooguyan-1252810309.cos.ap-chengdu.myqcloud.com/Multithreaded_Multiprocessor_Architecture/20190127104839586.png" alt=""></p>
<p>The pipelined SFUs execute thread instructions that compute special functions and interpolate pixel attributes from primitive vertex attributes. These instructions can execute concurrently with instructions on the SPs. The SFU is described later.<br>流水线SFU执行计算特殊函数的线程指令，并从原始顶点属性插入像素属性。 这些指令可以与SP上的指令同时执行。 SFU将在后面描述。</p>
<p>The multiprocessor executes texture fetch instructions on the texture unit via the texture interface, and uses the memory interface for external memory load, store, and atomic access instructions. These instructions can execute concurrently with instructions on the SPs. Shared memory access uses a low-latency interconnection network between the SP processors and the shared memory banks.<br>多处理器通过纹理接口在纹理单元上执行纹理获取指令，并使用存储器接口进行外部存储器加载，存储和原子访问指令。 这些指令可以与SP上的指令同时执行。 共享内存访问使用SP处理器和共享内存库之间的低延迟互连网络。</p>
<h2 id="Single-Instruction-Multiple-Thread-SIMT"><a href="#Single-Instruction-Multiple-Thread-SIMT" class="headerlink" title="Single-Instruction Multiple-Thread (SIMT)"></a>Single-Instruction Multiple-Thread (SIMT)</h2><p>To manage and execute hundreds of threads running several different programs effciently, the multiprocessor employs a single-instruction multiple-thread (SIMT) architecture. It creates, manages, schedules, and executes concurrent threads in groups of parallel threads called warps. The term warp originates from weaving, the first parallel thread technology. The photograph in Figure C.4.2 shows a warp of parallel threads emerging from a loom. This example multiprocessor uses a SIMT warp size of 32 threads, executing four threads in each of the eight SP cores over four clocks. The Tesla SM multiprocessor described in Section C.7 also uses a warp size of 32 parallel threads, executing four threads per SP core for effciency on plentiful pixel threads and computing threads. Thread blocks consist of one or more warps.<br>为了有效地管理和执行运行多个不同程序的数百个线程，多处理器采用单指令多线程（SIMT）架构。 它在称为warps的并行线程组中创建，管理，调度和执行并发线程。 术语warp源于编织，这是第一个并行线程技术。 图C.4.2中的照片显示了从织机中出现的平行线的翘曲。 此示例多处理器使用32个线程的SIMT warp大小，在四个时钟内的八个SP内核中的每一个中执行四个线程。 第C.7节中描述的Tesla SM多处理器还使用32个并行线程的warp大小，每个SP核心执行四个线程，以便在丰富的像素线程和计算线程上实现效率。 线程块由一个或多个warp组成。<br><img src="https://cooguyan-1252810309.cos.ap-chengdu.myqcloud.com/Multithreaded_Multiprocessor_Architecture/20190127095423525.png" alt=""></p>
<p>This example SIMT multiprocessor manages a pool of 16 warps, a total of 512 threads. Individual parallel threads composing a warp are the same type and start together at the same program address, but are otherwise free to branch and execute independently. At each instruction issue time, the SIMT multithreaded instruction unit selects a warp that is ready to execute its next instruction, and then issues that instruction to the active threads of that warp. A SIMT instruction is broadcast synchronously to the active parallel threads of a warp; individual threads may be inactive due to independent branching or predication. In this multiprocessor, each SP scalar processor core executes an instruction for four individual threads of a warp using four clocks, reﬂecting the 4:1 ratio of warp threads to cores.</p>
<blockquote>
<p>single-instruction multiple-thread (SIMT): A processor architecture that applies one instruction to multiple independent threads in parallel. </p>
</blockquote>
<blockquote>
<p>warp: The set of parallel threads that execute the same instruction together in a SIMT architecture.</p>
</blockquote>
<p>SIMT processor architecture is akin to single-instruction multiple data (SIMD) design, which applies one instruction to multiple data lanes, but diﬀers in that SIMT applies one instruction to multiple independent threads in parallel, not just to multiple data lanes. An instruction for a SIMD processor controls a vector of<br>multiple data lanes together, whereas an instruction for a SIMT processor controls an individual thread, and the SIMT instruction unit issues an instruction to a warp of independent parallel threads for efciency. Te SIMT processor fnds data-level parallelism among threads at runtime, analogous to the way a superscalar processor finds instruction-level parallelism among instructions at runtime.<br>此示例SIMT多处理器管理一个包含16个warp的池，总共512个线程。 组成warp的各个并行线程是相同的类型，并且在相同的程序地址处一起开始，但是可以独立地分支和执行。 在每个指令发布时，SIMT多线程指令单元选择准备执行其下一条指令的warp，然后将该指令发布到该warp的活动线程。 SIMT指令与warp的活动并行线程同步广播; 由于独立的分支或预测，各个线程可能不活动。 在这个多处理器中，每个SP标量处理器内核使用四个时钟执行一个经线的四个单独线程的指令，将经线的4：1比率反映到内核。</p>
<p>A SIMT processor realizes full efciency and performance when all threads of a warp take the same execution path. If threads of a warp diverge via a datadependent conditional branch, execution serializes for each branch path taken, and when all paths complete, the threads converge to the same execution path. For equal length paths, a divergent if-else code block is 50% efcient. The multiprocessor uses a branch synchronization stack to manage independent threads that diverge and converge. Diﬀerent warps execute independently at full speed regardless of whether they are executing common or disjoint code paths. As a result, SIMT GPUs are dramatically more efcient and ﬂexible on branching code than earlier GPUs, as their warps are much narrower than the SIMD width of prior GPUs.<br>当warp的所有线程采用相同的执行路径时，SIMT处理器可实现完全的效率和性能。 如果warp的线程通过数据相关的条件分支发散，则执行为所采用的每个分支路径进行序列化，并且当所有路径完成时，线程会聚到相同的执行路径。 对于等长路径，发散的if-else代码块效率为50％。 多处理器使用分支同步堆栈来管理分散和聚合的独立线程。 不同的warp全速独立执行，无论它们是执行公共还是不相交的代码路径。 因此，SIMT GPU在分支代码上比早期GPU更加高效和灵活，因为它们的经线比先前GPU的SIMD宽度窄得多。</p>
<p>In contrast with SIMD vector architectures, SIMT enables programmers to write thread-level parallel code for individual independent threads, as well as data-parallel code for many coordinated threads. For program correctness, the programmer can essentially ignore the SIMT execution attributes of warps; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge. In practice, this is analogous to the role of cache lines in traditional codes: cache line size can be safely ignored when designing for correctness but must be considered in the code structure when designing for peak performance.<br>与SIMD向量体系结构相比，SIMT使程序员能够为各个独立线程编写线程级并行代码，并为许多协调线程编写数据并行代码。 对于程序的正确性，程序员基本上可以忽略warp的SIMT执行属性; 然而，通过注意代码很少需要经线中的线程发散，可以实现显着的性能改进。 实际上，这类似于传统代码中缓存行的作用：在设计正确性时可以安全地忽略缓存行大小，但在设计峰值性能时必须在代码结构中考虑。</p>
<h2 id="SIMT-Warp-Execution-and-Divergence-SIMT-Warp-执行和分歧"><a href="#SIMT-Warp-Execution-and-Divergence-SIMT-Warp-执行和分歧" class="headerlink" title="SIMT Warp Execution and Divergence SIMT Warp 执行和分歧 ??"></a>SIMT Warp Execution and Divergence SIMT Warp 执行和分歧 ??</h2><p>The SIMT approach of scheduling independent warps is more ﬂexible than the scheduling of previous GPU architectures. A warp comprises parallel threads of the same type: vertex, geometry, pixel, or compute. The basic unit of pixel fragment shader processing is the 2-by-2 pixel quad implemented as four pixel shader threads. The multiprocessor controller packs the pixel quads into a warp. It similarly groups vertices and primitives into warps, and packs computing threads into a warp. A thread block comprises one or more warps. The SIMT design shares the instruction fetch and issue unit efciently across parallel threads of a warp, but requires a full warp of active threads to get full performance effciency.<br>调度独立warp的SIMT方法比先前GPU架构的调度更灵活。 扭曲包括相同类型的并行线程：顶点，几何，像素或计算。 像素片段着色器处理的基本单位是实现为四个像素着色器线程的2×2像素四边形。 多处理器控制器将像素四边形打包成扭曲。 它类似地将顶点和基元分组为warp，并将计算线程打包成warp。 线程块包括一个或多个warp。 SIMT设计在warp的并行线程之间有效地共享指令获取和发布单元，但需要完整的活动线程warp才能获得完全的性能效率。</p>
<p>This unifed multiprocessor schedules and executes multiple warp types concurrently, allowing it to concurrently execute vertex and pixel warps. Its warp scheduler operates at less than the processor clock rate, because there are four thread lanes per processor core. During each scheduling cycle, it selects a warp to execute a SIMT warp instruction, as shown in Figure C.4.2. An issued warp-instruction executes as four sets of eight threads over four processor cycles of throughput. The processor pipeline uses several clocks of latency to complete each instruction. If the number of active warps times the clocks per warp exceeds the pipeline latency, the programmer can ignore the pipeline latency. For this multiprocessor, a round-robin schedule of eight warps has a period of 32 cycles between successive instructions for the same warp. If the program can keep 256 threads active per multiprocessor, instruction latencies up to 32 cycles can be hidden from an individual sequential thread. However, with few active warps, the processor pipeline depth becomes visible and may cause processors to stall.<br>这个统一的多处理器同时调度和执行多个warp类型，允许它同时执行顶点和像素warp。它的warp调度程序以低于处理器时钟速率运行，因为每个处理器内核有四个线程通道。在每个调度周期中，它选择一个warp来执行SIMT warp指令，如图C.4.2所示。发出的warp-instruction在吞吐量的四个处理器周期内作为四组八个线程执行。处理器流水线使用几个延迟时钟来完成每条指令。如果每个warp的时钟的活动warp数乘以管道延迟，则程序员可以忽略管道延迟。对于这种多处理器，八个warp的循环调度在相同warp的连续指令之间具有32个周期的周期。如果程序可以在每个多处理器中保持256个线程处于活动状态，则可以从单个顺序线程中隐藏最多32个周期的指令延迟。但是，由于几乎没有活动warp，处理器管道深度变得可见，并可能导致处理器停止。</p>
<p>A challenging design problem is implementing zero-overhead warp scheduling for a dynamic mix of diﬀerent warp programs and program types. The instruction scheduler must select a warp every four clocks to issue one instruction per clock per thread, equivalent to an IPC of 1.0 per processor core. Because warps are independent, the only dependences are among sequential instructions from the same warp. The scheduler uses a register dependency scoreboard to qualify warps whose active threads are ready to execute an instruction. It prioritizes all such ready warps and selects the highest priority one for issue. Prioritization must consider warp type, instruction type, and the desire to be fair to all active warps.<br>一个具有挑战性的设计问题是为不同的warp程序和程序类型的动态组合实现零开销warp调度。 指令调度程序必须每四个时钟选择一个warp，每个线程每个时钟发出一条指令，相当于每个处理器内核的IPC为1.0。 由于warp是独立的，唯一的依赖是来自同一warp的顺序指令。 调度程序使用寄存器依赖性记分板来限定其活动线程已准备好执行指令的warp。 它优先考虑所有这些准备好的warp并选择最优先的warp。 优先级必须考虑warp类型，指令类型以及对所有活动warp公平的愿望。</p>
<h2 id="Managing-Threads-and-Thread-Blocks-管理线程和线程块"><a href="#Managing-Threads-and-Thread-Blocks-管理线程和线程块" class="headerlink" title="Managing Threads and Thread Blocks 管理线程和线程块"></a>Managing Threads and Thread Blocks 管理线程和线程块</h2><p>The multiprocessor controller and instruction unit manage threads and thread blocks. The controller accepts work requests and input data and arbitrates access to shared resources, including the texture unit, memory access path, and I/O paths. For graphics workloads, it creates and manages three types of graphics threads concurrently: vertex, geometry, and pixel. Each of the graphics work types has independent input and output paths. It accumulates and packs each of these input work types into SIMT warps of parallel threads executing the same thread program. It allocates a free warp, allocates registers for the warp threads, and starts warp execution in the multiprocessor. Every program declares its perthread register demand; the controller starts a warp only when it can allocate the requested register count for the warp threads. When all the threads of the warp exit, the controller unpacks the results and frees the warp registers and resources.<br>多处理器控制器和指令单元管理线程和线程块。 控制器接受工作请求和输入数据，并仲裁对共享资源的访问，包括纹理单元，内存访问路径和I / O路径。 对于图形工作负载，它同时创建和管理三种类型的图形线程：顶点，几何和像素。 每个图形工作类型都有独立的输入和输出路径。 它将这些输入工作类型中的每一个累积并打包到执行相同线程程序的并行线程的SIMT warp中。 它分配一个自由warp，为warp线程分配寄存器，并在多处理器中启动warp执行。 每个程序都声明其寄存器需求; 只有当控制器可以为经线分配所请求的寄存器计数时，控制器才会启动warp。 当warp的所有线程退出时，控制器将解压缩结果并释放warp寄存器和资源。</p>
<p>The controller creates cooperative thread arrays (CTAs) which implement CUDA thread blocks as one or more warps of parallel threads. It creates a CTA when it can create all CTA warps and allocate all CTA resources. In addition to threads and registers, a CTA requires allocating shared memory and barriers. The program declares the required capacities, and the controller waits until it can allocate those amounts before launching the CTA. Then it creates CTA warps at the warp scheduling rate, so that a CTA program starts executing immediately at full multiprocessor performance. The controller monitors when all threads of a CTA have exited, and frees the CTA shared resources and its warp resources.<br>控制器创建协作线程阵列（CTA），其将CUDA线程块实现为一个或多个并行线程的warp。 它可以在创建所有CTA warp并分配所有CTA资源时创建CTA。 除线程和寄存器外，CTA还需要分配共享内存和障碍。 程序声明所需的容量，控制器等待，直到它可以在启动CTA之前分配这些数量。 然后它以warp调度速率创建CTA warp，以便CTA程序在完全多处理器性能时立即开始执行。 Te控制器监视CTA的所有线程何时退出，并释放CTA共享资源及其warp资源。</p>
<blockquote>
<p>cooperative thread array (CTA) : A set of concurrent threads that executes the same thread program and may cooperate to compute a result. A GPU CTA implements a CUDA thread block. 一组并发线程，它们执行相同的线程程序并可协作计算结果。 GPU CTA实现了CUDA线程块。</p>
</blockquote>
<h2 id="Thread-Instructions-线程指令"><a href="#Thread-Instructions-线程指令" class="headerlink" title="Thread Instructions 线程指令"></a>Thread Instructions 线程指令</h2><p>The SP thread processors execute scalar instructions for individual threads, unlike earlier GPU vector instruction architectures, which executed four-component vector instructions for each vertex or pixel shader program. Vertex programs generally compute (x, y, z, w) position vectors, while pixel shader programs compute (red, green, blue, alpha) color vectors. However, shader programs are becoming longer and more scalar, and it is increasingly difcult to fully occupy even two components of a legacy GPU four-component vector architecture. In effect, the SIMT architecture parallelizes across 32 independent pixel threads, rather than parallelizing the four vector components within a pixel. CUDA C/C++ programs have predominantly scalar code per thread. Previous GPUs employed vector packing (e.g., combining subvectors of work to gain efciency) but that complicated the scheduling hardware as well as the compiler. Scalar instructions are simpler and compiler friendly. Texture instructions remain vector based, taking a source coordinate vector and returning a filtered color vector.<br>SP线程处理器执行各个线程的标量指令，不像早期的GPU矢量指令架构，后者为每个顶点或像素着色器程序执行四分量矢量指令。顶点程序通常计算（x，y，z，w）位置矢量，而像素着色器程序计算（红色，绿色，蓝色，alpha）颜色矢量。然而，着色器程序变得越来越长并且越来越标量化，并且甚至完全占据传统GPU四分量矢量体系结构的两个组件也变得越来越困难。实际上，SIMT架构在32个独立像素线程之间并行化，而不是并行化像素内的四个矢量分量。 CUDA C / C ++程序每个线程主要有标量代码。先前的GPU采用向量打包（例如，组合工作的子向量以获得效率）但是使调度硬件以及编译器复杂化。标量指令更简单，编译友好。纹理指令保持基于矢量，采用源坐标向量并返回滤波后的颜色向量。</p>
<p>To support multiple GPUs with diﬀerent binary microinstruction formats, highlevel graphics and computing language compilers generate intermediate assemblerlevel instructions (e.g., Direct3D vector instructions or PTX scalar instructions), which are then optimized and translated to binary GPU microinstructions. The NVIDIA PTX (parallel thread execution) instruction set defnition [2007] provides a stable target ISA for compilers, and provides compatibility over several generations of GPUs with evolving binary microinstruction-set architectures. The optimizer readily expands Direct3D vector instructions to multiple scalar binary microinstructions. PTX scalar instructions translate nearly one to one with scalar binary microinstructions, although some PTX instructions expand to multiple binary microinstructions, and multiple PTX instructions may fold into one binary microinstruction. Because the intermediate assembler-level instructions use virtual registers, the optimizer analyzes data dependencies and allocates real registers. The optimizer eliminates dead code, folds instructions together when feasible, and optimizes SIMT branch diverge and converge points.<br>为了支持具有不同二进制微指令格式的多个GPU，高级图形和计算语言编译器生成中间汇编级指令（例如，Direct3D向量指令或PTX标量指令），然后将其优化并转换为二进制GPU微指令。 NVIDIA PTX（并行线程执行）指令集定义[2007]为编译器提供了稳定的目标ISA，并提供了几代GPU与不断发展的二进制微指令集架构的兼容性。优化器很容易将Direct3D向量指令扩展为多个标量二进制微指令。尽管一些PTX指令扩展到多个二进制微指令，但PTX标量指令几乎一对一地转换为标量二进制微指令，并且多个PTX指令可折叠成一个二进制微指令。由于中间汇编程序级指令使用虚拟寄存器，优化程序会分析数据依赖性并分配实际寄存器。优化器消除了死代码，在可行时将指令折叠在一起，并优化SIMT分支发散和收敛点。</p>
<h2 id="Instruction-Set-Architecture-ISA-指令集架构"><a href="#Instruction-Set-Architecture-ISA-指令集架构" class="headerlink" title="Instruction Set Architecture (ISA) 指令集架构"></a>Instruction Set Architecture (ISA) 指令集架构</h2><p>The thread ISA described here is a simplifed version of the Tesla architecture PTX ISA, a register-based scalar instruction set comprising ﬂoating-point, integer, logical, conversion, special functions, ﬂow control, memory access, and texture operations. Figure C.4.3 lists the basic PTX GPU thread instructions; see the NVIDIA PTX specifcation [2007] for details. The instruction format is:<br>这里描述的线程ISA是特斯拉架构PTX ISA的简化版本，这是一种基于寄存器的标量指令集，包括浮点，整数，逻辑，转换，特殊函数，流控制，存储器访问和纹理操作。 图C.4.3列出了基本的PTX GPU线程指令; 有关详细信息，请参阅NVIDIA PTX规范[2007]。 指令格式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">opcode.type d, a, b, c;</span><br></pre></td></tr></table></figure>
<p>where <em>d</em> is the destination operand, a, b, c are source operands, and .type is one of:<br>其中<em>d</em> 是目标操作数，a，b，c是源操作数，而.type是以下之一：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Type</th>
<th style="text-align:center">.type Specifer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Untyped bits 8, 16, 32, and 64 bits</td>
<td style="text-align:center">.b8, .b16, .b32, .b64</td>
</tr>
<tr>
<td style="text-align:center">Unsigned integer 8, 16, 32, and 64 bits</td>
<td style="text-align:center">.u8, .u16, .u32, .u64</td>
</tr>
<tr>
<td style="text-align:center">Signed integer 8, 16, 32, and 64 bits</td>
<td style="text-align:center">.s8, .s16, .s32, .s64</td>
</tr>
<tr>
<td style="text-align:center">Floating-point 16, 32, and 64 bits</td>
<td style="text-align:center">.f16, .f32, .f64</td>
</tr>
</tbody>
</table>
<p><img src="https://cooguyan-1252810309.cos.ap-chengdu.myqcloud.com/Multithreaded_Multiprocessor_Architecture/20190127102843734.png" alt=""></p>
<p>Source operands are scalar 32-bit or 64-bit values in registers, an immediate value, or a constant; predicate operands are 1-bit Boolean values. Destinations are registers, except for store to memory. Instructions are predicated by prefxing them with @p or @!p, where p is a predicate register. Memory and texture instructions transfer scalars or vectors of two to four components, up to 128 bits in total. PTX instructions specify the behavior of one thread.<br>源操作数是寄存器中的标量32位或64位值，立即值或常量; 谓词操作数是1位布尔值。 目标是寄存器，除了存储到存储器。 通过使用@p或@！p对它们进行预处理来预测指令，其中p是谓词寄存器。 内存和纹理指令传输两到四个组件的标量或向量，总共最多128位。 PTX指令指定一个线程的行为。</p>
<p>The PTX arithmetic instructions operate on 32-bit and 64-bit ﬂoating-point, signed integer, and unsigned integer types. Recent GPUs support 64-bit double precision ﬂoating-point; see Section C.6. On current GPUs, PTX 64-bit integer and logical instructions are translated to two or more binary microinstructions that perform 32-bit operations. The GPU special function instructions are limited to 32-bit ﬂoating-point. The thread control ﬂow instructions are conditional branch, function call and return, thread exit, and bar.sync (barrier synchronization). The conditional branch instruction @p bra target uses a predicate register p (or !p) previously set by a compare and set predicate setp instruction to determine whether the thread takes the branch or not. Other instructions can also be predicated on a predicate register being true or false.<br>PTX算术指令对32位和64位浮点，有符号整数和无符号整数类型进行操作。 最近的GPU支持64位双精度浮点; 见C.6节。 在当前的GPU上，PTX 64位整数和逻辑指令被转换为两个或更多个执行32位操作的二进制微指令。 GPU特殊功能指令仅限于32位浮点。 线程控制流程指令是条件分支，函数调用和返回，线程退出和bar.sync（屏障同步）。 条件分支指令@p bra target使用先前由compare和set predicate setp指令设置的谓词寄存器p（或！p）来确定线程是否接受分支。 其他指令也可以在谓词寄存器为真或假的情况下进行预测。</p>
<h2 id="Memory-Access-Instructions-内存访问指令"><a href="#Memory-Access-Instructions-内存访问指令" class="headerlink" title="Memory Access Instructions 内存访问指令"></a>Memory Access Instructions 内存访问指令</h2><p>The <em>tex</em> instruction fetches and filters texture samples from 1D, 2D, and 3D texture arrays in memory via the texture subsystem. Texture fetches generally use interpolated ﬂoating-point coordinates to address a texture. Once a graphics pixel shader thread computes its pixel fragment color, the raster operations processor blends it with the pixel color at its assigned (x, y) pixel position and writes the final color to memory.<br><em>tex</em>指令通过纹理子系统从内存中的1D，2D和3D纹理数组中提取和过滤纹理样本。 纹理提取通常使用插值的浮点坐标来寻址纹理。 一旦图形像素着色器线程计算其像素片段颜色，光栅操作处理器将其与其指定的（x，y）像素位置处的像素颜色混合，并将最终颜色写入存储器。<br>To support computing and C/C++ language needs, the Tesla PTX ISA implements memory load/store instructions. It uses integer byte addressing with register plus oﬀset address arithmetic to facilitate conventional compiler code optimizations. Memory load/store instructions are common in processors, but are a signifcant new capability in the Tesla architecture GPUs, as prior GPUs provided only the texture and pixel accesses required by the graphics APIs.<br>为了支持计算和C / C ++语言需求，Tesla PTX ISA实现了内存加载/存储指令。 它使用整数字节寻址和寄存器加o ff设置地址算法来促进传统的编译器代码优化。 内存加载/存储指令在处理器中很常见，但在Tesla架构GPU中是一项重要的新功能，因为之前的GPU仅提供图形API所需的纹理和像素访问。<br>For computing, the load/store instructions access three read/write memory spaces that implement the corresponding CUDA memory spaces in Section C.3:<br>对于计算，加载/存储指令访问在C.3节中实现相应CUDA存储空间的三个读/写存储空间：</p>
<ul>
<li>Local memory for per-thread private addressable temporary data (implemented in external DRAM) 每线程专用可寻址临时数据的本地内存（在外部DRAM中实现）</li>
<li>Shared memory for low-latency access to data shared by cooperating threads in the same CTA/thread block (implemented in on-chip SRAM) 共享内存，用于对同一CTA /线程块中的协作线程共享的数据进行低延迟访问（在片上SRAM中实现）</li>
<li>Global memory for large data sets shared by all threads of a computing application (implemented in external DRAM) 计算应用程序的所有线程共享的大型数据集的全局内存（在外部DRAM中实现）</li>
<li>The memory load/store instructions ld.global, st.global, ld.shared, st.shared, ld.local, and st.local access the global, shared, and local memory spaces. Computing programs use the fast barrier synchronization instruction bar.sync to synchronize threads within a CTA/thread block that communicate with each other via shared and global memory.<br>内存加载/存储指令ld.global，st.global，ld.shared，st.shared，ld.local和st.local访问全局，共享和本地内存空间。 计算程序使用快速屏障同步指令bar.sync来同步CTA /线程块内的线程，这些线程通过共享和全局内存相互通信。</li>
</ul>
<p>To improve memory bandwidth and reduce overhead, the local and global load/store instructions coalesce individual parallel thread requests from the same SIMT warp together into a single memory block request when the addresses fall in the same block and meet alignment criteria. Coalescing memory requests provides a signifcant performance boost over separate requests from individual threads. The multiprocessor’s large thread count, together with support for many outstanding load requests, helps cover load-to-use latency for local and global memory implemented in external DRAM.<br>为了改善存储器带宽并减少开销，当地址落在同一块中并满足对齐标准时，本地和全局加载/存储指令将来自相同SIMT warp的各个并行线程请求合并为单个存储器块请求。 合并内存请求相对于来自各个线程的单独请求提供了显着的性能提升。 多处理器的大线程数以及对许多未完成的负载请求的支持有助于覆盖外部DRAM中实现的本地和全局内存的负载使用延迟。</p>
<p>The latest Tesla architecture GPUs also provide efcient atomic memory operations on memory with the atom.op.u32 instructions, including integer operations add, min, max, and, or, xor, exchange, and cas (compare-and-swap) operations, facilitating parallel reductions and parallel data structure management.<br>最新的Tesla架构GPU还通过atom.op.u32指令在内存上提供有效的原子内存操作，包括整数运算add，min，max和，或者xor，exchange和cas（比较和交换）操作， 促进并行减少和并行数据结构管理。</p>
<h2 id="Barrier-Synchronization-for-Thread-Communication-线程通信的屏障同步"><a href="#Barrier-Synchronization-for-Thread-Communication-线程通信的屏障同步" class="headerlink" title="Barrier Synchronization for Thread Communication 线程通信的屏障同步"></a>Barrier Synchronization for Thread Communication 线程通信的屏障同步</h2><p>Fast barrier synchronization permits CUDA programs to communicate frequently via shared memory and global memory by simply calling __syncthreads(); as part of each interthread communication step. The synchronization intrinsic function generates a single bar.sync instruction. However, implementing fast barrier synchronization among up to 512 threads per CUDA thread block is a challenge.<br>快速屏障同步允许CUDA程序通过简单地调用__syncthreads（）;来经常通过共享内存和全局内存进行通信。 作为每个线程交流步骤的一部分。 同步内部函数生成单个bar.sync指令。 但是，在每个CUDA线程块中最多512个线程之间实现快速屏障同步是一项挑战。<br>Grouping threads into SIMT warps of 32 threads reduces the synchronization difculty by a factor of 32. Treads wait at a barrier in the SIMT thread scheduler so they do not consume any processor cycles while waiting. When a thread executes a bar.sync instruction, it increments the barrier’s thread arrival counter and the scheduler marks the thread as waiting at the barrier. Once all the CTA threads arrive, the barrier counter matches the expected terminal count, and the scheduler releases all the threads waiting at the barrier and resumes executing threads.<br>将线程分组为32个线程的SIMT warp可将同步困难减少32倍.Tread在SIMT线程调度程序中等待障碍，以便它们在等待时不消耗任何处理器周期。 当一个线程执行bar.sync指令时，它会递增屏障的线程到达计数器，并且调度程序将该线程标记为在屏障处等待。 一旦所有CTA线程到达，屏障计数器匹配预期的终端计数，并且调度程序释放在屏障处等待的所有线程并继续执行线程。</p>
<h2 id="Streaming-Processor-SP-流处理器（SP）"><a href="#Streaming-Processor-SP-流处理器（SP）" class="headerlink" title="Streaming Processor (SP) 流处理器（SP）"></a>Streaming Processor (SP) 流处理器（SP）</h2><p>The multithreaded streaming processor (SP) core is the primary thread instruction processor in the multiprocessor. Its register fle (RF) provides 1024 scalar 32-bit registers for up to 64 threads. It executes all the fundamental ﬂoating-point operations, including add.f32, mul.f32, mad.f32 (ﬂoating multiply-add), min.f32, max.f32, and setp.f32 (ﬂoating compare and set predicate). Te ﬂoatingpoint add and multiply operations are compatible with the IEEE 754 standard for single precision FP numbers, including not-a-number (NaN) and infnity values. Te SP core also implements all of the 32-bit and 64-bit integer arithmetic, comparison, conversion, and logical PTX instructions shown in Figure C.4.3.<br>多线程流处理器（SP）内核是多处理器中的主要线程指令处理器。 其寄存器文件（RF）提供1024个标量32位寄存器，最多可支持64个线程。 它执行所有基本的浮点运算，包括add.f32，mul.f32，mad.f32（浮动乘法 - 加法），min.f32，max.f32和setp.f32（浮点数比较和设置谓词）。 对于单精度FP编号，包括非数字（NaN）和无穷大值，Te floatingpoint加法和乘法运算与IEEE 754标准兼容。 Te SP内核还实现了图C.4.3中所示的所有32位和64位整数运算，比较，转换和逻辑PTX指令。</p>
<p>The ﬂoating-point add and mul operations employ IEEE round-to-nearest-even as the default rounding mode. Te mad.f32 ﬂoating-point multiply-add operation performs a multiplication with truncation, followed by an addition with roundto-nearest-even. The SP ﬂushes input denormal operands to sign-preserved-zero. Results that underﬂow the target output exponent range are ﬂushed to signpreserved-zero after rounding.<br>浮点加法和mul运算采用IEEE舍入到最近 - 甚至作为默认舍入模式。 Te mad.f32浮点乘法加法运算执行与截断的乘法运算，然后使用roundto-nearest-even进行加法运算。 SP将输入非正规操作数用于符号保留为零。 在舍入之后，将目标输出指数范围下的结果浮动到符号保留为零</p>
<h2 id="Special-Function-Unit-SFU-特殊功能单元（SFU）"><a href="#Special-Function-Unit-SFU-特殊功能单元（SFU）" class="headerlink" title="Special Function Unit (SFU) 特殊功能单元（SFU）"></a>Special Function Unit (SFU) 特殊功能单元（SFU）</h2><p>Certain thread instructions can execute on the SFUs, concurrently with other thread instructions executing on the SPs. The SFU implements the special function instructions of Figure C.4.3, which compute 32-bit ﬂoating-point approximations to reciprocal, reciprocal square root, and key transcendental functions. It also implements 32-bit ﬂoating-point planar attribute interpolation for pixel shaders, providing accurate interpolation of attributes such as color, depth, and texture coordinates.<br>某些线程指令可以在SFU上执行，与在SP上执行的其他线程指令同时执行。 SFU实现了图C.4.3中的特殊函数指令，它们计算倒数，倒数平方根和关键超越函数的32位浮点近似。 它还为像素着色器实现了32位浮点平面属性插值，提供了颜色，深度和纹理坐标等属性的精确插值。</p>
<p>Each pipelined SFU generates one 32-bit ﬂoating-point special function result per cycle; the two SFUs per multiprocessor execute special function instructions at a quarter the simple instruction rate of the eight SPs. The SFUs also execute the mul.f32 multiply instruction concurrently with the eight SPs, increasing the peak<br>computation rate up to 50% for threads with a suitable instruction mixture.<br>每个流水线SFU在每个周期产生一个32位浮点特殊功能结果; 每个多处理器的两个SFU以八个SP的简单指令速率的四分之一执行特殊功能指令。 SFU还与8个SP同时执行mul.f32乘法指令，对于具有合适指令混合的线程，峰值计算速率提高了50％。</p>
<p>For functional evaluation, the Tesla architecture SFU employs quadratic interpolation based on enhanced minimax approximations for approximating the reciprocal, reciprocal square-root, log2x, 2x, and sin/cos functions. Te accuracy of the function estimates ranges from 22 to 24 mantissa bits. See Section C.6 for more details on SFU arithmetic.<br>对于功能评估，特斯拉架构SFU采用基于增强的极小极大近似的二次插值来近似倒数，倒数平方根，log2x，2x和正弦/余弦函数。 功能估计的准确度范围从22到24个尾数位。 有关SFU算法的更多详细信息，请参见第C.6节。</p>
<h2 id="Comparing-with-Other-Multiprocessors-与其他多处理器比较"><a href="#Comparing-with-Other-Multiprocessors-与其他多处理器比较" class="headerlink" title="Comparing with Other Multiprocessors 与其他多处理器比较"></a>Comparing with Other Multiprocessors 与其他多处理器比较</h2><p>Compared with SIMD vector architectures such as x86 SSE, the SIMT multiprocessor can execute individual threads independently, rather than always executing them together in synchronous groups. SIMT hardware fnds data parallelism among independent threads, whereas SIMD hardware requires the sofware to express data parallelism explicitly in each vector instruction. A SIMT machine executes a warp of 32 threads synchronously when the threads take the same execution path, yet can execute each thread independently when they diverge. Te advantage is signifcant because SIMT programs and instructions simply describe the behavior of a single independent thread, rather than a SIMD data vector of four or more data lanes. Yet the SIMT multiprocessor has SIMD-like efciency, spreading the area and cost of one instruction unit across the 32 threads of a warp and across the eight streaming processor cores. SIMT provides the performance of SIMD together with the productivity of multithreading, avoiding the need to explicitly code SIMD vectors for edge conditions and partial divergence.<br>与SIM86矢量体系结构（如x86 SSE）相比，SIMT多处理器可以独立执行各个线程，而不是始终在同步组中一起执行它们。 SIMT硬件支持独立线程之间的数据并行性，而SIMD硬件要求软件在每个向量指令中明确表达数据并行性。当线程采用相同的执行路径时，SIMT机器同步执行32个线程的warp，但是当它们发散时可以独立地执行每个线程。优点是显着的，因为SIMT程序和指令简单地描述了单个独立线程的行为，而不是四个或更多数据通道的SIMD数据向量。然而，SIMT多处理器具有类似SIMD的效率，在一个扭曲的32个线程和八个流处理器内核之间扩展了一个指令单元的面积和成本。 SIMT提供SIMD的性能以及多线程的生产率，无需为边缘条件和部分发散明确编码SIMD向量。</p>
<p>The SIMT multiprocessor imposes little overhead because it is hardware multithreaded with hardware barrier synchronization. That allows graphics shaders and CUDA threads to express very fne-grained parallelism. Graphics and CUDA programs use threads to express fne-grained data parallelism in a perthread program, rather than forcing the programmer to express it as SIMD vector instructions. It is simpler and more productive to develop scalar single-thread code than vector code, and the SIMT multiprocessor executes the code with SIMD-like effciency.<br>SIMT多处理器的开销很小，因为它是具有硬件屏障同步的硬件多线程。 这允许图形着色器和CUDA线程表达非常细致的并行性。 图形和CUDA程序使用线程在perthread程序中表达细粒度数据并行性，而不是强迫程序员将其表达为SIMD向量指令。 开发标量单线程代码比矢量代码更简单，更高效，而SIMT多处理器以类似SIMD的效率执行代码。</p>
<p>Coupling eight streaming processor cores together closely into a multiprocessor and then implementing a scalable number of such multiprocessors makes a twolevel multiprocessor composed of multiprocessors. The CUDA programming model exploits the two-level hierarchy by providing individual threads for fne-grained parallel computations, and by providing grids of thread blocks for coarse-grained parallel operations. The same thread program can provide both fine-grained and coarse-grained operations. In contrast, CPUs with SIMD vector instructions must use two diﬀerent programming models to provide fne-grained and coarse-grained operations: coarse-grained parallel threads on different cores, and SIMD vector instructions for fne-grained data parallelism.<br>将八个流处理器核心紧密地耦合到多处理器中，然后实现可扩展数量的这种多处理器，使得由多处理器组成的两级多处理器成为可能。 CUDA编程模型通过为细粒度并行计算提供单独的线程，并通过为粗粒度并行操作提供线程块网格来利用两级层次结构。 相同的线程程序可以提供细粒度和粗粒度操作。 相反，具有SIMD向量指令的CPU必须使用两种不同的编程模型来提供细粒度和粗粒度操作：不同内核上的粗粒度并行线程，以及用于细粒度数据并行性的SIMD向量指令。</p>
<h2 id="Multithreaded-Multiprocessor-Conclusion-多线程多处理器结论"><a href="#Multithreaded-Multiprocessor-Conclusion-多线程多处理器结论" class="headerlink" title="Multithreaded Multiprocessor Conclusion 多线程多处理器结论"></a>Multithreaded Multiprocessor Conclusion 多线程多处理器结论</h2><p>The example GPU multiprocessor based on the Tesla architecture is highly multithreaded, executing a total of up to 512 lightweight threads concurrently to support fne-grained pixel shaders and CUDA threads. It uses a variation on SIMD architecture and multithreading called SIMT (single-instruction multiple-thread) to effciently broadcast one instruction to a warp of 32 parallel threads, while permitting each thread to branch and execute independently. Each thread executes its instruction stream on one of the eight streaming processor (SP) cores, which are multithreaded up to 64 threads.<br>基于Tesla架构的示例GPU多处理器是高度多线程的，同时执行总共多达512个轻量级线程，以支持细粒度像素着色器和CUDA线程。 它使用SIMD架构的变体和称为SIMT（单指令多线程）的多线程来有效地将一条指令广播到32个并行线程的warp，同时允许每个线程独立地分支和执行。 每个线程在八个流处理器（SP）内核之一上执行其指令流，这些内核是多线程的，最多64个线程。</p>
<p>The PTX ISA is a register-based load/store scalar ISA that describes the execution of a single thread. Because PTX instructions are optimized and translated to binary microinstructions for a specifc GPU, the hardware instructions can evolve rapidly without disrupting compilers and sofware tools that generate PTX instructions.<br>PTX ISA是一个基于寄存器的加载/存储标量ISA，用于描述单个线程的执行。 由于PTX指令经过优化并转换为特定GPU的二进制微指令，因此硬件指令可以快速发展，而不会中断生成PTX指令的编译器和软件工具</p>

          
        
      
    </div>

    
      

  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\30\5_Parallel_Memory_System\" rel="bookmark">5_Parallel_Memory_System</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\24\Programming_GPUs\" rel="bookmark">3_Programming_GPUs</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\23\GPU_System_Architectures\" rel="bookmark">2_GPU_System_Architectures</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\11\Graphics_and_Computing_GPUs\" rel="bookmark">1_图形和计算GPU</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2018\05\05\VSCode插件入门\" rel="bookmark">VSCode插件开发</a></div>
      
    </li>
  
  </ul>


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2019/01/24/Programming_GPUs/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/24/Programming_GPUs/" class="post-title-link" itemprop="url">3_Programming_GPUs</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-24 09:30:01" itemprop="dateCreated datePublished" datetime="2019-01-24T09:30:01+08:00">2019-01-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-02-04 12:26:16" itemprop="dateModified" datetime="2019-02-04T12:26:16+08:00">2019-02-04</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/GPU相关/" itemprop="url" rel="index"><span itemprop="name">GPU相关</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><img src="https://img.shields.io/badge/version-v1.1.1-519dd9.svg" alt=""></p>
<p>Programming multiprocessor GPUs is qualitatively different than programming other multiprocessors like multicore CPUs. GPUs provide two to three orders of magnitude more thread and data parallelism than CPUs, scaling to hundreds of processor cores and tens of thousands of concurrent threads. GPUs continue to increase their parallelism, doubling it about every 12 to 18 months, enabled by Moore’s law [1965] of increasing integrated circuit density and by improving architectural efciency. To span the wide price and performance range of different market segments, different GPU products implement widely varying numbers of processors and threads. Yet users expect games, graphics, imaging, and computing applications to work on any GPU, regardless of how many parallel threads it executes or how many parallel processor cores it has, and they expect more expensive GPUs (with more threads and cores) to run applications faster. As a result, GPU programming models and application programs are designed to scale transparently to a wide range of parallelism.<br>编程多处理器GPU与编程其他多处理器（如多核CPU）的质量不同。 GPU比CPU提供两到三个数量级的线程和数据并行性，可扩展到数百个处理器内核和数万个并发线程。随着集成电路密度的提高和提高架构效率的摩尔定律[1965]，GPU每12到18个月就会继续增加并行性，并且会增加一倍。为了跨越不同细分市场的广泛价格和性能范围，不同的GPU产品实现了大量不同数量的处理器和线程。然而，用户期望游戏，图形，成像和计算应用程序可以在任何GPU上运行，无论它执行多少并行线程或它有多少并行处理器核心，他们期望更多昂贵的GPU（具有更多线程和核心）可以更快地运行应用程序。因此，GPU编程模型和应用程序旨在透明地扩展到各种并行性。</p>
<p>The driving force behind the large number of parallel threads and cores in a GPU is real-time graphics performance—the need to render complex 3D scenes with high resolution at interactive frame rates, at least 60 frames per second. Correspondingly, the scalable programming models of graphics shading languages such as Cg (C for graphics) and HLSL (high-level shading language) are designed to exploit large degrees of parallelism via many independent parallel threads and to scale to any number of processor cores. The CUDA scalable parallel programming model similarly enables general parallel computing applications to leverage large numbers of parallel threads and scale to any number of parallel processor cores, transparently to the application.<br>GPU中大量并行线程和核心背后的驱动力是实时图形性能 - 需要以交互式帧速率（至少每秒60帧）渲染具有高分辨率的复杂3D场景。 相应地，图形着色语言（如Cg（图形用C）和HLSL（高级着色语言））的可伸缩编程模型旨在通过许多独立的并行线程利用大程度的并行性，并扩展到任意数量的处理器内核。 CUDA可扩展并行编程模型同样使通用并行计算应用程序能够利用大量并行线程，并扩展到任意数量的并行处理器内核，对应用程序透明。</p>
<p>In these scalable programming models, the programmer writes code for a single thread, and the GPU runs myriad thread instances in parallel. Programs thus scale transparently over a wide range of hardware parallelism. Tis simple paradigm arose from graphics APIs and shading languages that describe how to shade one vertex or one pixel. It has remained an effective paradigm as GPUs have rapidly increased their parallelism and performance since the late 1990s.<br>在这些可伸缩的编程模型中，程序员为单个线程编写代码，GPU并行运行无数的线程实例。 因此，程序在广泛的硬件并行性上透明地扩展。 这是一种简单的范例，它来自描述如何遮蔽一个顶点或一个像素的图形API和着色语言。 自从20世纪90年代末以来，GPU一直在迅速提高其并行性和性能，这仍然是一种有效的范例。<br>This section brieﬂy describes programming GPUs for real-time graphics applications using graphics APIs and programming languages. It then describes programming GPUs for visual computing and general parallel computing applications using the C language and the CUDA programming model.<br>本节简要介绍如何使用图形API和编程语言为实时图形应用程序编程GPU。 然后，它描述了使用C语言和CUDA编程模型编程用于可视计算和通用并行计算应用程序的GPU。<br>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/2019/01/24/Programming_GPUs/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    
      

  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\30\5_Parallel_Memory_System\" rel="bookmark">5_Parallel_Memory_System</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\27\Multithreaded_Multiprocessor_Architecture\" rel="bookmark">4_Multithreaded_Multiprocessor_Architecture</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\23\GPU_System_Architectures\" rel="bookmark">2_GPU_System_Architectures</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\11\Graphics_and_Computing_GPUs\" rel="bookmark">1_图形和计算GPU</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2018\05\05\VSCode插件入门\" rel="bookmark">VSCode插件开发</a></div>
      
    </li>
  
  </ul>


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2019/01/23/GPU_System_Architectures/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/23/GPU_System_Architectures/" class="post-title-link" itemprop="url">2_GPU_System_Architectures</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-23 14:38:34" itemprop="dateCreated datePublished" datetime="2019-01-23T14:38:34+08:00">2019-01-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-02-04 11:51:22" itemprop="dateModified" datetime="2019-02-04T11:51:22+08:00">2019-02-04</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/GPU相关/" itemprop="url" rel="index"><span itemprop="name">GPU相关</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><img src="https://img.shields.io/badge/version-v1.1.1-519dd9.svg" alt=""></p>
<p>In this section, we survey GPU system architectures in common use today. We discuss system configurations, GPU functions and services, standard programming interfaces, and a basic GPU internal architecture.<br>在本节中，我们将调查目前常用的GPU系统架构。 我们将讨论系统配置，GPU功能和服务，标准编程接口以及基本的GPU内部架构。</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/2019/01/23/GPU_System_Architectures/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
      

  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\30\5_Parallel_Memory_System\" rel="bookmark">5_Parallel_Memory_System</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\27\Multithreaded_Multiprocessor_Architecture\" rel="bookmark">4_Multithreaded_Multiprocessor_Architecture</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\24\Programming_GPUs\" rel="bookmark">3_Programming_GPUs</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\11\Graphics_and_Computing_GPUs\" rel="bookmark">1_图形和计算GPU</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2018\05\05\VSCode插件入门\" rel="bookmark">VSCode插件开发</a></div>
      
    </li>
  
  </ul>


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2019/01/11/Graphics_and_Computing_GPUs/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/11/Graphics_and_Computing_GPUs/" class="post-title-link" itemprop="url">1_图形和计算GPU</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-11 19:56:55" itemprop="dateCreated datePublished" datetime="2019-01-11T19:56:55+08:00">2019-01-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-30 22:01:46" itemprop="dateModified" datetime="2019-01-30T22:01:46+08:00">2019-01-30</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/GPU相关/" itemprop="url" rel="index"><span itemprop="name">GPU相关</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="https://img.shields.io/badge/version-v1.1.1-519dd9.svg" alt=""></p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>This appendix focuses on the <strong>GPU</strong>—the ubiquitous <strong>graphics processing unit</strong> in every PC, laptop, desktop computer, and workstation. In its most basic form, the GPU generates 2D and 3D graphics, images, and video that enable windowbased operating systems, graphical user interfaces, video games, visual imaging applications, and video. The modern GPU that we describe here is a highly parallel, highly multithreaded multiprocessor optimized for <strong>visual computing</strong>. To provide real-time visual interaction with computed objects via graphics, images, and video, the GPU has a unifed graphics and computing architecture that serves as both a programmable graphics processor and a scalable parallel computing platform. PCs and game consoles combine a GPU with a CPU to form <strong>heterogeneous systems</strong>. </p>
<p>本附录重点介绍GPU——每台PC，笔记本电脑，台式机和工作站中无处不在的<strong>图形处理单元</strong>。 GPU最基本的功能是生成2D和3D图形，图像和视频，以及支持基于窗口的操作系统，图形用户界面，视频游戏，视觉成像应用程序和视频。 我们在这里描述的现代GPU是一个高度并行，高度多线程的多处理器，针对<strong>视觉计算</strong>进行了优化。 为了通过图形，图像和视频提供与计算对象的实时可视交互，GPU具有统一的图形和计算架构，可用作可编程图形处理器和可扩展的并行计算平台。 PC和游戏主机（game console）将GPU与CPU相结合，形成<strong>异构系统</strong>。</p>
<blockquote>
<p><strong>graphics processing unit (gpu)</strong> ： a processor optimized for 2d and 3d graphics, video, visual computing, and display 一种针对2D和3D图形，视频，视觉计算和显示进行了优化的处理器</p>
</blockquote>
<blockquote>
<p><strong>visual computing视觉计算</strong> ：a mix of graphics processing and computing that lets you visually interact with computed objects via graphics, images, and video 一种图形处理和计算的混合，使你可以通过图形、图像和视频与计算对象进行可视化交互。</p>
</blockquote>
<blockquote>
<p><strong>heterogeneous system</strong>: A system combining different processor types. A PC is a heterogeneous CPU–GPU system. 一种组合了不同处理器类型的系统。 PC就是一种异构CPU-GPU系统。</p>
</blockquote>
<h2 id="A-Brief-History-of-GPU-Evolution-GPU发展简史"><a href="#A-Brief-History-of-GPU-Evolution-GPU发展简史" class="headerlink" title="A Brief History of GPU Evolution GPU发展简史"></a>A Brief History of GPU Evolution GPU发展简史</h2><p>fifteen years ago, there was no such thing as a gpu. graphics on a pc were performed by a video graphics array (vga) controller. a vga controller was simply a memory controller and display generator connected to some DRAM. in the 1990s, semiconductor technology advanced sufciently that more functions could be added to the vga controller. by 1997, vga controllers were beginning to incorporate some three-dimensional (3d) acceleration functions, including hardware for triangle setup and rasterization (dicing triangles into individual pixels) and texture mapping and shading (applying “decals” or patterns to pixels and blending colors).<br>十五年前，还没有像GPU这样的东西。 PC上的图形由视频图形阵列（video graphics array ,VGA）控制器展现。 VGA控制器只是一个连接到某些DRAM的存储器控制器和显示发生器。 在20世纪90年代，半导体技术充分发展，可以在VGA控制器中添加更多功能。 到1997年，VGA控制器开始采用一些三维（3d）加速功能，包括用于<a href="#">三角形设置（Triangle Setup）</a>和<a href="#">光栅化(Rasterization)</a>的硬件（将三角形切割成单个像素）和<a href="#">纹理映射</a>和着色（将“贴花”或图案应用于像素和混合颜色）。</p>
<p>In 2000, the single chip graphics processor incorporated almost every detail of the traditional high-end workstation graphics pipeline and, therefore, deserved a new name beyond vga controller. the term gpu was coined to denote that the graphics device had become a processor.<br>2000年，单芯片图形处理器几乎整合了传统高端工作站<a href="#">图形管道</a>的每个细节，因此，它应当有一个除VGA外的全新的名称。 术语GPU被用来表示这个图形设备（graphics device）已成为一种处理器。</p>
<p>Over time, GPUs became more programmable, as programmable processors replaced fixed function dedicated logic while maintaining the basic 3D graphics pipeline organization. In addition, computations became more precise over time, progressing from indexed arithmetic, to integer and fixed point, to single precision ﬂoating-point, and recently to double precision ﬂoating-point. GPUs have become massively parallel programmable processors with hundreds of cores and thousands of threads.</p>
<p>随着时间的推移，GPU变得更加可编程，因为可编程处理器取代了固定功能专用逻辑，同时保持了基本的3D图形管道组织。此外，计算随着时间的推移变得更加精确，从<a href="#">索引算术(indexed arithmetic)</a>，整数和固定点，到单精度浮点，最近又到双精度浮点。 GPU已成为具有数百个内核和数千个线程的大规模并行可编程处理器。</p>
<p>Recently, processor instructions and memory hardware were added to support general purpose programming languages, and a programming environment was created to allow GPUs to be programmed using familiar languages, including C and C++. This innovation makes a GPU a fully general-purpose, programmable, manycore processor, albeit still with some special benefits and limitations.<br>最近，添加了处理器指令和存储器硬件以支持通用编程语言，并且创建了编程环境以允许使用熟悉的语言（包括C和C ++）对GPU进行编程。 这项创新使GPU成为一个完全通用的可编程多核处理器，尽管仍有一些特殊的好处和局限。</p>
<h2 id="GPU-Graphics-Trends-GPU图形的发展趋势"><a href="#GPU-Graphics-Trends-GPU图形的发展趋势" class="headerlink" title="GPU Graphics Trends GPU图形的发展趋势"></a>GPU Graphics Trends GPU图形的发展趋势</h2><p>GPUs and their associated drivers implement the OpenGL and DirectX models of graphics processing. OpenGL is an open standard for 3D graphics programming available for most computers. DirectX is a series of Microsof multimedia programming interfaces, including Direct3D for 3D graphics. Since these <strong>application programming interfaces (APIs)</strong> have well-defned behavior, it is possible to build effective hardware acceleration of the graphics processing functions defned by the APIs. This is one of the reasons (in addition to increasing device density) why new GPUs are being developed every 12 to 18 months that double the performance of the previous generation on existing applications.<br>GPU及其相关驱动程序实现了图形处理的OpenGL和DirectX模型。 OpenGL是大多数计算机可用的3D图形编程的开放标准。 DirectX是一系列Microsof多媒体编程接口，包括用于3D图形的Direct3D。 由于这些应用程序编程接口（API）具有良好的行为，因此可以构建由API定义的图形处理功能的有效硬件加速。 这是为什么（除了增加设备密度之外）每12到18个月开发新GPU以使现有应用的前一代性能翻倍的原因之一。</p>
<blockquote>
<p><strong>application programming interface (API)</strong> : A set of function and data structure definitions providing an<br>interface to a library of functions. 一组函数和数据结构定义，提供了函数库的接口。</p>
</blockquote>
<p>Frequent doubling of GPU performance enables new applications that were not previously possible. The intersection of graphics processing and parallel computing invites a new paradigm for graphics, known as visual computing. It replaces large sections of the traditional sequential hardware graphics pipeline model with programmable elements for geometry, vertex, and pixel programs.Visual computing in a modern GPU combines graphics processing and parallel computing in novel ways that permit new graphics algorithms to be implemented, and opens the door to entirely new parallel processing applications on pervasive high-performance GPUs.<br>GPU性能的频繁加倍可实现以前无法实现的新应用程序。 图形处理和并行计算的交集引发了一种新的图形范例，称为视觉计算。 它将传统顺序硬件图形管道模型的大部分替换为几何，顶点和像素程序的可编程元素。现代GPU中的可视计算以新颖的方式将图形处理和并行计算相结合，允许实现新的图形算法，并打开 在普及的高性能GPU上实现全新并行处理应用的大门。</p>
<h2 id="Heterogeneous-System-异构系统"><a href="#Heterogeneous-System-异构系统" class="headerlink" title="Heterogeneous System 异构系统"></a>Heterogeneous System 异构系统</h2><p>Although the GPU is arguably the most parallel and most powerful processor in a typical PC, it is certainly not the only processor. The CPU, now multicore and soon to be manycore, is a complementary, primarily serial processor companion to the massively parallel manycore GPU. Together, these two types of processors comprise a heterogeneous multiprocessor system.<br>虽然GPU可以说是典型PC中最并行，最强大的处理器，但它肯定不是唯一的处理器。 CPU，现在是多核(multicore)的，很快就会成为众核（manycore），是大规模并行多核GPU互补的，重要的串行处理器伙伴（好拗口）。 这两种类型的处理器一起构成了异构多处理器系统。</p>
<p>The best performance for many applications comes from using both the CPU and the GPU. Tis appendix will help you understand how and when to best split the work between these two increasingly parallel processors.<br>许多应用程序的最佳性能来自于使用CPU和GPU。 附录将帮助您了解如何以及何时最好地分割这两个日益并行的处理器之间的工作。</p>
<h2 id="GPU-Evolves-into-Scalable-Parallel-Processor-GPU发展成可扩展的并行处理器"><a href="#GPU-Evolves-into-Scalable-Parallel-Processor-GPU发展成可扩展的并行处理器" class="headerlink" title="GPU Evolves into Scalable Parallel Processor GPU发展成可扩展的并行处理器"></a>GPU Evolves into Scalable Parallel Processor GPU发展成可扩展的并行处理器</h2><p>GPUs have evolved functionally from hardwired, limited capability VGA controllers to programmable parallel processors. This evolution has proceeded by changing the logical (API-based) graphics pipeline to incorporate programmable elements and also by making the underlying hardware pipeline stages less specialized and more programmable. Eventually, it made sense to merge disparate programmable pipeline elements into one unifed array of many programmable processors.<br>GPU已经从硬连线，有限功能的VGA控制器发展到可编程并行处理器。 通过改变逻辑（基于API）的图形流水线以结合可编程元件以及通过使底层硬件流水线阶段不那么专业化和更可编程来进行这种演变。 最终，将不同的可编程流水线元件合并到一个由多个可编程处理器组成的统一阵列中是有意义的。<br>In the GeForce 8-series generation of GPUs, the geometry, vertex, and pixel processing all run on the same type of processor. This unification allows for dramatic scalability. More programmable processor cores increase the total system throughput. Unifying the processors also delivers very eﬀective load balancing, since any processing function can use the whole processor array. At the other end of the spectrum, a processor array can now be built with very few processors, since all of the functions can be run on the same processors.<br>在GeForce 8系列GPU中，几何，顶点和像素处理都在同一类型的处理器上运行。 这种统一允许显着的可扩展性。 更多可编程处理器内核可提高系统总吞吐量。 统一处理器还可以提供非常有效的负载平衡，因为任何处理功能都可以使用整个处理器阵列。 另一方面，处理器阵列现在可以用很少的处理器构建，因为所有功能都可以在相同的处理器上运行。</p>
<h2 id="Why-CUDA-and-GPU-Computing-为什么选择CUDA和GPU计算？"><a href="#Why-CUDA-and-GPU-Computing-为什么选择CUDA和GPU计算？" class="headerlink" title="Why CUDA and GPU Computing? 为什么选择CUDA和GPU计算？"></a>Why CUDA and GPU Computing? 为什么选择CUDA和GPU计算？</h2><p>This uniform and scalable array of processors invites a new model of programming for the GPU. The large amount of ﬂoating-point processing power in the GPU processor array is very attractive for solving nongraphics problems. Given the large degree of parallelism and the range of scalability of the processor array for graphics applications, the programming model for more general computing must express the massive parallelism directly, but allow for scalable execution.<br>这种统一且可扩展的处理器阵列为GPU提供了一种新的编程模型。 GPU处理器阵列中的大量浮点处理能力对于解决非图形问题非常有吸引力。 鉴于用于图形应用程序的处理器阵列的高度并行性和可扩展性范围，用于更一般计算的编程模型必须直接表达大规模并行性，但允许可伸缩执行。</p>
<p><strong>GPU computing</strong> is the term coined for using the GPU for computing via a parallel programming language and API, without using the traditional graphics API and graphics pipeline model. This is in contrast to the earlier <strong>General Purpose computation on GPU (GPGPU)</strong> approach, which involves programming the GPU using a graphics API and graphics pipeline to perform nongraphics tasks.<br>GPU计算是通过并行编程语言和API使用GPU进行计算而创造的术语，而不使用传统的图形API和图形管道模型。 这与早期的GPU上通用计算（GPGPU）方法形成对比，后者涉及使用图形API和图形管道对GPU进行编程以执行非图形任务。</p>
<blockquote>
<p><strong>GPU computing</strong> : Using a GPU for computing via a parallel programming language and API. 使用GPU通过并行编程语言和API进行计算。<br><strong>GPGPU</strong> : Using a GPU for general-purpose computation via a traditional graphics API and graphics pipeline.通过传统的图形API和图形管道将GPU用于通用计算。</p>
</blockquote>
<p><strong>Compute Unifed Device Architecture (CUDA)</strong> is a scalable parallel programming model and sofware platform for the GPU and other parallel processors that allows the programmer to bypass the graphics API and graphics interfaces of the GPU and simply program in C or C++. The CUDA programming model has an SPMD (single-program multiple data) software style, in which a programmer writes a program for one thread that is instanced and executed by many threads in parallel on the multiple processors of the GPU. In fact, CUDA also provides a facility for programming multiple CPU cores as well, so CUDA is an environment for writing parallel programs for the entire heterogeneous computer system.<br>计算统一设备架构（CUDA）是GPU和其他并行处理器的可扩展并行编程模型和软件平台，允许程序员绕过GPU的图形API和图形接口，只需用C或C ++编程。 CUDA编程模型具有SPMD（单程序多数据）的软件风格，其中程序员为一个线程编写程序，该程序由多个线程并行执行并由GPU的多个处理器执行。 实际上，CUDA也提供了编程多个CPU内核的工具，因此CUDA是一个为整个异构计算机系统编写并行程序的环境。</p>
<blockquote>
<p><strong>CUDA</strong>: A scalable parallel programming model and language based on C/C++. It is a parallel programming platform for GPUs and multicore CPUs 一种可扩展的并行编程模型和基于C / C ++的语言。 它是GPU和多核CPU的并行编程平台</p>
</blockquote>
<h2 id="GPU-Unifes-Graphics-and-Computing-GPU统一了图形和计算"><a href="#GPU-Unifes-Graphics-and-Computing-GPU统一了图形和计算" class="headerlink" title="GPU Unifes Graphics and Computing GPU统一了图形和计算"></a>GPU Unifes Graphics and Computing GPU统一了图形和计算</h2><p>With the addition of CUDA and GPU computing to the capabilities of the GPU, it is now possible to use the GPU as both a graphics processor and a computing processor at the same time, and to combine these uses in visual computing applications. The underlying processor architecture of the GPU is exposed in two ways: first, as implementing the programmable graphics APIs, and second, as a massively parallel processor array programmable in C/C++ with CUDA.<br>通过将CUDA和GPU计算添加到GPU的功能，现在可以同时将GPU用作图形处理器和计算处理器，并将这些用途结合在视觉计算应用中。 GPU的底层处理器架构以两种方式暴露：第一，作为实现可编程图形API，第二，作为使用CUDA在C / C ++中编程的大规模并行处理器阵列。<br>Although the underlying processors of the GPU are unified, it is not necessary that all of the SPMD thread programs are the same. The GPU can run graphics shader programs for the graphics aspect of the GPU, processing geometry, vertices, and pixels, and also run thread programs in CUDA.<br>虽然GPU的底层处理器是统一的，但并不是所有的SPMD线程程序都是相同的。 GPU可以为GPU的图形方面运行图形着色器程序，处理几何，顶点和像素，还可以在CUDA中运行线程程序。<br>The GPU is truly a versatile multiprocessor architecture, supporting a variety of processing tasks. GPUs are excellent at graphics and visual computing as they were specifcally designed for these applications. GPUs are also excellent at many generalpurpose throughput applications that are “first cousins” of graphics, in that they perform a lot of parallel work, as well as having a lot of regular problem structure. In general, they are a good match to data-parallel problems (see Chapter 6), particularly large problems, but less so for less regular, smaller problems.<br>GPU是真正的多功能多处理器架构，支持各种处理任务。 GPU在图形和视觉计算方面非常出色，因为它们是专门为这些应用程序设计的。 GPU在许多通用吞吐量应用程序中也非常出色，这些应用程序是图形的“第一代表现形式”，因为它们执行大量并行工作，并且具有许多常规问题结构。 一般来说，它们与数据并行问题（见第6章）非常匹配，特别是大问题，但对于不那么规律，较小的问题则不那么重要。</p>
<h2 id="GPU-Visual-Computing-Applications-GPU视觉计算应用程序"><a href="#GPU-Visual-Computing-Applications-GPU视觉计算应用程序" class="headerlink" title="GPU Visual Computing Applications GPU视觉计算应用程序"></a>GPU Visual Computing Applications GPU视觉计算应用程序</h2><p>Visual computing includes the traditional types of graphics applications plus many new applications. The original purview of a GPU was “anything with pixels,” but it now includes many problems without pixels but with regular computation and/or data structure. GPUs are effective at 2D and 3D graphics, since that is the purpose for which they are designed. <strong>Failure to deliver this application performance would be fatal</strong>. 2D and 3D graphics use the GPU in its “graphics mode”, accessing the processing power of the GPU through the graphics APIs, OpenGL™, and DirectX™. Games are built on the 3D graphics processing capability.<br>视觉计算包括传统类型的图形应用程序以及许多新应用程序。 GPU的原始范围是“任何带有像素的东西”，但它现在包括许多没有像素但具有常规计算和/或数据结构的问题。 GPU在2D和3D图形上是有效的，因为这是它们的设计目的。 未能提供此<a href="#">应用性能(application performance)</a>将是致命的。 2D和3D图形在其“图形模式”中使用GPU，通过图形API，OpenGL™和DirectX™来获取GPU的处理能力。 游戏是基于3D图形处理功能的。</p>
<blockquote>
<p><strong>Application performance</strong>, 这是什么？感觉像是在指“功能”，比如没有提供这个功能将是致命的</p>
</blockquote>
<p>Beyond 2D and 3D graphics, image processing and video are important applications for GPUs. These can be implemented using the graphics APIs or as computational programs, using CUDA to program the GPU in computing mode. Using CUDA, image processing is simply another data-parallel array program. To the extent that the data access is regular and there is good locality, the program will be efficient. In practice, image processing is a very good application for GPUs. Video processing, especially encode and decode (compression and decompression according to some standard algorithms), is quite efficient.<br>除了2D和3D图形，图像处理和视频是GPU的重要应用。 这些可以使用图形API或计算程序来实现，使用CUDA在计算模式下对GPU进行编程。 使用CUDA，图像处理只是另一种数据并行阵列程序。 如果数据访问是规则的并且具有良好的位置，则该程序将是有效的。 实际上，图像处理是GPU的一个非常好的应用程序。 视频处理，尤其是编码和解码（根据一些标准算法进行压缩和解压缩）非常有效。</p>
<p>The greatest opportunity for visual computing applications on GPUs is to “break the graphics pipeline.” Early GPUs implemented only specific graphics APIs, albeit at very high performance. This was wonderful if the API supported the operations that you wanted to do. If not, the GPU could not accelerate your task, because early GPU functionality was immutable. Now, with the advent of GPU computing and CUDA, these GPUs can be programmed to implement a different virtual pipeline by simply writing a CUDA program to describe the computation and data ﬂow that is desired. So, all applications are now possible, which will stimulate new visual computing approaches.<br>GPU上可视化计算应用程序的最大的机遇是“打破图形管道。” 早期的GPU只实现了特定的图形API，尽管性能非常高。 如果API支持你想要执行的操作，那就太棒了。 如果没有，GPU就无法加速您的任务，因为早期的GPU功能是不可变的。 现在，随着GPU计算和CUDA的出现，这些GPU可以通过编写CUDA程序来编程实现不同的虚拟管道，以描述所需的计算和数据流。 因此，现在所有应用程序都可以实现，这将刺激新的视觉计算方法。</p>

          
        
      
    </div>

    
      

  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\30\5_Parallel_Memory_System\" rel="bookmark">5_Parallel_Memory_System</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\27\Multithreaded_Multiprocessor_Architecture\" rel="bookmark">4_Multithreaded_Multiprocessor_Architecture</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\24\Programming_GPUs\" rel="bookmark">3_Programming_GPUs</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\01\23\GPU_System_Architectures\" rel="bookmark">2_GPU_System_Architectures</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2018\05\05\VSCode插件入门\" rel="bookmark">VSCode插件开发</a></div>
      
    </li>
  
  </ul>


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2019/01/05/Tello/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/05/Tello/" class="post-title-link" itemprop="url">Tello特洛无人机编程小结</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-05 20:53:14" itemprop="dateCreated datePublished" datetime="2019-01-05T20:53:14+08:00">2019-01-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-06 00:31:46" itemprop="dateModified" datetime="2019-01-06T00:31:46+08:00">2019-01-06</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/IoT/" itemprop="url" rel="index"><span itemprop="name">IoT</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>研究了一下Tello无人机的编程，结果发现这玩具的编程还真的是简单。官方给了一个教程是使用scratch平台进行编程，然而局限性有点大，积木就那几个而SDK中的API要更多一些。对于有点编程基础的人来说，提供了API就意味着不必局限于任何平台了。</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>建立特洛 Tello 和 PC、Mac 或移动设备之间的 Wi-Fi 通信</p>
<h3 id="发送命令和接收响应"><a href="#发送命令和接收响应" class="headerlink" title="发送命令和接收响应"></a>发送命令和接收响应</h3><p>Tello IP：192.168.10.1 UDP PORT：8889 &lt;&lt; - - &gt;&gt; PC / Mac / Mobile</p>
<div class="note info">
            <p>备注1：在PC，Mac或移动设备上设置UDP客户端，向特洛 Tello UDP 端口 8889 发送命令<br>和接收响应。</p>
          </div>
<div class="note info">
            <p>备注2：在发送所有其他命令之前，向特洛 Tello UDP 端口 8889 发送“command” 命令<br>以启动特洛 Tello 的 SDK 模式。</p>
          </div>
<h3 id="接收特洛-Tello-状态"><a href="#接收特洛-Tello-状态" class="headerlink" title="接收特洛 Tello 状态"></a>接收特洛 Tello 状态</h3><p>Tello IP：192.168.10.1 - &gt;&gt; PC / Mac / Mobile UDP Server：0.0.0.0 UDP PORT：8890<br><div class="note info">
            <p>备注3：在 PC，Mac 或移动设备上建立 UDP 服务器，通过 UDP 端口 8890 从 IP 0.0.0.0<br>收听消息。如果未进行备注1和2的操作，请先完成。</p>
          </div></p>
<h3 id="接收特洛-Tello-视频流"><a href="#接收特洛-Tello-视频流" class="headerlink" title="接收特洛 Tello 视频流"></a>接收特洛 Tello 视频流</h3><p>Tello IP：192.168.10.1 - &gt;&gt; PC / Mac / Mobile UDP Server：0.0.0.0 UDP PORT：11111<br><div class="note info">
            <p>备注4：在 PC，Mac 或移动设备上设置 UDP 服务器，通过服务器 UDP 端口 11111 从<br>IP 0.0.0.0 收听消息。</p>
          </div><br><div class="note info">
            <p>备注5：先进行备注1和2的操作。然后向特洛 Tello UDP 端口 8889 发送 “streamon” 命<br>令，开始接受特洛 Tello 视频流。</p>
          </div></p>
<div class="tabs" id="ss"><ul class="nav-tabs"><li class="tab active"><a href="#ss-1">设置命令</a></li><li class="tab"><a href="#ss-2">控制命令</a></li><li class="tab"><a href="#ss-3">读取命令</a></li><li class="tab"><a href="#ss-4">Tello状态</a></li></ul><div class="tab-content"><div class="tab-pane active" id="ss-1"><table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">可能的响应</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">speed xx</td>
<td style="text-align:center">设置当前速度为xx(1-100)</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">rc a b c d</td>
<td style="text-align:center">设置遥控器的4个通道杆量，<br>a:横滚（-100~100）<br>b:俯仰（-100~100）<br>c:油门（~100-100）<br>d:旋转（-100~100）</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">wifi ssid pass</td>
<td style="text-align:center">设置 WiFi SSID 密码</td>
<td style="text-align:center">ok<br>error</td>
</tr>
</tbody>
</table></div><div class="tab-pane" id="ss-2"><table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">可能的响应</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">command</td>
<td style="text-align:center">进入SDK命令模式</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">takeoff</td>
<td style="text-align:center">自动起飞</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">land</td>
<td style="text-align:center">自动降落</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">streamon</td>
<td style="text-align:center">打开视频流</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">streamoff</td>
<td style="text-align:center">关闭视频流</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">emergency</td>
<td style="text-align:center">停止电机转动</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">up x</td>
<td style="text-align:center">向上飞 x 厘米<br>x = 20-500</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">down x</td>
<td style="text-align:center">向下飞 x 厘米<br>x = 20-500</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">left x</td>
<td style="text-align:center">向左飞 x 厘米<br>x = 20-500</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">right x</td>
<td style="text-align:center">向右飞x 厘米<br>x = 20-500</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">forward xx</td>
<td style="text-align:center">向前飞 x 厘米<br>x = 20-500</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">back xx</td>
<td style="text-align:center">向后飞 x 厘米<br>x = 20-500</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">cw xx</td>
<td style="text-align:center">顺时针旋转x°<br>x = 1-3600</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">ccw</td>
<td style="text-align:center">逆时针旋转 x°<br>x = 1-3600</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">flip x</td>
<td style="text-align:center">朝 x 方向翻滚<br>l = (left)<br>r =(right)<br>f = (forward<br>b = (back)</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">go x y z speed</td>
<td style="text-align:center">以设置速度（cm/s）飞往坐标<br>（x,y,z）<br>x: 20-500<br>y: 20-500<br>z: 20-500<br>speed: 10-100</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">curve x1 y1 z1 x2 y2 z2 speed</td>
<td style="text-align:center">以设置速度（ cm/s ）飞弧线，经过（x1,y1,z1）到（x2,y2,z2）<br>如果弧线半径不在 0.5-10 米范围内，则返回相应提醒。<br>x1, x2: -500 - 500<br>y1, y2: -500 - 500<br>z1, z2: -500 - 500<br>speed: 10-60<br>x、y、z 不能同时在-20 ~ 20 之间</td>
<td style="text-align:center">ok<br>error</td>
</tr>
<tr>
<td style="text-align:center">speed?</td>
<td style="text-align:center">获取当前速度</td>
<td style="text-align:center">xx</td>
</tr>
<tr>
<td style="text-align:center">battery?</td>
<td style="text-align:center">获取电量信息</td>
<td style="text-align:center">xx(0~100)</td>
</tr>
</tbody>
</table></div><div class="tab-pane" id="ss-3"><table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">可能的响应</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">speed?</td>
<td style="text-align:center">获取当前设置速度<br>（cm/s）</td>
<td style="text-align:center">x<br>x = (10-100)</td>
</tr>
<tr>
<td style="text-align:center">battery?</td>
<td style="text-align:center">获取当前电池剩余<br>电量的百分比值</td>
<td style="text-align:center">x<br>x = (0-100)</td>
</tr>
<tr>
<td style="text-align:center">time?</td>
<td style="text-align:center">获取电机运转时间时间<br>（s）</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">height?</td>
<td style="text-align:center">获取相对高度 (cm)</td>
<td style="text-align:center">x: 10-3000</td>
</tr>
<tr>
<td style="text-align:center">temp?</td>
<td style="text-align:center">获取主板<br>最高和最低温度(℃)</td>
<td style="text-align:center">x: 0-90</td>
</tr>
<tr>
<td style="text-align:center">attitude?</td>
<td style="text-align:center">获取 IMU 三轴姿态数据</td>
<td style="text-align:center">pitch roll yaw<br>pitch=（-89°- 89°）<br>roll=（-179°- 179°）<br>yaw=（-179°- 179°）</td>
</tr>
<tr>
<td style="text-align:center">baro?</td>
<td style="text-align:center">获取气压计高度(m)</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">acceleration?</td>
<td style="text-align:center">获取 IMU 三轴加速度数据(0.001g)</td>
<td style="text-align:center">x y z</td>
</tr>
<tr>
<td style="text-align:center">tof?</td>
<td style="text-align:center">获取 ToF 的高度值(cm)</td>
<td style="text-align:center">x: 10-400 &amp; 6553<br>返回 6553 意味着测量值超过<br>ToF 量程。</td>
</tr>
<tr>
<td style="text-align:center">wifi?</td>
<td style="text-align:center">获得 Wi-Fi 信噪比</td>
<td style="text-align:center">snr</td>
</tr>
</tbody>
</table></div><div class="tab-pane" id="ss-4"><p>数据类型：字符串</p>
<p>例如<br>“pitch:%d;roll:%d;yaw:%d;vgx:%d;vgy%d;vgz:%d;templ:%d;temph:%d;tof:%d;h:%d;bat:%d;baro:%f;\r \n”<br>说明： </p>
<ul>
<li>pitch：俯仰角度，度数 </li>
<li>roll：横滚角度，度数 </li>
<li>yaw：偏航偏航，度数</li>
<li>vgx：x 轴速度，</li>
<li>vgy：y 轴速度，</li>
<li>vgz：z 轴速度，</li>
<li>templ：主板最低温度，摄氏度 </li>
<li>temph：主板最高温度，摄氏度 </li>
<li>tof：ToF 距离，厘米 </li>
<li>h：相对起飞点高度，厘米 </li>
<li>bat：当前电量百分比，％ </li>
<li>baro：气压计测量高度，米 </li>
<li>time: 电机运转时间，秒 </li>
<li>agx: x 轴加速度 </li>
<li>agy: y 轴加速度 </li>
<li>agz: z 轴加速度,</li>
</ul></div></div></div>

          
        
      
    </div>

    
      


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2019/01/02/Next主题个性化/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/02/Next主题个性化/" class="post-title-link" itemprop="url">Next主题个性化</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-02 15:24:09" itemprop="dateCreated datePublished" datetime="2019-01-02T15:24:09+08:00">2019-01-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-05 18:14:55" itemprop="dateModified" datetime="2019-01-05T18:14:55+08:00">2019-01-05</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Hexo相关/" itemprop="url" rel="index"><span itemprop="name">Hexo相关</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
        <div class="post-gallery" itemscope="" itemtype="http://schema.org/ImageGallery">
          
          
            <div class="post-gallery-row">
              <a class="post-gallery-img fancybox" href="https://cooguyan-1252810309.cos.ap-chengdu.myqcloud.com/beaupic/pixiv/illust_34735001_20181119_164728.jpg" rel="gallery_cjrrwpetw004npsvakrom6ivn" itemscope="" itemtype="http://schema.org/ImageObject" itemprop="url">
                <img src="https://cooguyan-1252810309.cos.ap-chengdu.myqcloud.com/beaupic/pixiv/illust_34735001_20181119_164728.jpg" itemprop="contentUrl">
              </a>
            
          

          
          </div>
        </div>
      

      
        
          <p>这篇博客是对过去几天来折腾Next主题经验的一次大总结。本来用的是 <a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="noopener">yilia</a>主题，可是不知道怎么着脑抽了想换一个主题玩玩，这下好了，一折腾折腾了好几天····<br>博客嘛，当然是内容最重要···不过高颜值更能让我有动力哈哈哈😂没办法我就喜欢这些花里胡哨的东西</p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/2019/01/02/Next主题个性化/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    
      


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2018/12/30/RAID的理解/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/12/30/RAID的理解/" class="post-title-link" itemprop="url">RAID(Reduntant Arrays of Inexpensive Disks)</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-12-30 10:17:34 / 修改时间：12:11:55" itemprop="dateCreated datePublished" datetime="2018-12-30T10:17:34+08:00">2018-12-30</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/读书笔记/" itemprop="url" rel="index"><span itemprop="name">读书笔记</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>以下是《计算机组成与设计》第四版原文</p>
<h2 id="无冗余-RAID-0"><a href="#无冗余-RAID-0" class="headerlink" title="无冗余 (RAID 0)"></a>无冗余 (RAID 0)</h2><p>   仅仅把数据分做到多个磁盘，称为条带化，自动把访问强制分布到几个磁盘上。在一组磁盘上进行条带化使得这一组磁盘对于软件来说是一个大磁盘，从面简化了存储管理。而且对多个同时的访问来说有利于改进性能，因为多个磁盘可以同时操作。例如，视频编辑系统经常对它们的数据进行条带化，不需要像数据库那样关心可靠性问题。</p>
<p>   RAID 0的称谓有些不妥，因为它根本没有冗余。然而，RAID的级别通常由操作员在创建系统时设置，而RAIDO经常被列为其中一个选项。因此，RAID 0的说法就被广泛使用了。</p>
<h2 id="镜像-RAID-1"><a href="#镜像-RAID-1" class="headerlink" title="镜像(RAID 1)"></a>镜像(RAID 1)</h2><p>   这种传统的容忍磁盘失效的方法，被称为镜像”或者影像(shadowing), 使用比RAID O 多一倍的磁盘数。数据写人某个盘时，同样的数据会写人其冗余盘，因此始终存在信息的两份副本。如果一个磁盘出现故障，系统就转向其“镜像”读取内容以获得所需信息。镜像是最昂贵的RAID方案，因为它需要最多的磁盘。</p>
<h2 id="错误检测和纠错码-RAID-2"><a href="#错误检测和纠错码-RAID-2" class="headerlink" title="错误检测和纠错码 (RAID 2)"></a>错误检测和纠错码 (RAID 2)</h2><p>   RAID 2借用了主存常用的错误校验和恢复技术(参见光盘中的附录C)。RAID 2已经不再使用了，因此我们这里不做介绍。</p>
<h2 id="位交叉奇偶校验-RAID-3"><a href="#位交叉奇偶校验-RAID-3" class="headerlink" title="位交叉奇偶校验 (RAID 3)"></a>位交叉奇偶校验 (RAID 3)</h2><p>   增加可用性的开销可以减至I/n,这里n为保护组“内磁盘的数目。我们不再为每个磁盘做一个原始数据的完全备份，面只需要加入足够的冗余信息以便在出错的时候恢复丢失的信息。读写操作在组内所有磁盘上进行，一个额外的磁盘存有校验信息以防错误的发生。RAID 3在使用大数据集的应用( 如多媒体和科学计算)中报流行。</p>
<p>   奇偶校验(parity) 就是这样的一个策略。不熟悉奇偶校验的读者可以把冗余磁盘想象成保存有其他磁盘所有数据的和。当.个磁盘出错时，用奇偶校验盘减去正常破盘的数据的和;余数就是丢失的信息。奇偶校验就是模2下的求和。</p>
<p>   与RAID 1不同，RAID3 必须读很多磁盘才能确定丟失的数据。该技术背后的假设就是用更长的时间来恢复错误而用更少的冗余存储得到个好的平衡。</p>
<h2 id="块交叉奇偶校验-RAID-4"><a href="#块交叉奇偶校验-RAID-4" class="headerlink" title="块交叉奇偶校验 (RAID 4)"></a>块交叉奇偶校验 (RAID 4)</h2><p>   RAID 4使用同RAID3数目比率- -样大的数据磁盘和校验盘,但是访问数据的方式不同。奇偶校验码以块为单位存储，和-组数据块相关。</p>
<p>   在RAID 3中，每次访向都用到所有磁盘。然面，某些应用偏重于较小的数据访问，允许并行地发生多个独立访问。这就是发明RAID4 - RAID 6的目的。由于读操作需要校验每个扇区的错误检测信息来判断数据正确与否，只要少量的访问数据仍为同一个扇区，各磁盘上这些“小)数据量的读操作”就可以独立地进行。在RAID环境中，小数据访向在保护组中的一个磁盘发生，而大数据量访问需要用到保护组中的所有磁盘。</p>
<p>   写操作是另外个问题。看上去似乎每-次小数据量的写操作都需要访问其他磁盘信息，使用这些信息计算新的奇偶校验值，如图所示。一次“小数据量的写操作”需要读取旧数据和旧奇偶校验，添加新信息，接着把新的奇偶校验写入校验盘，把新的数据写入数据盘。<br>    <center><br>   <img src="http://cooguyan-1252810309.cos.ap-chengdu.myqcloud.com/RAID%E7%9A%84%E7%90%86%E8%A7%A3/20181230103402961.png" alt=""></center></p>
<p>   <em>小数据量写更新在RAID 3 和 RAID 4 上的比较</em><br>  </p>
<p>对小数据量写操作的优化减少了磁盘访问的数量，也减少了占用磁盘空间的数量。本图假设有4块数据和1块校验码。图左侧的RAID 3校验计算在加人块D0’之前要读数据块D1、D2和D3才能计算新校验码P’。(需要注意的是，新数据D0’直接来自CPU,所以不需要读磁盘来获取。)图右侧的RAID 4优化方法是读取旧值DO并与新值DO’比较看是否改变。然后读取旧校验码P,修改对应的位,形成新校验码P’。使用或逻辑操作即可实现。图中把三次读磁盘(D1、 D2、D3)和两次写磁盘(D0’, P’) 替换为两次读磁盘(DO, P)和两次写磁盘(D0’, P’),前者访问了所有磁盘，而后者仅访问其中的两个磁盘。随着校验组大小的增加将使得优化的效果更加明显。RAID 5亦使用同样的方式。</p>
<p>   减小开销的关键在于校验码不过是信息的-一个总和;通过观察写人新信息后哪些位发生了变化，我们只需改变校验盘上的对应位的信息即可。图6-13的右图说明了该方法。我们必须从要写的磁盘读取旧数据,用旧数据和新数据比较,看哪些位发生了变化。读旧奇偶校验和,改变对应的位，然后写人新数据以及新的校验和。这样，一次小数据量的写操作包含对两个磁盘的4次访问，而不是访问所有的磁盘。这种组织结构就是RAID 4</p>
<h2 id="分布式块交叉奇偶校验（RAID-5）"><a href="#分布式块交叉奇偶校验（RAID-5）" class="headerlink" title="分布式块交叉奇偶校验（RAID 5）"></a>分布式块交叉奇偶校验（RAID 5）</h2><p>RAID 4 有效的支持了大数据量读、大数据量写和小数据量读、小数据量写的混合操作。它的缺点是每次写操作都要更新校验盘、从而校验盘成为连续写的瓶颈。<br>为了解决校验-写瓶颈，校验信息可由分布到所有盘上，是的写操作不存在单一的瓶颈。这种分布式的奇偶校验组织方式就是RAID 5。<br>下图展示了数据在RAID 4 和 RAID 5 上是如何分布的。右图展示的是RAID 5 组织方式。其中数据块每行的校验信息不再限定在单个磁盘。只要校验块不在相同的磁盘上，这种组织方式就使得多 </p>
<p>   <img src="http://cooguyan-1252810309.cos.ap-chengdu.myqcloud.com/RAID%E7%9A%84%E7%90%86%E8%A7%A3/20181230103429087.png" alt=""></p>
<p>   个写操作可以同时发生。例如，右侧第1写操作是向第8块写数据，需委同时访向P2中的奇偶值，从而需要访问第I个和第3个磁盘。右侧第2个写操作对第5块进行写，意味着要更新其校验块P, ,从而需要访问第2个和第4个磁盘,所以它可以和写第8个数据并发进行。对于左侧的组织结构来说，同样的写操作则需要修改第5个磁盘上的PI和P2,这就构成了瓶颈。</p>
<h2 id="P-Q冗余-RAID-6"><a href="#P-Q冗余-RAID-6" class="headerlink" title="P+Q冗余(RAID 6)"></a>P+Q冗余(RAID 6)</h2><p>   基于奇偶校验的机制可使系统免受单个可自动识别的错误的破坏。当单个错误纠正机制不足以保护系统时，可利用奇偶校验对数据和另-个校验盘的信息进行二次计算。二次校验块可使系统从二次错误中恢复过来。因此，它的存储开销是RAID 5 的两倍。图6-13中的小数据方法还能成立，只是现在更新P和Q信息需要访向6个盘而不是访问4个盘。</p>
<h2 id="RAID-小结"><a href="#RAID-小结" class="headerlink" title="RAID 小结"></a>RAID 小结</h2><p>   RAID 1 和RAID 5 广泛用于服务器；一项估计是服务器中 80% 的磁盘都使用了某种RAID.</p>
<p>   RAID 系统的弱点是修复。首先，为了避免在修复时数据不可用，阵列必须设计为不必美闭系统就能替换出错盘。RAID 拥有足够的冗余性以保证不间断的操作，但是热交换磁盘对阵列和磁盘接口的物理及电路设计提出了要求。其次，修复中可能出现另外的错误，这样修复时间会影响丢失数据的概率:修复时间越长，另一错误引起丢失数据的概率越大。某些系统并不用等待操作员来装上好的磁盘，它们包含应急备用”，这样一旦检测出错误，数据就可以立即重建。操作员就可轻松地更换出错磁盘。最后，操作人员最终决定撒掉哪个磁盘。如图6-3所示，注意，操作员是人，因此他们有时候会撒掉好的磁盘导致不可恢复的屯盘错误。</p>
<p>   除了设计可以修复的RAID,还存在一些如何随着磁盘技术变化的问题。尽管磁盘厂商标称他们的产品具有很高的MTTF,但是这些数据是在假设的情况下得到的。如果某个特定磁盘阵列迎遇了由于空调系统故障、糟糕的磁盘架设计、构建或者安装引起震动面引起温度周期变化，出错率将大大增加，增加3-6倍(见6.12节)。RAD可靠性的计算假设多个磁盘失效之间是独立的，但实际上这些失效可能是相关的，因为环境引起的损伤可能会发生在阵列中的所有磁盘上。另一个问题是磁盘的带宽相对磁盘的容量变化得越来越慢，在个RAID系统中修复一个磁盘的时间变得越来越长，这一点反过来增加第二次故障出现的概率。例如，在假设没有干扰时，一个1000 GB SATA磁盘可能需要花费3个小时来顺序读。假设这个损坏的RAID 很可能被维续用来提供数据，重建过程就会被延长很多。除了增加时间外，另-个问题是在重建过程中一次读很多数据将意味者增加不可恢复的读媒体故障发生的概率，而不可恢复的故障将导致数据丢失。其他关于同时发生多个故障的看法是增加阵列中的磁盘数目以及使用SATA磁盘，这样比传统的商用磁盘慢-些，但具有更高的容量。</p>
<p>   因此，这些趋势导致对防止系统免受多重故障的研究兴趣大大增加。所以RAID6成为一种可选项，在实际中被使用。</p>
<pre><code>条带化(striping)：将逻辑上连续的数据块分布到不同的磁盘t,得到比单个微盘更高的性能。
镜像(mirroring)：将相同的数据写到客个数盘上，目的是增加数据的可用性。
保护组(protection group):共享一个公共校验磁的数据磁盘组或者数据块。
</code></pre>
          
        
      
    </div>

    
      


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2018/12/26/Tem/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/12/26/Tem/" class="post-title-link" itemprop="url">Tampermonkey的运行原理</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-12-26 15:03:02" itemprop="dateCreated datePublished" datetime="2018-12-26T15:03:02+08:00">2018-12-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-12-30 21:00:59" itemprop="dateModified" datetime="2018-12-30T21:00:59+08:00">2018-12-30</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Tampermonkey相关/" itemprop="url" rel="index"><span itemprop="name">Tampermonkey相关</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>TamperMonkey is a Google Chrome (and Opera and Chromium) plugin similar to GreaseMonkey for Firefox. It allows you to inject additional JavaScript into web pages you load in your browser, adding features, removing features, or in our case doing hacky, automated things</p>
</blockquote>
<hr>

          
        
      
    </div>

    
      


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cooguyan.github.io/2018/12/26/Chrome插件开发原理/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cooguyan">
      <meta itemprop="description" content="多读书读好书">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="结果元素">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/12/26/Chrome插件开发原理/" class="post-title-link" itemprop="url">Chrome插件开发原理</a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-12-26 15:01:50" itemprop="dateCreated datePublished" datetime="2018-12-26T15:01:50+08:00">2018-12-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-12-30 21:00:50" itemprop="dateModified" datetime="2018-12-30T21:00:50+08:00">2018-12-30</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Chrome相关/" itemprop="url" rel="index"><span itemprop="name">Chrome相关</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>Extensions are small software programs that customize the browsing experience. They enable users to tailor Chrome functionality and behavior to individual needs or preferences. They are built on web technologies such as HTML, JavaScript, and CSS.</p>
</blockquote>
<hr>

          
        
      
    </div>

    
      


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Cooguyan</p>
              <p class="site-description motion-element" itemprop="description">多读书读好书</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">45</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">15</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">31</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/cooguyan" title="GitHub &rarr; https://github.com/cooguyan" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://weibo.com/u/1005056978317811" title="Weibo &rarr; https://weibo.com/u/1005056978317811" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
            </div>
          

          
             <div class="cc-license motion-element" itemprop="license">
              
                
              
              
              
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
             </div>
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                快看啊有大神
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.elitk.com" title="https://www.elitk.com" rel="noopener" target="_blank">elitk</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.cloudkylin.me/" title="https://www.cloudkylin.me/" rel="noopener" target="_blank">cloudkylin</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">  <a href="http://www.miitbeian.gov.cn" rel="noopener" target="_blank">豫ICP备16034049号 </a>&copy; 2015 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-fas fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">cooguyan</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.0.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>







  






  













  



  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script src="/lib/reading_progress/reading_progress.js"></script>


  


  <script src="/js/src/utils.js?v=7.0.0"></script>

  <script src="/js/src/motion.js?v=7.0.0"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=7.0.0"></script>



  



  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var range = document.createRange(); //For Chrome
        var sel = window.getSelection(); //For Chrome
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; //Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.value = code;
        ta.textContent = code; //For FireFox
        ta.contentEditable = true;
        ta.readOnly = false;
        document.body.appendChild(ta);
        range.selectNode(ta);
        sel.removeAllRanges();
        sel.addRange(range);
        ta.setSelectionRange(0, code.length);
        var result = document.execCommand('copy');
        
        ta.blur(); //For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


</body>
</html>
